TextContent,TextType,url,Creator,CreatedAt,IssueID,RelatedPrID,IssueCategory,IsMPLF,CleanedTextContent
i experience this too,IssueComment,https://github.com/apache/mxnet/issues/5685#issuecomment-291617209,freddycct,2017-04-04 20:09:35,5685,[5689],Build bug,0,i experience this too
any quickfix? i cannot continue when i cannot compile...,IssueComment,https://github.com/apache/mxnet/issues/5685#issuecomment-291664279,freddycct,2017-04-04 22:56:21,5685,[5689],Build bug,0,any quickfix? i cannot continue when i cannot compile...
"arange op is Fixed by #8268, but other operators that uses mshadow::range have this issue",IssueComment,https://github.com/apache/mxnet/issues/8303#issuecomment-337063409,eric-haibin-lin,2017-10-16 22:34:01,8303,[8398],Data bug,1,"arange op is Fixed by #8268, but other operators that uses mshadow::range have this issue"
"It should have been fixed by https://github.com/apache/incubator-mxnet/pull/9256
Keep this open since we haven't figured out why it was not caught by CI build.",IssueComment,https://github.com/apache/mxnet/issues/9229#issuecomment-354522418,yzhliu,2017-12-30 02:23:53,9229,[9256],Build bug,1,It should have been fixed by [url] Keep this open since we haven't figured out why it was not caught by CI build.
Verified that #9256 fixed the problem on my end,IssueComment,https://github.com/apache/mxnet/issues/9229#issuecomment-354522617,szha,2017-12-30 02:28:40,9229,[9256],Build bug,1,Verified that #9256 fixed the problem on my end
"I can confirm I can now build this on my Mac. Many thanks.
",IssueComment,https://github.com/apache/mxnet/issues/9229#issuecomment-354539339,helloniklas,2017-12-30 10:41:41,9229,[9256],Build bug,1,I can confirm I can now build this on my Mac. Many thanks.
Thanks @javelinjs for the quick turnaround.,IssueComment,https://github.com/apache/mxnet/issues/9229#issuecomment-354562031,szha,2017-12-30 18:48:02,9229,[9256],Build bug,1,Thanks @javelinjs for the quick turnaround.
"Thanks for reporting this, we will improve our documentation and support NDArray as the input.",IssueComment,https://github.com/apache/mxnet/issues/9865#issuecomment-368117301,sxjscience,2018-02-23 19:39:10,9865,[9930],Algorithm design bug,0,"Thanks for reporting this, we will improve our documentation and support NDArray as the input."
"I find this is actually a bug. The `pred` will be reshaped if it has ndim=1, see https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/metric.py#L911-L912. In this case, the `pred` and the `label` have both shape (4,) and after reshaping, `pred.shape` will be changed to  (4,1). `pred - label` will then generate an array with shape (4, 4) and give the incorrect output.",IssueComment,https://github.com/apache/mxnet/issues/9865#issuecomment-368120147,sxjscience,2018-02-23 19:50:22,9865,[9930],Algorithm design bug,0,"I find this is actually a bug. The [code] will be reshaped if it has ndim=1, see [url]#L911-L912. In this case, the [code] and the [code] have both shape (4,) and after reshaping, [code] will be changed to (4,1). [code] will then generate an array with shape (4, 4) and give the incorrect output."
"This is a bug of `CopyFromTo` function. `a[0]` and `a[1]` share the same `var` and thus the copy is skipped.
https://github.com/apache/incubator-mxnet/blob/master/src/ndarray/ndarray.cc#L1131",IssueComment,https://github.com/apache/mxnet/issues/9976#issuecomment-370125976,reminisce,2018-03-03 07:12:24,9976,[9981],Data bug,1,This is a bug of [code] function. [code] and [code] share the same [code] and thus the copy is skipped. [url]#L1131
@reminisce Thank you! I will try to fix it.,IssueComment,https://github.com/apache/mxnet/issues/9976#issuecomment-370127367,wkcn,2018-03-03 07:40:33,9976,[9981],Data bug,1,@reminisce Thank you! I will try to fix it.
+1 for this. This is actually a very annoying bug.,IssueComment,https://github.com/apache/mxnet/issues/9405#issuecomment-365563721,tdomhan,2018-02-14 10:40:19,9405,[10094],Code bug,0,+1 for this. This is actually a very annoying bug.
@sandeep-krishnamurthy  : Please tag: Bug,IssueComment,https://github.com/apache/mxnet/issues/9405#issuecomment-369015334,rajanksin,2018-02-27 20:28:38,9405,[10094],Code bug,0,@sandeep-krishnamurthy : Please tag: Bug
"Facing the same problem. However, in Gluon, it cannot define a name",IssueComment,https://github.com/apache/mxnet/issues/9405#issuecomment-370181587,ShownX,2018-03-03 21:33:53,9405,[10094],Code bug,0,"Facing the same problem. However, in Gluon, it cannot define a name"
"Also the same error for row_sparse.tostype('row_sparse') 
We should fix this",IssueComment,https://github.com/apache/mxnet/issues/8524#issuecomment-341606082,eric-haibin-lin,2017-11-03 01:50:05,8524,[10400],Data bug,1,Also the same error for row_sparse.tostype('row_sparse') We should fix this
"Yes, I can work on this.",IssueComment,https://github.com/apache/mxnet/issues/8524#issuecomment-341619884,anirudh2290,2017-11-03 04:20:33,8524,[10400],Data bug,1,"Yes, I can work on this."
We should add `dtype` as an argument.,IssueComment,https://github.com/apache/mxnet/issues/10146#issuecomment-375742815,sxjscience,2018-03-23 17:31:04,10146,[10615],Algorithm design bug,0,We should add [code] as an argument.
Also tested on master branch 10ac52993aeaf2fa589a6b3636c8e23a65c8e639 and bug is present ,IssueComment,https://github.com/apache/mxnet/issues/10866#issuecomment-390522453,jessebrizzi,2018-05-20 23:54:00,10866,[11009],Code bug,0,Also tested on master branch 10ac52993aeaf2fa589a6b3636c8e23a65c8e639 and bug is present
@lanking520 - Any inputs on this?,IssueComment,https://github.com/apache/mxnet/issues/10866#issuecomment-392382344,sandeep-krishnamurthy,2018-05-27 22:42:20,10866,[11009],Code bug,0,@lanking520 - Any inputs on this?
"Sorry for the late reply, will start looking this today.",IssueComment,https://github.com/apache/mxnet/issues/10866#issuecomment-393757165,lanking520,2018-06-01 04:51:15,10866,[11009],Code bug,0,"Sorry for the late reply, will start looking this today."
"The following code, which always use `seq_len=500` will not trigger the seg fault. This is a very critical bug.

```python
from mxnet.gluon.rnn import LSTM
import mxnet as mx
import numpy as np

ctx = mx.gpu()
lstm = LSTM(num_layers=1, hidden_size=200, dropout=0.0)
lstm.initialize(ctx=ctx)
batch_size = 32
for seq_len in range(500, 10, -1):
    for repeat in range(10):
        real_seq_len = 500
        print(real_seq_len, repeat)
        inputs_nd = mx.nd.random.normal(0, 1, shape=(real_seq_len, batch_size, 200), ctx=ctx)
        out = lstm(inputs_nd)
        print(out[0].sum().asscalar())
        mx.nd.waitall()

```",IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379426878,sxjscience,2018-04-07 02:46:35,10453,[11041],Memory bug ,1,"The following code, which always use [code] will not trigger the seg fault. This is a very critical bug. ``[code]``"
The bug occurs when we have variable sequence length. I think it may be related to how the mxnet reuses the memory.,IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379429151,szhengac,2018-04-07 03:38:12,10453,[11041],Memory bug ,1,The bug occurs when we have variable sequence length. I think it may be related to how the mxnet reuses the memory.
I was able to finish running the script by setting `export MXNET_GPU_MEM_POOL_RESERVE=7`,IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379435474,szha,2018-04-07 06:11:43,10453,[11041],Memory bug ,1,I was able to finish running the script by setting [code]
"@szha how much memory consumption did you observe?
One thing to try is to can run the code step by step and figure out which batch causes the err",IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379435652,eric-haibin-lin,2018-04-07 06:15:20,10453,[11041],Memory bug ,1,@szha how much memory consumption did you observe? One thing to try is to can run the code step by step and figure out which batch causes the err
"What I observed is that it doesn't fail consistently on certain specific batch. Another team observed the same issue before, and it is likely caused by our backend memory pool holding too much memory, in which case the curand doesn't have enough memory to keep the random number generator states for each stream multiprocessor.",IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379809938,szha,2018-04-09 16:21:20,10453,[11041],Memory bug ,1,"What I observed is that it doesn't fail consistently on certain specific batch. Another team observed the same issue before, and it is likely caused by our backend memory pool holding too much memory, in which case the curand doesn't have enough memory to keep the random number generator states for each stream multiprocessor."
"I have similar issue when training speech model. even after 
`export MXNET_GPU_MEM_POOL_RESERVE=7`
will try larger RESERVE",IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-379910659,Jerryzcn,2018-04-09 22:15:19,10453,[11041],Memory bug ,1,I have similar issue when training speech model. even after [code] will try larger RESERVE
I find that the `dw_desc_` is created but never destroyed. See https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L516 and  https://github.com/apache/incubator-mxnet/blob/master/src/operator/cudnn_rnn-inl.h#L98,IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-381779488,sxjscience,2018-04-16 23:17:43,10453,[11041],Memory bug ,1,I find that the [code] is created but never destroyed. See [url]#L516 and [url]#L98
It's related to https://github.com/pytorch/pytorch/issues/953. `cudnnSetDropoutDescriptor` uses a large amount of GPU memory. One choice to solve this problem is to create a DropoutDescriptor when we create a stream and always use cudnnGetDropoutDescriptor. This will also accelerate the speed of RNN layer in Gluon because we can avoid calling Alloc and Free.,IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-381819126,sxjscience,2018-04-17 03:04:56,10453,[11041],Memory bug ,1,It's related to [url] [code] uses a large amount of GPU memory. One choice to solve this problem is to create a DropoutDescriptor when we create a stream and always use cudnnGetDropoutDescriptor. This will also accelerate the speed of RNN layer in Gluon because we can avoid calling Alloc and Free.
"https://github.com/apache/incubator-mxnet/pull/11004/ ""fixes"" this issue. The filter descriptors that are freed in the destructor were not created if cudaMalloc would fail during `Forward` or `Backward`.

Now the following error will be returned in an OOM situation:
```

mxnet.base.MXNetError: [05:13:15] src/storage/./pooled_storage_manager.h:108: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f358103783b]
[bt] (1) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f35810383a8]
[bt] (2) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::Alloc(mxnet::Storage::Handle*)+0x154) [0x7f358398b384]
[bt] (3) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::StorageImpl::Alloc(mxnet::Storage::Handle*)+0x5d) [0x7f358398d80d]
[bt] (4) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp<float>::Init(mshadow::Stream<mshadow::gpu>*, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x1de5) [0x7f3585606a55]
[bt] (5) /home/leonard/software/mxnet-master/python/mxnet/../../lib/libmxnet.so(mxnet::op::CuDNNRNNOp<float>::Forward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xa5d) [0x7f358560e07d]
...
```

In particular, #11004 makes sure that the descriptors are always created during class initialization and not just somewhere down the line in`Forward` / `Backward`.",IssueComment,https://github.com/apache/mxnet/issues/10453#issuecomment-390458859,leezu,2018-05-20 05:24:30,10453,[11041],Memory bug ,1,"[url] ""fixes"" this issue. The filter descriptors that are freed in the destructor were not created if cudaMalloc would fail during [code] or [code]. Now the following error will be returned in an OOM situation: ``[code]`[code]Forward[code]Backward`."
cc @zhanghang1989,IssueComment,https://github.com/apache/mxnet/issues/11077#issuecomment-392663799,ZiyueHuang,2018-05-29 06:14:54,11077,[11145],Algorithm design bug,1,cc @zhanghang1989
"RoiAlign was added in this PR - https://github.com/apache/incubator-mxnet/pull/10852 by @zhanghang1989 . can you take a look.

@sandeep-krishnamurthy please add label - ""Operator"", ""Bug""",IssueComment,https://github.com/apache/mxnet/issues/11077#issuecomment-394500721,anirudhacharya,2018-06-04 21:10:37,11077,[11145],Algorithm design bug,1,"RoiAlign was added in this PR - [url] by @zhanghang1989 . can you take a look. @sandeep-krishnamurthy please add label - ""Operator"", ""Bug"""
"I am following up this issue, will fix it soon.",IssueComment,https://github.com/apache/mxnet/issues/11077#issuecomment-394507664,zhanghang1989,2018-06-04 21:36:28,11077,[11145],Algorithm design bug,1,"I am following up this issue, will fix it soon."
This hasn't broken in like a year to my knowledge.,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367526499,cjolivier01,2018-02-22 00:33:19,9853,[11259],Test bug,0,This hasn't broken in like a year to my knowledge.
is that an mkl build?,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367527415,cjolivier01,2018-02-22 00:37:52,9853,[11259],Test bug,0,is that an mkl build?
"No, on Windows we only run OpenBLAS.
",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367528116,marcoabreu,2018-02-22 00:41:24,9853,[11259],Test bug,0,"No, on Windows we only run OpenBLAS."
It seems https://github.com/apache/incubator-mxnet/issues/9853 fails in the same test. It's strange how this fails.,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367846746,zheng-da,2018-02-22 22:37:08,9853,[11259],Test bug,0,It seems [url] fails in the same test. It's strange how this fails.
"Wasn't elemwise_add changed to use mkl?
btw, was it verified that mkl is faster for all shapes and types?  I saw it allocates memory, which seems like it might be slow.",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367850176,cjolivier01,2018-02-22 22:51:28,9853,[11259],Test bug,0,"Wasn't elemwise_add changed to use mkl? btw, was it verified that mkl is faster for all shapes and types? I saw it allocates memory, which seems like it might be slow."
"I don't think we have the tools to measure performance on that scale yet. As far as I know, this is in the works. Since there's still some time until 1.2, we can definitely gather these numbers.",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367859620,marcoabreu,2018-02-22 23:34:53,9853,[11259],Test bug,0,"I don't think we have the tools to measure performance on that scale yet. As far as I know, this is in the works. Since there's still some time until 1.2, we can definitely gather these numbers."
"Well even if it was changed to use MKL, this would not apply here since we're running on OpenBLAS, right?",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367860051,marcoabreu,2018-02-22 23:37:08,9853,[11259],Test bug,0,"Well even if it was changed to use MKL, this would not apply here since we're running on OpenBLAS, right?"
"@cjolivier01 in both cases (https://github.com/apache/incubator-mxnet/issues/9853 and https://github.com/apache/incubator-mxnet/issues/9844), the tests fail in test_bmod. It shouldn't have invoked elemwise_add ",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367860345,zheng-da,2018-02-22 23:38:19,9853,[11259],Test bug,0,@cjolivier01 in both cases ([url] and [url] the tests fail in test_bmod. It shouldn't have invoked elemwise_add
"i don’t know what is invoked in the process of calling test_bmod(). could
be that elemwise_add() isn’t called, or is called before and corrupts
memory, or maybe has nothing to do with elemwise_add.

however, we seem to have a lot of tests that are suddenly failing... any
ideas?

On Thu, Feb 22, 2018 at 3:38 PM Da Zheng <notifications@github.com> wrote:

> @cjolivier01 <https://github.com/cjolivier01> in both cases (#9853
> <https://github.com/apache/incubator-mxnet/issues/9853> and #9844
> <https://github.com/apache/incubator-mxnet/issues/9844>), the tests fail
> in test_bmod. It shouldn't have invoked elemwise_add
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/incubator-mxnet/issues/9853#issuecomment-367860345>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AKts_fYOpzaNXyAWlk7woscdV0Pz1w4Iks5tXfqHgaJpZM4SOaWO>
> .
>
",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367866040,cjolivier01,2018-02-23 00:09:11,9853,[11259],Test bug,0,"i don’t know what is invoked in the process of calling test_bmod(). could be that elemwise_add() isn’t called, or is called before and corrupts memory, or maybe has nothing to do with elemwise_add. however, we seem to have a lot of tests that are suddenly failing... any ideas? On Thu, Feb 22, 2018 at 3:38 PM Da Zheng <[email]> wrote: > @cjolivier01 <[url] in both cases (#9853 > <[url] and #9844 > <[url] the tests fail > in test_bmod. It shouldn't have invoked elemwise_add > > — > You are receiving this because you were mentioned. > > > Reply to this email directly, view it on GitHub > <[url]#issuecomment-367860345>, > or mute the thread > <[url] > . >"
I don't have a clue right now. so far we see failures in random generators and binary operators. it's weird why it fails in these simple operators that are seemingly irrelevant to MKLDNN operators.,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-367868599,zheng-da,2018-02-23 00:23:02,9853,[11259],Test bug,0,I don't have a clue right now. so far we see failures in random generators and binary operators. it's weird why it fails in these simple operators that are seemingly irrelevant to MKLDNN operators.
https://github.com/apache/incubator-mxnet/issues/9844,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-373882624,marcoabreu,2018-03-17 01:12:48,9853,[11259],Test bug,0,[url]
"I think the cause of this is that operator mod is using doubles to make the computation, while the test is forcing float32, also the modulo operator for floating point seems to give different results in GPU vs CPU. Why is fmod in cuda giving different results?

According to table 7  here https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#introduction-cuda-dynamic-parallelism

there should be no differences in fmod.

https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L1511
https://github.com/apache/incubator-mxnet/blob/master/src/operator/mshadow_op.h#L402

>>> np.double(1.68) % np.double(1.30123)
0.37876999999999983
>>> np.float32(1.68) % np.float32(1.30123)
0.37877

",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-386875707,larroy,2018-05-06 12:24:17,9853,[11259],Test bug,0,"I think the cause of this is that operator mod is using doubles to make the computation, while the test is forcing float32, also the modulo operator for floating point seems to give different results in GPU vs CPU. Why is fmod in cuda giving different results? According to table 7 here [url]#introduction-cuda-dynamic-parallelism there should be no differences in fmod. [url]#L1511 [url]#L402 >>> np.double(1.68) % np.double(1.30123) 0.37876999999999983 >>> np.float32(1.68) % np.float32(1.30123) 0.37877"
"I tried to increase the tolerance, but I found out one failure where the difference is much bigger than expected 0.28679015 . I think we should look deeper into this

[-116.15162] <-input
<NDArray 1 @gpu(0)>
[-115.8648288] <- gradient
[0.28679015] <- diff
[0.8396868] <- a 
[0.0020733]  <- b
FAIL",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-386885067,larroy,2018-05-06 14:51:59,9853,[11259],Test bug,0,"I tried to increase the tolerance, but I found out one failure where the difference is much bigger than expected 0.28679015 . I think we should look deeper into this [-116.15162] <-input <NDArray 1 @gpu(0)> [-115.8648288] <- gradient [0.28679015] <- diff [0.8396868] <- a [0.0020733] <- b FAIL"
"reproducible 100% with export MXNET_TEST_SEED=1688524483

nosetests-3.4 -s -v test_operator_gpu.py:test_binary_op

```
diff --git a/tests/python/unittest/test_operator.py b/tests/python/unittest/test_operator.py
index 5d38222..04e880c 100644
--- a/tests/python/unittest/test_operator.py
+++ b/tests/python/unittest/test_operator.py
@@ -1429,6 +1429,16 @@ def check_binary_op_backward(symbol, baseline, gen_data, rtol=1e-3, atol=1e-5):
         y.forward(is_train=True)
         y.backward([mx.nd.array(out)])
         assert_allclose(y_1.asnumpy(), x_1, rtol=rtol, atol=atol)
+        z = np.abs(y_2.asnumpy() - x_2)
+        w = np.where(z>atol)
+        if w[0].size > 0:
+            print(""d[0].shape: {} d[1].shape: {} baseline_grad2.shape: {}"".format(d[0].shape, d[1].shape, baseline_grad2.shape))
+            print(w)
+            print(y_2[w])
+            print(x_2[w])
+            print(z[w])
+            print(d[0][w])
+            print(d[1][w])
```",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-386885128,larroy,2018-05-06 14:53:05,9853,[11259],Test bug,0,reproducible 100% with export MXNET_TEST_SEED=1688524483 nosetests-3.4 -s -v test_operator_gpu.py:test_binary_op ``[code]``
"I have likely found the root cause of this problem, just so we don't duplicate resources on this one.",IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-396886126,larroy,2018-06-13 10:08:52,9853,[11259],Test bug,0,"I have likely found the root cause of this problem, just so we don't duplicate resources on this one."
seed 1060292419,IssueComment,https://github.com/apache/mxnet/issues/9853#issuecomment-397011088,larroy,2018-06-13 16:58:43,9853,[11259],Test bug,0,seed 1060292419
@anbrjohn thanks for reporting. Would you send a patch to fix the example?,IssueComment,https://github.com/apache/mxnet/issues/11352#issuecomment-399171330,szha,2018-06-21 16:51:26,11352,[11375],Algorithm design bug,0,@anbrjohn thanks for reporting. Would you send a patch to fix the example?
@sandeep-krishnamurthy requesting this be labeled as cuda and operator,IssueComment,https://github.com/apache/mxnet/issues/11568#issuecomment-403257247,andrewfayres,2018-07-08 02:35:10,11568,[11806],Test bug,1,@sandeep-krishnamurthy requesting this be labeled as cuda and operator
Added disabled test since the tests are disabled for USE_CUDA=ON USE_CUDNN=OFF as part of #11470,IssueComment,https://github.com/apache/mxnet/issues/11568#issuecomment-404320744,anirudh2290,2018-07-11 21:45:58,11568,[11806],Test bug,1,Added disabled test since the tests are disabled for USE_CUDA=ON USE_CUDNN=OFF as part of #11470
"@mxnet-label-bot could you please add [gluon, bug] to this label?",IssueComment,https://github.com/apache/mxnet/issues/12087#issuecomment-411818660,lanking520,2018-08-09 16:31:35,12087,[12093],Data bug,0,"@mxnet-label-bot could you please add [gluon, bug] to this label?"
"@apache/mxnet-committers: This issue has been inactive for the past 90 days. It has no label and needs triage.

For general ""how-to"" questions, our [user forum](https://discuss.mxnet.io/) (and [Chinese version](https://discuss.gluon.ai/)) is a good place to get help.",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-357431288,szha,2018-01-13 12:26:27,8270,[12295],Version compatibility bug,0,"@apache/mxnet-committers: This issue has been inactive for the past 90 days. It has no label and needs triage. For general ""how-to"" questions, our [user forum]([url] (and [Chinese version]([url] is a good place to get help."
Sorry could you clarify the issue and give a bit of context?,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-380194426,ThomasDelteil,2018-04-10 18:05:08,8270,[12295],Version compatibility bug,0,Sorry could you clarify the issue and give a bit of context?
"These are variables that are not defined in the current context which can raise NameError at runtime.  Often they are typos or missing imports or code refactoring errors or things like __basestring__ or __xrange()__ being defined in Python 2 but being removed in Python 3.

Just try reading thru the code as if you were the Python interpreter and ask yourself ""Where is this variable defined?"".",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-380203936,cclauss,2018-04-10 18:35:31,8270,[12295],Version compatibility bug,0,"These are variables that are not defined in the current context which can raise NameError at runtime. Often they are typos or missing imports or code refactoring errors or things like __basestring__ or __xrange()__ being defined in Python 2 but being removed in Python 3. Just try reading thru the code as if you were the Python interpreter and ask yourself ""Where is this variable defined?""."
"Oh I understand now, that's some static code analysis right? Thanks for running that, seems like some files need fixing.",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-380207663,ThomasDelteil,2018-04-10 18:47:47,8270,[12295],Version compatibility bug,0,"Oh I understand now, that's some static code analysis right? Thanks for running that, seems like some files need fixing."
"Are there any specific advantages that flake8 has over pylint? 
pylint has equivalent messages such as 'undefined-variable' (equivalent to F821). So just wondering if it would be easier to add more options to pylintrc and include in the existing build process.",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-407888667,vandanavk,2018-07-25 20:42:24,8270,[12295],Version compatibility bug,0,Are there any specific advantages that flake8 has over pylint? pylint has equivalent messages such as 'undefined-variable' (equivalent to F821). So just wondering if it would be easier to add more options to pylintrc and include in the existing build process.
"flake8 will find _undefined names_ that pylint will not because flake8 makes an abstract syntax tree for each Python file and plyint does not.  This also explains why flake8 gives different results in Python 2 and Python 3 (the have different ASTs while pylint does not.

Take running code like __isinstance(unicode('Hello'), basestring)__ through pylint on Python 2 and Python 3 and see if there are differences in the output.  Now try same with flake8.",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-407891316,cclauss,2018-07-25 20:51:36,8270,[12295],Version compatibility bug,0,"flake8 will find _undefined names_ that pylint will not because flake8 makes an abstract syntax tree for each Python file and plyint does not. This also explains why flake8 gives different results in Python 2 and Python 3 (the have different ASTs while pylint does not. Take running code like __isinstance(unicode('Hello'), basestring)__ through pylint on Python 2 and Python 3 and see if there are differences in the output. Now try same with flake8."
"I take it back...  You are right.

PyLint is now able to find undefined names.

$ __python3 -m pylint *.py__
```
************* Module pylint_test
pylint_test.py:1:0: C0111: Missing module docstring (missing-docstring)
pylint_test.py:1:11: E0602: Undefined variable 'unicode' (undefined-variable)
pylint_test.py:1:29: E0602: Undefined variable 'basestring' (undefined-variable)
```",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-407893666,cclauss,2018-07-25 20:59:50,8270,[12295],Version compatibility bug,0,I take it back... You are right. PyLint is now able to find undefined names. $ __python3 -m pylint *.py__ ``[code]``
"
flake8 testing of https://github.com/apache/incubator-mxnet on Python 3.7.0

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
/home/travis/virtualenv/python3.7.0/lib/python3.7/site-packages/pycodestyle.py:113: FutureWarning: Possible nested set at position 1
  EXTRANEOUS_WHITESPACE_REGEX = re.compile(r'[[({] | []}),;:]')
./example/reinforcement-learning/a3c/a3c.py:206:16: F821 undefined name 'robo_data'
    dataiter = robo_data.RobosimsDataIter('scenes', args.batch_size, args.input_length, web_viz=True)
               ^
./example/ssd/dataset/pycocotools/coco.py:266:41: F821 undefined name 'm'
                        img = np.ones( (m.shape[0], m.shape[1], 3) )
                                        ^
./example/ssd/dataset/pycocotools/coco.py:266:53: F821 undefined name 'm'
                        img = np.ones( (m.shape[0], m.shape[1], 3) )
                                                    ^
./example/ssd/dataset/pycocotools/coco.py:273:52: F821 undefined name 'm'
                        ax.imshow(np.dstack( (img, m*0.5) ))
                                                   ^
./example/ssd/dataset/pycocotools/coco.py:435:16: F821 undefined name 'm'
        return m
               ^
./example/ssd/symbol/common.py:209:23: F821 undefined name 'start_offset'
         min_sizes = [start_offset] + tmp.tolist()
                      ^
./example/ssd/symbol/common.py:210:46: F821 undefined name 'start_offset'
         max_sizes = tmp.tolist() + [tmp[-1]+start_offset]
                                             ^
./example/fcn-xs/image_segmentaion.py:100:60: F821 undefined name 'ctx'
    fcnxs_args[""data""] = mx.nd.array(get_data(args.input), ctx)
                                                           ^
./example/fcn-xs/image_segmentaion.py:103:60: F821 undefined name 'ctx'
    fcnxs_args[""softmax_label""] = mx.nd.empty(label_shape, ctx)
                                                           ^
./example/fcn-xs/image_segmentaion.py:104:26: F821 undefined name 'ctx'
    exector = fcnxs.bind(ctx, fcnxs_args, args_grad=None, grad_req=""null"", aux_states=fcnxs_args)
                         ^
./example/neural-style/end_to_end/model_vgg19.py:94:17: F821 undefined name 'out'
    arg_names = out.list_arguments()
                ^
./example/neural-style/end_to_end/model_vgg19.py:105:16: F821 undefined name 'out'
    executor = out.bind(ctx=ctx, args=arg_dict, args_grad=[], grad_req=""null"")
               ^
./example/sparse/factorization_machine/metric.py:111:22: F821 undefined name 'label_zero_num'
        total_area = label_zero_num * label_one_num
                     ^
./example/sparse/factorization_machine/metric.py:111:39: F821 undefined name 'label_one_num'
        total_area = label_zero_num * label_one_num
                                      ^
./example/profiler/profiler_executor.py:89:15: F821 undefined name 'search_plan'
        sym = search_plan(sym, data=data_shapes)
              ^
./example/profiler/profiler_executor.py:89:37: F821 undefined name 'data_shapes'
        sym = search_plan(sym, data=data_shapes)
                                    ^
./python/mxnet/initializer.py:700:20: F821 undefined name '_INITIALIZER_REGISTRY'
            init = _INITIALIZER_REGISTRY[klass.lower()](**kwargs)
                   ^
./python/mxnet/optimizer.py:718:34: F821 undefined name 'array'
            weight_master_copy = array(weight, ctx=weight.context, dtype=numpy.float32)
                                 ^
./python/mxnet/optimizer.py:769:16: F821 undefined name 'multiply'
        norm = multiply(v, v).asnumpy().sum()
               ^
./tests/python/unittest/test_engine_import.py:33:13: F821 undefined name 'reload'
            reload(mxnet)
            ^
./tests/nightly/model_backwards_compatibility_check/common.py:214:12: F821 undefined name 'cmp'
    return cmp(normalize(version1), normalize(version2))
           ^
./docs/mxdoc.py:75:16: F821 undefined name 'root_path'
    pdf_path = root_path + '/docs/api/r/mxnet-r-reference-manual.pdf'
               ^
22    F821 undefined name 'root_path'
22
```",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-409746782,cclauss,2018-08-01 22:33:32,8270,[12295],Version compatibility bug,0,"flake8 testing of [url] on Python 3.7.0 $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__ ``[code]``"
@cclauss I'm working on fixing these errors. Will add you to the PRs as I submit them.,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-409750987,vandanavk,2018-08-01 22:52:45,8270,[12295],Version compatibility bug,0,@cclauss I'm working on fixing these errors. Will add you to the PRs as I submit them.
Pylint reports undefined variables when there is a wildcard import. The list from Pylint is at https://github.com/apache/incubator-mxnet/issues/11904,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-409753922,vandanavk,2018-08-01 23:06:36,8270,[12295],Version compatibility bug,0,Pylint reports undefined variables when there is a wildcard import. The list from Pylint is at [url]
"[flake8](http://flake8.pycqa.org) (on Python 3.7.0) moved from finding 22 undefined names down to 19.

flake8 testing of https://github.com/apache/incubator-mxnet on Python 3.7.0

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./example/reinforcement-learning/a3c/a3c.py:206:16: F821 undefined name 'robo_data'
    dataiter = robo_data.RobosimsDataIter('scenes', args.batch_size, args.input_length, web_viz=True)
               ^
./example/ssd/dataset/pycocotools/coco.py:266:41: F821 undefined name 'm'
                        img = np.ones( (m.shape[0], m.shape[1], 3) )
                                        ^
./example/ssd/dataset/pycocotools/coco.py:266:53: F821 undefined name 'm'
                        img = np.ones( (m.shape[0], m.shape[1], 3) )
                                                    ^
./example/ssd/dataset/pycocotools/coco.py:273:52: F821 undefined name 'm'
                        ax.imshow(np.dstack( (img, m*0.5) ))
                                                   ^
./example/ssd/dataset/pycocotools/coco.py:435:16: F821 undefined name 'm'
        return m
               ^
./example/ssd/symbol/common.py:209:23: F821 undefined name 'start_offset'
         min_sizes = [start_offset] + tmp.tolist()
                      ^
./example/ssd/symbol/common.py:210:46: F821 undefined name 'start_offset'
         max_sizes = tmp.tolist() + [tmp[-1]+start_offset]
                                             ^
./example/fcn-xs/image_segmentaion.py:100:60: F821 undefined name 'ctx'
    fcnxs_args[""data""] = mx.nd.array(get_data(args.input), ctx)
                                                           ^
./example/fcn-xs/image_segmentaion.py:103:60: F821 undefined name 'ctx'
    fcnxs_args[""softmax_label""] = mx.nd.empty(label_shape, ctx)
                                                           ^
./example/fcn-xs/image_segmentaion.py:104:26: F821 undefined name 'ctx'
    exector = fcnxs.bind(ctx, fcnxs_args, args_grad=None, grad_req=""null"", aux_states=fcnxs_args)
                         ^
./example/neural-style/end_to_end/model_vgg19.py:94:17: F821 undefined name 'out'
    arg_names = out.list_arguments()
                ^
./example/neural-style/end_to_end/model_vgg19.py:105:16: F821 undefined name 'out'
    executor = out.bind(ctx=ctx, args=arg_dict, args_grad=[], grad_req=""null"")
               ^
./example/sparse/factorization_machine/metric.py:111:22: F821 undefined name 'label_zero_num'
        total_area = label_zero_num * label_one_num
                     ^
./example/sparse/factorization_machine/metric.py:111:39: F821 undefined name 'label_one_num'
        total_area = label_zero_num * label_one_num
                                      ^
./example/profiler/profiler_executor.py:89:15: F821 undefined name 'search_plan'
        sym = search_plan(sym, data=data_shapes)
              ^
./example/profiler/profiler_executor.py:89:37: F821 undefined name 'data_shapes'
        sym = search_plan(sym, data=data_shapes)
                                    ^
./tests/python/unittest/test_engine_import.py:33:13: F821 undefined name 'reload'
            reload(mxnet)
            ^
./tests/nightly/model_backwards_compatibility_check/common.py:216:12: F821 undefined name 'cmp'
    return cmp(normalize(version1), normalize(version2))
           ^
./docs/mxdoc.py:75:16: F821 undefined name 'root_path'
    pdf_path = root_path + '/docs/api/r/mxnet-r-reference-manual.pdf'
               ^
19    F821 undefined name 'root_path'
19
```",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-411455720,cclauss,2018-08-08 15:51:06,8270,[12295],Version compatibility bug,0,"[flake8]([url] (on Python 3.7.0) moved from finding 22 undefined names down to 19. flake8 testing of [url] on Python 3.7.0 $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__ ``[code]``"
"With the merge of #11982, flake8 (on Python 3) is now down to 10 undefined names:

[flake8](http://flake8.pycqa.org) testing of https://github.com/apache/incubator-mxnet on Python 3.7.0

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./example/reinforcement-learning/a3c/a3c.py:206:16: F821 undefined name 'robo_data'
    dataiter = robo_data.RobosimsDataIter('scenes', args.batch_size, args.input_length, web_viz=True)
               ^
./example/neural-style/end_to_end/model_vgg19.py:94:17: F821 undefined name 'out'
    arg_names = out.list_arguments()
                ^
./example/neural-style/end_to_end/model_vgg19.py:105:16: F821 undefined name 'out'
    executor = out.bind(ctx=ctx, args=arg_dict, args_grad=[], grad_req=""null"")
               ^
./example/sparse/factorization_machine/metric.py:111:22: F821 undefined name 'label_zero_num'
        total_area = label_zero_num * label_one_num
                     ^
./example/sparse/factorization_machine/metric.py:111:39: F821 undefined name 'label_one_num'
        total_area = label_zero_num * label_one_num
                                      ^
./example/profiler/profiler_executor.py:89:15: F821 undefined name 'search_plan'
        sym = search_plan(sym, data=data_shapes)
              ^
./example/profiler/profiler_executor.py:89:37: F821 undefined name 'data_shapes'
        sym = search_plan(sym, data=data_shapes)
                                    ^
./tests/python/unittest/test_engine_import.py:33:13: F821 undefined name 'reload'
            reload(mxnet)
            ^
./tests/nightly/model_backwards_compatibility_check/common.py:216:12: F821 undefined name 'cmp'
    return cmp(normalize(version1), normalize(version2))
           ^
./docs/mxdoc.py:75:16: F821 undefined name 'root_path'
    pdf_path = root_path + '/docs/api/r/mxnet-r-reference-manual.pdf'
               ^
10    F821 undefined name 'root_path'
10
```",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-411628132,cclauss,2018-08-09 03:50:23,8270,[12295],Version compatibility bug,0,"With the merge of #11982, flake8 (on Python 3) is now down to 10 undefined names: [flake8]([url] testing of [url] on Python 3.7.0 $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__ ``[code]``"
"`./python/mxnet/base.py:735:12: F821 undefined name 'C'
    return C.c_int(init_val)
           ^
./python/mxnet/base.py:739:14: F821 undefined name 'C'
    x_addr = C.addressof(x)
             ^
./python/mxnet/base.py:740:13: F821 undefined name 'C'
    int_p = C.POINTER(C.c_int)
            ^
./python/mxnet/base.py:740:23: F821 undefined name 'C'
    int_p = C.POINTER(C.c_int)
                      ^
./python/mxnet/base.py:741:18: F821 undefined name 'C'
    x_int_addr = C.cast(x_addr, int_p)
                 ^
`
@mkolod @KellenSunderland These have been introduced by the TensorRT commit. Could you have a look at these?",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-412399056,vandanavk,2018-08-13 03:21:55,8270,[12295],Version compatibility bug,0,[code] @mkolod @KellenSunderland These have been introduced by the TensorRT commit. Could you have a look at these?
@vandanavk Taking a look.,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-412441700,KellenSunderland,2018-08-13 08:15:12,8270,[12295],Version compatibility bug,0,@vandanavk Taking a look.
@vandanavk Thanks for the catch!  Should be addressed in https://github.com/apache/incubator-mxnet/pull/12147,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-412489332,KellenSunderland,2018-08-13 11:35:26,8270,[12295],Version compatibility bug,0,@vandanavk Thanks for the catch! Should be addressed in [url]
"Thanks @KellenSunderland 

@cclauss I don't see the errors
```
./tests/python/unittest/test_engine_import.py:33:13: F821 undefined name 'reload'
            reload(mxnet)
            ^
./tests/nightly/model_backwards_compatibility_check/common.py:216:12: F821 undefined name 'cmp'
    return cmp(normalize(version1), normalize(version2))
```
I've tried flake8 and pylint, Python2 and Python3.

Although I do see 
```
************* Module mxnet.ndarray
python/mxnet/ndarray/__init__.py(34): [E0602 ] Undefined variable 'ndarray'
************* Module mxnet.symbol
python/mxnet/symbol/__init__.py(31): [E0602 ] Undefined variable 'symbol'
```
After these are fixed, I'll add ""undefined-variable"" to pylintrc in the CI build. This will check future commits to python/mxnet.",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413254384,vandanavk,2018-08-15 16:31:32,8270,[12295],Version compatibility bug,0,"Thanks @KellenSunderland @cclauss I don't see the errors ``[code]`[code]`[code]`` After these are fixed, I'll add ""undefined-variable"" to pylintrc in the CI build. This will check future commits to python/mxnet."
"* reload(): https://docs.python.org/3/whatsnew/3.0.html#builtins
    * Was moved to https://docs.python.org/3/library/importlib.html#importlib.reload
* cmp(): https://docs.python.org/3/whatsnew/3.0.html#ordering-comparisons
    * http://python-future.org/compatible_idioms.html#cmp has a formula for building your own __cmp()__",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413258613,cclauss,2018-08-15 16:44:55,8270,[12295],Version compatibility bug,0,* reload(): [url]#builtins * Was moved to [url]#importlib.reload * cmp(): [url]#ordering-comparisons * [url]#cmp has a formula for building your own __cmp()__
"See PEP8 for reasons why star imports are for the birds...  

On the `python/mxnet/ndarray/__init__.py` issue, you could try adding a line `from .ndarray import __all__ as ndarray___all__` and then below replace `ndarray.__all__` with `ndarray___all__` .
",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413264002,cclauss,2018-08-15 17:01:22,8270,[12295],Version compatibility bug,0,"See PEP8 for reasons why star imports are for the birds... On the [code] issue, you could try adding a line [code] and then below replace [code] with [code] ."
"* #12191 should fix __cmp()__
* #12188 Should fix __reload()__
* https://github.com/apache/incubator-mxnet/commit/f3070fbe04b94e9715ef950cb33ce3402be5c077#r30092025 added three new undefined names

[flake8](http://flake8.pycqa.org) testing of https://github.com/apache/incubator-mxnet on Python 3.7.0

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./amalgamation/python/mxnet_predict.py:68:40: F821 undefined name 'libinfo_py'
                libinfo = {'__file__': libinfo_py}
                                       ^
./amalgamation/python/mxnet_predict.py:69:35: F821 undefined name 'libinfo_py'
                exec(compile(open(libinfo_py, ""rb"").read(), libinfo_py, 'exec'), libinfo, libinfo)
                                  ^
./amalgamation/python/mxnet_predict.py:69:61: F821 undefined name 'libinfo_py'
                exec(compile(open(libinfo_py, ""rb"").read(), libinfo_py, 'exec'), libinfo, libinfo)
                                                            ^
./tests/python/unittest/test_engine_import.py:33:13: F821 undefined name 'reload'
            reload(mxnet)
            ^
./tests/nightly/model_backwards_compatibility_check/common.py:216:12: F821 undefined name 'cmp'
    return cmp(normalize(version1), normalize(version2))
           ^
5     F821 undefined name 'libinfo_py'
6
```",IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413354049,cclauss,2018-08-15 22:08:06,8270,[12295],Version compatibility bug,0,"* #12191 should fix __cmp()__ * #12188 Should fix __reload()__ * [url]#r30092025 added three new undefined names [flake8]([url] testing of [url] on Python 3.7.0 $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__ ``[code]``"
@hqucms Could you have a look at these https://github.com/apache/incubator-mxnet/commit/f3070fbe04b94e9715ef950cb33ce3402be5c077#r30092025?,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413635316,vandanavk,2018-08-16 18:08:23,8270,[12295],Version compatibility bug,0,@hqucms Could you have a look at these [url]#r30092025?
@vandanavk Thank you for spotting this! I made a fix in https://github.com/apache/incubator-mxnet/pull/12211.,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413685943,hqucms,2018-08-16 21:10:40,8270,[12295],Version compatibility bug,0,@vandanavk Thank you for spotting this! I made a fix in [url]
@cclauss looks like all the PRs for fixing these errors are merged. Do you think this issue can be closed now?,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-413923633,vandanavk,2018-08-17 16:44:35,8270,[12295],Version compatibility bug,0,@cclauss looks like all the PRs for fixing these errors are merged. Do you think this issue can be closed now?
https://github.com/apache/incubator-mxnet/pull/12295 is this the last open PR related to this issue @cclauss ?,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-415461086,vandanavk,2018-08-23 15:32:38,8270,[12295],Version compatibility bug,0,[url] is this the last open PR related to this issue @cclauss ?
Yes.,IssueComment,https://github.com/apache/mxnet/issues/8270#issuecomment-415468265,cclauss,2018-08-23 15:52:58,8270,[12295],Version compatibility bug,0,Yes.
"`kernel_size` must be odd number, otherwise strange things happen! It's mentioned in the docs string, but I think it would be better for this to be added as strict validation to avoid confusion like this in the future.

With regards to the stride (once a odd `kernel_size` is used), `stride1` relates to the striding on the `data1` feature map 1 and `stride2` relates to the striding within the 'neighborhood' defined on feature map 2. Without a neighborhood this method would compare all patches from feature map 1 with all patches from feature map 2. With large feature maps, the number of comparisons becomes too large, so we restrict to a region in feature map 2 centered around the corresponding location of the patch in feature map 1. And `stride2` strides within this neighborhood region.

I recommend taking a look the ['FlowNet: Learning Optical Flow with Convolutional Networks'](https://arxiv.org/pdf/1504.06852.pdf ) paper that defined this type of layer.",IssueComment,https://github.com/apache/mxnet/issues/9034#issuecomment-404578327,thomelane,2018-07-12 16:52:15,9034,[12558],Documentation bug,1,"[code] must be odd number, otherwise strange things happen! It's mentioned in the docs string, but I think it would be better for this to be added as strict validation to avoid confusion like this in the future. With regards to the stride (once a odd [code] is used), [code] relates to the striding on the [code] feature map 1 and [code] relates to the striding within the 'neighborhood' defined on feature map 2. Without a neighborhood this method would compare all patches from feature map 1 with all patches from feature map 2. With large feature maps, the number of comparisons becomes too large, so we restrict to a region in feature map 2 centered around the corresponding location of the patch in feature map 1. And [code] strides within this neighborhood region. I recommend taking a look the ['FlowNet: Learning Optical Flow with Convolutional Networks']([url] ) paper that defined this type of layer."
"@dsqx71, @sxjscience, @eric-haibin-lin it would be good to get some input validation on `kernel_size` here to avoid this issue in the future.",IssueComment,https://github.com/apache/mxnet/issues/9034#issuecomment-404579001,thomelane,2018-07-12 16:54:31,9034,[12558],Documentation bug,1,"@dsqx71, @sxjscience, @eric-haibin-lin it would be good to get some input validation on [code] here to avoid this issue in the future."
"@sandeep-krishnamurthy can we now retag, thanks! I think Bug and Operator are the most appropriate tags.",IssueComment,https://github.com/apache/mxnet/issues/9034#issuecomment-404579552,thomelane,2018-07-12 16:56:17,9034,[12558],Documentation bug,1,"@sandeep-krishnamurthy can we now retag, thanks! I think Bug and Operator are the most appropriate tags."
"@mxnet-label-bot : [Bug, Operator]",IssueComment,https://github.com/apache/mxnet/issues/9034#issuecomment-415191770,ThomasDelteil,2018-08-22 21:46:20,9034,[12558],Documentation bug,1,"@mxnet-label-bot : [Bug, Operator]"
"Thank you for filing this issue.
@mxnet-label-bot [Bug, Gluon] ",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428617349,piyushghai,2018-10-10 15:26:58,12783,[12794],Code bug,0,"Thank you for filing this issue. @mxnet-label-bot [Bug, Gluon]"
"@lostella I tried running your example code provided, but I ran into the following error when I instantiated the block object : 

Can you have a look at your example code once more :) 

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-db4f3e2308e1> in <module>()
----> 1 block = MyBlock()
      2 block.initialize()
      3 block.hybridize()

<ipython-input-2-07390cb3caa7> in __init__(self)
      1 class MyBlock(mx.gluon.HybridBlock):
      2     def __init__(self):
----> 3         super().__init__()
      4         with self.name_scope():
      5             self.lstm = mx.gluon.rnn.HybridSequentialRNNCell()

TypeError: super() takes at least 1 argument (0 given)
``` 
",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428673942,piyushghai,2018-10-10 18:09:06,12783,[12794],Code bug,0,"@lostella I tried running your example code provided, but I ran into the following error when I instantiated the block object : Can you have a look at your example code once more :) ``[code]``"
@mxnet-label-bot [Pending Requester Info],IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428674640,piyushghai,2018-10-10 18:11:00,12783,[12794],Code bug,0,@mxnet-label-bot [Pending Requester Info]
"It runs fine with Python 3 (see my environment details above). Attaching it as a gist as well:

https://gist.github.com/lostella/261fd5d08dfb5e2054c4d01a7e2bc88e",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428718293,lostella,2018-10-10 20:25:18,12783,[12794],Code bug,0,It runs fine with Python 3 (see my environment details above). Attaching it as a gist as well: [url]
Aah. My bad here. Seems like my Jupyter Notebook was running python 2 as default kernel. ,IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428719798,piyushghai,2018-10-10 20:29:46,12783,[12794],Code bug,0,Aah. My bad here. Seems like my Jupyter Notebook was running python 2 as default kernel.
"@lostella Here are my findings on the issue : 

The symbol names are not getting saved properly in the symbol.json file generated. 
More specifically, with the unroll sequence length generates/replicates the same LSTM layer 'n' times, where n is the unroll seq length. 
Now if you closely examine the layer name in error : ```myblock0_lstm0__plus0_output``` it has a double``` '_'``` in between indicating something is amiss here. 
On further investigation, I found that a time stamp suffix : ```t0, t1``` etc indicating the unroll sequence number is missing here. To verify a quick fix, I opened up the symbol.json and manually added the time prefixes on places which were complaining about the error in imports method. 
I also had to fix the suffix issue in activation layers with name prefix as : ```myblock0_lstm<layer_number>_activation<time-stamp>``` to have correct values for timestamp.
eg : ```myblock0_lstm2_activation0```
After fixing the symbol.json file, the imports statement was working fine.

I will now investigate the root cause of this issue to fix it in code. 

Attached are gists of my working .ipynb notebook, and the corrected symbol.json file. 

https://gist.github.com/piyushghai/ad18f1290ec05d96ef5e9631474ae553
",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428762224,piyushghai,2018-10-10 23:03:26,12783,[12794],Code bug,0,"@lostella Here are my findings on the issue : The symbol names are not getting saved properly in the symbol.json file generated. More specifically, with the unroll sequence length generates/replicates the same LSTM layer 'n' times, where n is the unroll seq length. Now if you closely examine the layer name in error : ``[code]`[code]`[code]`[code]`[code]`[code]`[code]`[code]`[code]`` After fixing the symbol.json file, the imports statement was working fine. I will now investigate the root cause of this issue to fix it in code. Attached are gists of my working .ipynb notebook, and the corrected symbol.json file. [url]"
"I simplified the MWE: https://gist.github.com/lostella/9a790fd89726c1741a1fcf4194a5dac6

It seems like it's ultimately an LSTMCell problem.

```
import mxnet as mx

class MyBlock(mx.gluon.HybridBlock):
    def __init__(self):
        super().__init__()
        with self.name_scope():
            self.lstmcell = mx.gluon.rnn.LSTMCell(hidden_size=20)

    def hybrid_forward(self, F, seq):
        outputs, state = self.lstmcell.unroll(inputs=seq, length=10, layout=""NTC"", merge_outputs=True)
        return outputs

block = MyBlock()
block.initialize()
block.hybridize()

input = mx.nd.random_normal(shape=(32, 10, 5))
output = block(input)

block.export(path=""./model"", epoch=0)
symbol = mx.gluon.SymbolBlock.imports(
    symbol_file=""./model-symbol.json"",
    input_names=[f""data""],
    param_file=""./model-0000.params"",
    ctx=mx.Context.default_ctx
)
```",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-428921900,lostella,2018-10-11 11:34:30,12783,[12794],Code bug,0,I simplified the MWE: [url] It seems like it's ultimately an LSTMCell problem. ``[code]``
"For the first piece of code, the problem is not using the container's name_scope. Since HybridSequentialRNNCell is a container block, you need to use its name_scope if you intend to properly export it as a symbol.
```
class MyBlock(mx.gluon.HybridBlock):
    def __init__(self):
        super().__init__()
        with self.name_scope():
            self.lstm = mx.gluon.rnn.HybridSequentialRNNCell()
            with self.lstm.name_scope():
                for layer in range(3):
                    self.lstm.add(mx.gluon.rnn.LSTMCell(hidden_size=20))

    def hybrid_forward(self, F, seq):
        outputs, state = self.lstm.unroll(inputs=seq, length=10, layout=""NTC"", merge_outputs=True)
        return outputs
```",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429086305,szha,2018-10-11 19:23:02,12783,[12794],Code bug,0,"For the first piece of code, the problem is not using the container's name_scope. Since HybridSequentialRNNCell is a container block, you need to use its name_scope if you intend to properly export it as a symbol. ``[code]``"
"Unfortunately, that does not seem to solve the issue. See also the simpler example in my previous comment, which does not involve HybridSequentialRNNCell.",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429091366,lostella,2018-10-11 19:39:36,12783,[12794],Code bug,0,"Unfortunately, that does not seem to solve the issue. See also the simpler example in my previous comment, which does not involve HybridSequentialRNNCell."
"For the problem with LSTM alone, the problem is in not naming some of the elementwise operations. In LSTM there are three names that are repeating:
```
      ""name"": ""myblock0_lstm0__plus0"",
      ""name"": ""myblock0_lstm0__mul0"",
      ""name"": ""myblock0_lstm0__mul1"",
```
They come from the [plus0 here](https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/rnn/rnn_cell.py#L514), and the [mul0 and mul1 here](https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/rnn/rnn_cell.py#L524)

The fix should be to replace these operations with F.elemwise_X, with the proper prefix just like other operators.",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429092412,szha,2018-10-11 19:43:02,12783,[12794],Code bug,0,"For the problem with LSTM alone, the problem is in not naming some of the elementwise operations. In LSTM there are three names that are repeating: ``[code]`` They come from the [plus0 here]([url]#L514), and the [mul0 and mul1 here]([url]#L524) The fix should be to replace these operations with F.elemwise_X, with the proper prefix just like other operators."
"This problem exists in RNN and GRU as well, so all three needs to be patched.",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429092674,szha,2018-10-11 19:43:52,12783,[12794],Code bug,0,"This problem exists in RNN and GRU as well, so all three needs to be patched."
Similar error occurred in https://github.com/apache/incubator-mxnet/issues/11542,IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429097068,vandanavk,2018-10-11 19:57:36,12783,[12794],Code bug,0,Similar error occurred in [url]
"@szha yes, thanks. That's basically #12794.",IssueComment,https://github.com/apache/mxnet/issues/12783#issuecomment-429128509,lostella,2018-10-11 21:40:42,12783,[12794],Code bug,0,"@szha yes, thanks. That's basically #12794."
"@haven-jeon Thank you submitting this issue. We will look into this

@mxnet-label-bot [Bug, Gluon]",IssueComment,https://github.com/apache/mxnet/issues/12778#issuecomment-428442992,piyushghai,2018-10-10 05:30:59,12778,[12796],Visualization bug,0,"@haven-jeon Thank you submitting this issue. We will look into this @mxnet-label-bot [Bug, Gluon]"
"https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/visualization.py

It seems that the Embedding layers is currently missing. 
So It needs to be added at Line 162 as below.

```
elif op == 'Embedding':
    cur_param = int(node[""attrs""]['input_dim']) * int(node[""attrs""]['output_dim'])
````",IssueComment,https://github.com/apache/mxnet/issues/12778#issuecomment-428455018,soeque1,2018-10-10 06:36:45,12778,[12796],Visualization bug,0,[url] It seems that the Embedding layers is currently missing. So It needs to be added at Line 162 as below. ``[code]```
@soeque1 Thank you for your suggestion! Maybe you can raise a PR for this fix and contribute to the development of MXNet community :) ,IssueComment,https://github.com/apache/mxnet/issues/12778#issuecomment-428644588,piyushghai,2018-10-10 16:42:56,12778,[12796],Visualization bug,0,@soeque1 Thank you for your suggestion! Maybe you can raise a PR for this fix and contribute to the development of MXNet community :)
Thank you for the quick reply. I will PR soon :),IssueComment,https://github.com/apache/mxnet/issues/12778#issuecomment-428822485,soeque1,2018-10-11 05:09:05,12778,[12796],Visualization bug,0,Thank you for the quick reply. I will PR soon :)
"I compared versions.

mac 
======
maven 3.5.3
java 1.8.0_171
scala 2.12.6

ubuntu
=====
maven 3.3.9
java 1.8.0_162
scala 2.11.6
sbt 1.1.5

I believe the fix for this - to get Mac and Ubuntu on the same version scala, and this will fix the Mac docs build, by getting everything to work with the latest version of scala:

1. Remove index and package.html from mxdoc.py line 89
2. For CI and general Ubuntu deps, run the following:
```
sudo apt-get remove scala-library scala
sudo wget http://scala-lang.org/files/archive/scala-2.12.6.deb
sudo dpkg -i scala-2.12.6.deb
```

You go from 2000 and [late](https://mxnet.incubator.apache.org/api/scala/docs/index.html#org.apache.mxnet.Accuracy
).
To 2000 and [next](http://54.172.99.8/api/scala/docs/org/apache/mxnet/Accuracy.html
).",IssueComment,https://github.com/apache/mxnet/issues/10858#issuecomment-388176530,aaronmarkham,2018-05-10 20:32:15,10858,[13071],Documentation bug,0,"I compared versions. mac ====== maven 3.5.3 java 1.8.0_171 scala 2.12.6 ubuntu ===== maven 3.3.9 java 1.8.0_162 scala 2.11.6 sbt 1.1.5 I believe the fix for this - to get Mac and Ubuntu on the same version scala, and this will fix the Mac docs build, by getting everything to work with the latest version of scala: 1. Remove index and package.html from mxdoc.py line 89 2. For CI and general Ubuntu deps, run the following: ``[code]`` You go from 2000 and [late]([url]#org.apache.mxnet.Accuracy ). To 2000 and [next]([url] )."
"Is adding platform check in mxdoc.py could be a solution to this for short term? I think we really need a version control for dependencies of docs, update them on a timely manner.",IssueComment,https://github.com/apache/mxnet/issues/10858#issuecomment-388181872,lanking520,2018-05-10 20:51:26,10858,[13071],Documentation bug,0,"Is adding platform check in mxdoc.py could be a solution to this for short term? I think we really need a version control for dependencies of docs, update them on a timely manner."
"This will pin Scala. Sbt and Maven should be pinned too. Would need to dig further on how that would work for those. Also, even for this Scala upgrade, we'd want to see if it has any impact. ",IssueComment,https://github.com/apache/mxnet/issues/10858#issuecomment-388491205,aaronmarkham,2018-05-11 21:32:27,10858,[13071],Documentation bug,0,"This will pin Scala. Sbt and Maven should be pinned too. Would need to dig further on how that would work for those. Also, even for this Scala upgrade, we'd want to see if it has any impact."
"@sandeep-krishnamurthy I don't think this is a Doc issue. This is a bug in building docs, so I'd recommend removing Doc and Scala label and adding Bug and Build labels.",IssueComment,https://github.com/apache/mxnet/issues/10858#issuecomment-408967240,safrooze,2018-07-30 18:39:26,10858,[13071],Documentation bug,0,"@sandeep-krishnamurthy I don't think this is a Doc issue. This is a bug in building docs, so I'd recommend removing Doc and Scala label and adding Bug and Build labels."
"@mxnet-label-bot [Doc, Website]",IssueComment,https://github.com/apache/mxnet/issues/12318#issuecomment-415594184,ankkhedia,2018-08-23 22:42:33,12318,[13259],Deployment bug,0,"@mxnet-label-bot [Doc, Website]"
Hi @marcoabreu - this is more of a Python issue on the engineering side than something that can be fixed from within the website or docs systems. I might even label it a bug.,IssueComment,https://github.com/apache/mxnet/issues/12318#issuecomment-415595609,aaronmarkham,2018-08-23 22:49:22,12318,[13259],Deployment bug,0,Hi @marcoabreu - this is more of a Python issue on the engineering side than something that can be fixed from within the website or docs systems. I might even label it a bug.
"@sandeep-krishnamurthy Could you please label above as [Python, Build, Bug] and remove [Doc, Website]",IssueComment,https://github.com/apache/mxnet/issues/12318#issuecomment-415598762,ankkhedia,2018-08-23 23:00:30,12318,[13259],Deployment bug,0,"@sandeep-krishnamurthy Could you please label above as [Python, Build, Bug] and remove [Doc, Website]"
@aaronmarkham : What needs to be done here for the docs? ,IssueComment,https://github.com/apache/mxnet/issues/12318#issuecomment-437560591,vdantu,2018-11-10 05:46:26,12318,[13259],Deployment bug,0,@aaronmarkham : What needs to be done here for the docs?
@nswamy ,IssueComment,https://github.com/apache/mxnet/issues/13141#issuecomment-436389482,azai91,2018-11-06 20:03:15,13141,[13328],Version compatibility bug,0,@nswamy
@azai91 could you help to add a testcase for this issue after the #12953 is merged?,IssueComment,https://github.com/apache/mxnet/issues/13141#issuecomment-436511160,pengzhao-intel,2018-11-07 05:40:24,13141,[13328],Version compatibility bug,0,@azai91 could you help to add a testcase for this issue after the #12953 is merged?
"@azai91 PR is merged, go ahead to create a test for this issue 👍 
Feel free to let me know if any help is needed.

@TaoLv 

Original: MKL-DNN 0.14 CI: 498e03da7ce420a48050fcd363aacb8dec2516f7
```
patric@mlt-skx122 mxnet]$ python soft.py
/home/patric/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[
[[[[nan nan]]]]
<NDArray 1x1x1x2 @cpu(0)>]

```
Now: MKL-DNN 0.17 CI: a32fa840262cdad5e7556a53d7ce3d7218ae7120
```
[patric@mlt-skx122 mxnet]$ python soft.py
/home/patric/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[
[[[[1. 1.]]]]
<NDArray 1x1x1x2 @cpu(0)>]
```
",IssueComment,https://github.com/apache/mxnet/issues/13141#issuecomment-436872748,pengzhao-intel,2018-11-08 04:51:02,13141,[13328],Version compatibility bug,0,"@azai91 PR is merged, go ahead to create a test for this issue 👍 Feel free to let me know if any help is needed. @TaoLv Original: MKL-DNN 0.14 CI: 498e03da7ce420a48050fcd363aacb8dec2516f7 ``[code]float[code]np.floating[code]np.float64 == np.dtype(float).type[code]`[code]`[code]float[code]np.floating[code]np.float64 == np.dtype(float).type[code]``"
@mxnet-label-bot [MKLDNN],IssueComment,https://github.com/apache/mxnet/issues/13141#issuecomment-436878425,pengzhao-intel,2018-11-08 05:31:31,13141,[13328],Version compatibility bug,0,@mxnet-label-bot [MKLDNN]
Thanks for reporting the issue and proposing a fix too @wangxinanbook. You could go ahead and raise a PR with this proposed fix. Please verify if all the ONNX import tests pass (tests/python-pytest/onnx/import/mxnet_backend_test.py),IssueComment,https://github.com/apache/mxnet/issues/13300#issuecomment-439467501,vandanavk,2018-11-16 17:30:41,13300,[13413],Deployment bug,0,Thanks for reporting the issue and proposing a fix too @wangxinanbook. You could go ahead and raise a PR with this proposed fix. Please verify if all the ONNX import tests pass (tests/python-pytest/onnx/import/mxnet_backend_test.py)
"@mxnet-label-bot add [ONNX, Bug]",IssueComment,https://github.com/apache/mxnet/issues/13300#issuecomment-439467746,vandanavk,2018-11-16 17:31:36,13300,[13413],Deployment bug,0,"@mxnet-label-bot add [ONNX, Bug]"
Sure @vandanavk ,IssueComment,https://github.com/apache/mxnet/issues/13300#issuecomment-439670890,wangxinanbook,2018-11-18 06:21:27,13300,[13413],Deployment bug,0,Sure @vandanavk
"I ran the test. It came out another issue on `test_clip_default_min_cpu`,  even I used the master code.  I think this should be fixed elsewhere.  How do you think ?  @vandanavk 
```
Traceback (most recent call last):
  File ""/opt/home/wangxinan/anaconda3/envs/test_mxnet/lib/python3.6/site-packages/onnx/backend/test/runner/__init__.py"", line 243, in device_test_func
    return test_func(*args, device=device, **kwargs)
  File ""/opt/home/wangxinan/anaconda3/envs/test_mxnet/lib/python3.6/site-packages/onnx/backend/test/runner/__init__.py"", line 273, in run
    prepared_model = self.backend.prepare(model, device)
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/tests/python-pytest/onnx/import/mxnet_backend.py"", line 57, in prepare
    sym, arg_params, aux_params = graph.from_onnx(model.graph)
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/python/mxnet/contrib/onnx/onnx2mx/import_onnx.py"", line 131, in from_onnx
    raise err
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/python/mxnet/contrib/onnx/onnx2mx/import_onnx.py"", line 128, in from_onnx
    mxnet_sym = self._convert_operator(node_name, op_name, onnx_attr, inputs)
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/python/mxnet/contrib/onnx/onnx2mx/import_onnx.py"", line 69, in _convert_operator
    mxnet_sym = new_op(*inputs, **new_attrs)
  File ""<string>"", line 73, in clip
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/python/mxnet/_ctypes/symbol.py"", line 125, in _symbol_creator
    ctypes.byref(sym_handle)))
  File ""/opt/home/wangxinan/workspace/incubator-mxnet/python/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: Invalid Parameter format for a_max expect float but value='inf', in operator clip(name="""", a_max=""inf"", a_min=""0.0"")
```",IssueComment,https://github.com/apache/mxnet/issues/13300#issuecomment-439693344,wangxinanbook,2018-11-18 13:36:41,13300,[13413],Deployment bug,0,"I ran the test. It came out another issue on [code], even I used the master code. I think this should be fixed elsewhere. How do you think ? @vandanavk ``[code]``"
@wangxinanbook I don't see this error. It seems to pass on my setup,IssueComment,https://github.com/apache/mxnet/issues/13300#issuecomment-439999600,vandanavk,2018-11-19 18:45:49,13300,[13413],Deployment bug,0,@wangxinanbook I don't see this error. It seems to pass on my setup
"@YutingZhang Seems like it's caused by jupyter since it may cache the sessions?
I've tried it in terminal and the processes are gabage collected just fine.",IssueComment,https://github.com/apache/mxnet/issues/13521#issuecomment-444236096,zhreshold,2018-12-04 19:56:44,13521,[13537],Data bug,0,@YutingZhang Seems like it's caused by jupyter since it may cache the sessions? I've tried it in terminal and the processes are gabage collected just fine.
"@YutingZhang Okay, I found the problem is present on linux but not mac. 
As discussed offline, it's better to secure the terminate manually, I will file a PR regarding this. ",IssueComment,https://github.com/apache/mxnet/issues/13521#issuecomment-444259240,zhreshold,2018-12-04 21:09:56,13521,[13537],Data bug,0,"@YutingZhang Okay, I found the problem is present on linux but not mac. As discussed offline, it's better to secure the terminate manually, I will file a PR regarding this."
"@zhreshold Great. Thanks!
FYI, I tested it on my Mac using anaconda Python3.6. It also caused problems. Maybe caused by the anaconda version of python?",IssueComment,https://github.com/apache/mxnet/issues/13521#issuecomment-444314392,YutingZhang,2018-12-05 00:43:10,13521,[13537],Data bug,0,"@zhreshold Great. Thanks! FYI, I tested it on my Mac using anaconda Python3.6. It also caused problems. Maybe caused by the anaconda version of python?"
@zhreshold Confirmed this as a python bug: https://bugs.python.org/issue34172,IssueComment,https://github.com/apache/mxnet/issues/13521#issuecomment-444813703,YutingZhang,2018-12-06 09:52:29,13521,[13537],Data bug,0,@zhreshold Confirmed this as a python bug: [url]
"@YutingZhang Good to know, thanks",IssueComment,https://github.com/apache/mxnet/issues/13521#issuecomment-444988007,zhreshold,2018-12-06 18:59:39,13521,[13537],Data bug,0,"@YutingZhang Good to know, thanks"
"@mxnet-label-bot add [gluon, python, bug]",IssueComment,https://github.com/apache/mxnet/issues/13710#issuecomment-449544835,andrewfayres,2018-12-22 04:42:52,13710,[13727],Memory bug,0,"@mxnet-label-bot add [gluon, python, bug]"
"I have met the same problem and tried to find the exact place the bug happened.

ENV:
```
Centos 7.0
Python 2.7.5
mxnet 1.3.0
gluon-cv 0.3.0
````

Minimal script to reproduce the bug:
```
import numpy as np
import mxnet as mx
from mxnet import autograd
from gluoncv.model_zoo import get_model
from gluoncv import data as gdata
from gluoncv.data.transforms.presets.ssd import SSDDefaultTrainTransform

net_name = 'ssd_300_vgg16_atrous_voc'
net = get_model(net_name, pretrained_base=False)
net.initialize()

width = 300
height = 300
with autograd.train_mode():
    _, _, anchors = net(mx.nd.zeros((1, 3, height, width)))
trans = SSDDefaultTrainTransform(height, width, anchors)

# the bug happens when transform voc 2007 data, here I only create a random data
image = mx.nd.uniform(low=0, high=255, shape=(500, 400, 3)).astype('uint8')
box = np.array([[ 47., 239., 194., 370.,  11.,   0.],
                [  7.,  11., 351., 497.,  14.,   0.]])
for _ in range(10):
    trans(image, box)
```

The backtrace of gdb on debug version mxnet
```
#0  0x00007ffff6d53277 in raise () at /lib64/libc.so.6
#1  0x00007ffff6d54968 in abort () at /lib64/libc.so.6
#2  0x00007ffff6d95d37 in __libc_message () at /lib64/libc.so.6
#3  0x00007ffff6d9fc86 in _int_malloc () at /lib64/libc.so.6
#4  0x00007ffff6da284c in malloc () at /lib64/libc.so.6
#5  0x00007fffd8b84ecd in operator new(unsigned long) () at /lib64/libstdc++.so.6
#6  0x00007fff9697f27c in __gnu_cxx::new_allocator<unsigned long>::allocate(unsigned long, void const*) (this=0x7fff6b45b760, __n=17464)
    at /usr/include/c++/4.8.2/ext/new_allocator.h:104
#7  0x00007fff96aa2835 in std::_Vector_base<unsigned long, std::allocator<unsigned long> >::_M_allocate(unsigned long) (this=0x7fff6b45b760, __n=17464)
    at /usr/include/c++/4.8.2/bits/stl_vector.h:168
#8  0x00007fff96aa807f in std::_Vector_base<unsigned long, std::allocator<unsigned long> >::_M_create_storage(unsigned long) (this=0x7fff6b45b760, __n=17464)
    at /usr/include/c++/4.8.2/bits/stl_vector.h:181
#9  0x00007fff96aa3999 in std::_Vector_base<unsigned long, std::allocator<unsigned long> >::_Vector_base(unsigned long, std::allocator<unsigned long> const&) (this=0x7fff6b45b760, __n=17464, __a=...) at /usr/include/c++/4.8.2/bits/stl_vector.h:136
#10 0x00007fff96aa041c in std::vector<unsigned long, std::allocator<unsigned long> >::vector(unsigned long, std::allocator<unsigned long> const&) (this=0x7fff6b45b760, __n=17464, __a=...) at /usr/include/c++/4.8.2/bits/stl_vector.h:270
#11 0x00007fff96a9ca55 in mxnet::op::SortByKey<int, float>(mshadow::Tensor<mshadow::cpu, 1, int>, mshadow::Tensor<mshadow::cpu, 1, float>, bool, mshadow::Tensor<mshadow::cpu, 1, char>*, int, int) (keys=..., values=..., is_ascend=true, workspace=0x0, begin_bit=0, end_bit=32) at src/operator/contrib/./../tensor/sort_op.h:50
#12 0x00007fff96a8a08d in mxnet::op::BipartiteMatchingForward<mshadow::cpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) (attrs=..., ctx=..., inputs=std::vector of length 1, capacity 1 = {...}, req=std::vector of length 2, capacity 2 = {...}, outputs=std::vector of length 2, capacity 2 = {...})
    at src/operator/contrib/./bounding_box-inl.h:779
#13 0x00007fff96861e8d in std::_Function_handler<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&), void (*)(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::_M_invoke(std::_Any_data const&, nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) (__functor=..., __args#0=..., __args#1=..., __args#2=std::vector of length 1, capacity 1 = {...}, __args#3=std::vector of length 2, capacity 2 = {...}, __args#4=std::vector of length 2, capacity 2 = {...}) at /usr/include/c++/4.8.2/functional:2071
#14 0x00007fff98c896b8 in std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::operator()(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) const (this=0x1eb15d8, __args#0=..., __args#1=..., __args#2=std::vector of length 1, capacity 1 = {...}, __args#3=std::vector of length 2, capacity 2 = {...}, __args#4=std::vector of length 2, capacity 2 = {...})
    at /usr/include/c++/4.8.2/functional:2471
#15 0x00007fff98dae979 in mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const (__closure=0x1eb1550, rctx=...)
    at src/imperative/./imperative_utils.h:401
#16 0x00007fff98db4649 in std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext) (__functor=..., __args#0=...) at /usr/include/c++/4.8.2/functional:2071
#17 0x00007fff99481517 in std::function<void (mxnet::RunContext)>::operator()(mxnet::RunContext) const (this=0x1e5fd20, __args#0=...)
    at /usr/include/c++/4.8.2/functional:2471
#18 0x00007fff9949245a in mxnet::engine::ThreadedEngine::__lambda26::operator()(mxnet::RunContext, mxnet::Engine::CallbackOnComplete) const (__closure=0x1e5fd20, ctx=..., on_complete=...) at src/engine/threaded_engine.cc:342
#19 0x00007fff99493523 in std::_Function_handler<void(mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::PushSync(mxnet::Engine::SyncFn, mxnet::Context, const std::vector<mxnet::engine::Var*>&, const std::vector<mxnet::engine::Var*>&, mxnet::FnProperty, int, char const*)::__lambda26>::_M_invoke(const std::_Any_data &, mxnet::RunContext, mxnet::engine::CallbackOnComplete) (__functor=..., __args#0=..., __args#1=...) at /usr/include/c++/4.8.2/functional:2071
#20 0x00007fff9948218f in std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (this=0x1d294b0, __args#0=..., __args#1=...) at /usr/include/c++/4.8.2/functional:2471
#21 0x00007fff99487980 in mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*) (this=0x1121140, run_ctx=..., opr_block=0x112a6b8)
    at src/engine/./threaded_engine.h:363
#22 0x00007fff9949b103 in mxnet::engine::ThreadedEnginePerDevice::CPUWorker<(dmlc::ConcurrentQueueType)0>(mxnet::Context, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0>*, std::shared_ptr<dmlc::ManualEvent> const&) (this=0x1121140, ctx=..., block=0x18ac100, ready_event=std::shared_ptr (count 2, weak 0) 0x1b886c8) at src/engine/threaded_engine_perdevice.cc:296
#23 0x00007fff99499491 in mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}::operator()(dmlc::ManualEvent) const (__closure=0x1b95f20, ready_event=std::shared_ptr (count 2, weak 0) 0x1b886c8)
    at src/engine/threaded_engine_perdevice.cc:116
```

The bug is related malloc, it disappears when you change the malloc implementation. I'm not sure if there is still some hidden bug.
```
LD_PRELOAD=/usr/lib64/libtcmalloc.so python reproduce_bug.py
```",IssueComment,https://github.com/apache/mxnet/issues/13710#issuecomment-449760975,arcadiaphy,2018-12-24 18:16:10,13710,[13727],Memory bug,0,I have met the same problem and tried to find the exact place the bug happened. ENV: ``[code]``[code]`[code]`[code]`[code]`[code]`[code]``
"@wsqshiqing A PR regarding has been created, requesting you pull it in and verify it resolves the issue for you? So we can close this issue once the PR is merged",IssueComment,https://github.com/apache/mxnet/issues/13710#issuecomment-450031608,vrakesh,2018-12-26 21:52:40,13710,[13727],Memory bug,0,"@wsqshiqing A PR regarding has been created, requesting you pull it in and verify it resolves the issue for you? So we can close this issue once the PR is merged"
@vrakesh It can solve my issue.,IssueComment,https://github.com/apache/mxnet/issues/13710#issuecomment-450277351,ghost,2018-12-28 02:49:28,13710,[13727],Memory bug,0,@vrakesh It can solve my issue.
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: ",IssueComment,https://github.com/apache/mxnet/issues/13740#issuecomment-450301956,mxnet-label-bot,2018-12-28 06:50:46,13740,[13796],Visualization bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels:"
And the link to the environment variable page is found in https://mxnet.incubator.apache.org/faq/,IssueComment,https://github.com/apache/mxnet/issues/13740#issuecomment-450302006,szha,2018-12-28 06:51:13,13740,[13796],Visualization bug,0,And the link to the environment variable page is found in [url]
"@mxnet-label-bot [Operator, Doc, Bug]",IssueComment,https://github.com/apache/mxnet/issues/13078#issuecomment-435106826,anirudhacharya,2018-11-01 16:51:59,13078,[14035],Documentation bug,1,"@mxnet-label-bot [Operator, Doc, Bug]"
"@mxnet-label-bot [Call for Contribution, Good First Issue]",IssueComment,https://github.com/apache/mxnet/issues/13078#issuecomment-447600906,apeforest,2018-12-15 22:02:39,13078,[14035],Documentation bug,1,"@mxnet-label-bot [Call for Contribution, Good First Issue]"
"@eric-haibin-lin I tried reproducing the issue on MXNet v1.3.0 and I could not see the same error : 
```
Python 2.7.15 |Anaconda custom (64-bit)| (default, May  1 2018, 18:37:05) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import mxnet as mx
/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
>>> mx.__version__
'1.3.0'
>>> 
>>> 
>>> a = mx.nd.ones((256*35, 1024*1024))
>>> b = mx.nd.ones((256*35,))
>>> mx.nd.pick(a,b)

[1. 1. 1. ... 1. 1. 1.]
<NDArray 8960 @cpu(0)>
>>> 
```

Can you try to see if you still get this issue ? 
",IssueComment,https://github.com/apache/mxnet/issues/9314#issuecomment-428343066,piyushghai,2018-10-09 20:39:28,9314,[14082],Data bug,1,@eric-haibin-lin I tried reproducing the issue on MXNet v1.3.0 and I could not see the same error : ``[code]float[code]np.floating[code]np.float64 == np.dtype(float).type[code]`` Can you try to see if you still get this issue ?
"How many times did you try? I'm still getting this error. Another attempt on Mac:
```
➜  Documents python
Python 2.7.13 (default, Dec 18 2016, 07:03:39)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import mxnet as mx

>>> a = mx.nd.ones((256*35, 1024*1024))
>>> b = mx.nd.ones((256*35,))
>>>
>>> mx.nd.pick(a,b)

Segmentation fault: 11

Stack trace returned 10 entries:
[bt] (0) 0   libmxnet.so                         0x0000000116231c90 libmxnet.so + 15504
[bt] (1) 1   libmxnet.so                         0x0000000117da3956 MXTVMBridge + 4726662
[bt] (2) 2   libsystem_platform.dylib            0x00007fffbc8c6b3a _sigtramp + 26
[bt] (3) 3   ???                                 0x0000000000000000 0x0 + 0
[bt] (4) 4   libmxnet.so                         0x000000011665782e libmxnet.so + 4364334
[bt] (5) 5   libmxnet.so                         0x00000001177ebfed MXNDListFree + 620189
[bt] (6) 6   libmxnet.so                         0x00000001177658e4 MXNDListFree + 69524
[bt] (7) 7   libmxnet.so                         0x0000000117767e68 MXNDListFree + 79128
[bt] (8) 8   libmxnet.so                         0x000000011776b021 MXNDListFree + 91857
[bt] (9) 9   libmxnet.so                         0x000000011776af3f MXNDListFree + 91631
libc++abi.dylib: terminating
[1]    22838 abort      python
➜  Documents pip2 list | grep mx

mxnet (1.3.1b20181012)
```",IssueComment,https://github.com/apache/mxnet/issues/9314#issuecomment-429511469,eric-haibin-lin,2018-10-13 04:58:57,9314,[14082],Data bug,1,How many times did you try? I'm still getting this error. Another attempt on Mac: ``[code]``
"@mxnet-label-bot [Bug, ONNX]",IssueComment,https://github.com/apache/mxnet/issues/13061#issuecomment-434744206,frankfliu,2018-10-31 15:59:33,13061,[14121],Deployment bug,0,"@mxnet-label-bot [Bug, ONNX]"
@vandanavk ,IssueComment,https://github.com/apache/mxnet/issues/13061#issuecomment-434766156,anirudhacharya,2018-10-31 16:58:20,13061,[14121],Deployment bug,0,@vandanavk
PR https://github.com/apache/incubator-mxnet/pull/14121,IssueComment,https://github.com/apache/mxnet/issues/13061#issuecomment-462510904,vandanavk,2019-02-11 21:57:04,13061,[14121],Deployment bug,0,PR [url]
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Example",IssueComment,https://github.com/apache/mxnet/issues/14199#issuecomment-464946985,mxnet-label-bot,2019-02-19 01:52:35,14199,[14212],Processor bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Example"
"@mxnet-label-bot add [Bug, python]",IssueComment,https://github.com/apache/mxnet/issues/14199#issuecomment-465220955,frankfliu,2019-02-19 17:03:39,14199,[14212],Processor bug,0,"@mxnet-label-bot add [Bug, python]"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Scala, Bug",IssueComment,https://github.com/apache/mxnet/issues/14181#issuecomment-464243381,mxnet-label-bot,2019-02-15 23:20:02,14181,[14215],Code bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Scala, Bug"
"@mxnet-label-bot add [Bug, Scala]",IssueComment,https://github.com/apache/mxnet/issues/14181#issuecomment-464243736,andrewfayres,2019-02-15 23:21:23,14181,[14215],Code bug,0,"@mxnet-label-bot add [Bug, Scala]"
"@mxnet-label-bot add [Good First Issue, Call for Contribution]",IssueComment,https://github.com/apache/mxnet/issues/13220#issuecomment-438492169,apeforest,2018-11-14 00:32:43,13220,[14251],Data bug,1,"@mxnet-label-bot add [Good First Issue, Call for Contribution]"
"Thanks for reporting the issue @eric-haibin-lin 
Could you also add a link to the relevant place in the code where changes need to be made?",IssueComment,https://github.com/apache/mxnet/issues/13220#issuecomment-438783535,kalyc,2018-11-14 19:14:36,13220,[14251],Data bug,1,Thanks for reporting the issue @eric-haibin-lin Could you also add a link to the relevant place in the code where changes need to be made?
"Hard to trace. Can't find the  gen_op module in the current package while working on this line:
`from .gen_op import * # pylint: disable=unused-wildcard-import`. ",IssueComment,https://github.com/apache/mxnet/issues/13220#issuecomment-441151703,hanzhaogang,2018-11-23 03:50:44,13220,[14251],Data bug,1,Hard to trace. Can't find the gen_op module in the current package while working on this line: [code].
"It's most likely due to the float conversion in https://github.com/apache/incubator-mxnet/blob/f5ba2678f077b58d31d31029f680f93b313e1cea/src/operator/math_functions-inl.h#L43-L61 
",IssueComment,https://github.com/apache/mxnet/issues/13220#issuecomment-442255325,eric-haibin-lin,2018-11-27 23:16:33,13220,[14251],Data bug,1,It's most likely due to the float conversion in [url]#L43-L61
@anirudh2290 Please tag PR-work-in-progress,IssueComment,https://github.com/apache/mxnet/issues/13220#issuecomment-483399210,Vikas-kum,2019-04-15 20:08:17,13220,[14251],Data bug,1,@anirudh2290 Please tag PR-work-in-progress
"@apache/mxnet-committers: This issue has been inactive for the past 90 days. It has no label and needs triage.

For general ""how-to"" questions, our [user forum](https://discuss.mxnet.io/) (and [Chinese version](https://discuss.gluon.ai/)) is a good place to get help.",IssueComment,https://github.com/apache/mxnet/issues/7853#issuecomment-353686110,szha,2017-12-22 22:44:42,7853,[14273],Data bug,0,"@apache/mxnet-committers: This issue has been inactive for the past 90 days. It has no label and needs triage. For general ""how-to"" questions, our [user forum]([url] (and [Chinese version]([url] is a good place to get help."
"Error on MXNet v1.5 (built from source). Tested on OSX
```
---------------------------------------------------------------------------
MXNetError                                Traceback (most recent call last)
<ipython-input-13-b04ee09a6160> in <module>()
     12      element = nd.choose_element_0index(w.data(), nd.array([0]))
     13      y = nd.square(element)
---> 14      y.backward()

~/Documents/mxnet/incubator-mxnet/python/mxnet/ndarray/ndarray.py in backward(self, out_grad, retain_graph, train_mode)
   2213             ctypes.c_int(train_mode),
   2214             ctypes.c_void_p(0),
-> 2215             ctypes.c_void_p(0)))
   2216 
   2217     def tostype(self, stype):

~/Documents/mxnet/incubator-mxnet/python/mxnet/base.py in check_call(ret)
    250     """"""
    251     if ret != 0:
--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))
    253 
    254 

MXNetError: [15:24:46] src/pass/gradient.cc:192: Operator choose_element_0index is non-differentiable because it didn't register FGradient attribute.

Stack trace returned 10 entries:
[bt] (0) 0   libmxnet.so                         0x000000011232dbd6 dmlc::StackTrace() + 1238
[bt] (1) 1   libmxnet.so                         0x000000011232d5c5 dmlc::LogMessageFatal::~LogMessageFatal() + 53
[bt] (2) 2   libmxnet.so                         0x00000001162009ea nnvm::pass::(anonymous namespace)::Gradient(nnvm::Graph) + 13066
[bt] (3) 3   libmxnet.so                         0x00000001156de032 nnvm::Graph std::__1::__invoke_void_return_wrapper<nnvm::Graph>::__call<nnvm::Graph (*&)(nnvm::Graph), nnvm::Graph>(nnvm::Graph (*&&&)(nnvm::Graph), nnvm::Graph&&) + 162
[bt] (4) 4   libmxnet.so                         0x00000001156dded0 std::__1::__function::__func<nnvm::Graph (*)(nnvm::Graph), std::__1::allocator<nnvm::Graph (*)(nnvm::Graph)>, nnvm::Graph (nnvm::Graph)>::operator()(nnvm::Graph&&) + 64
[bt] (5) 5   libmxnet.so                         0x00000001161e5e18 nnvm::ApplyPasses(nnvm::Graph, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) + 1448
[bt] (6) 6   libmxnet.so                         0x000000011512548e nnvm::ApplyPass(nnvm::Graph, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 590
[bt] (7) 7   libmxnet.so                         0x00000001152376fc nnvm::pass::Gradient(nnvm::Graph, std::__1::vector<nnvm::NodeEntry, std::__1::allocator<nnvm::NodeEntry> >, std::__1::vector<nnvm::NodeEntry, std::__1::allocator<nnvm::NodeEntry> >, std::__1::vector<nnvm::NodeEntry, std::__1::allocator<nnvm::NodeEntry> >, std::__1::function<nnvm::NodeEntry (std::__1::vector<nnvm::NodeEntry, std::__1::allocator<nnvm::NodeEntry> >&&)>, std::__1::function<int (nnvm::Node const&)>, std::__1::function<nnvm::NodeEntry (nnvm::NodeEntry const&, nnvm::NodeEntry const&)>, std::__1::vector<nnvm::Op const*, std::__1::allocator<nnvm::Op const*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) + 8764
[bt] (8) 8   libmxnet.so                         0x000000011537a8c5 mxnet::Imperative::Backward(std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, std::__1::vector<mxnet::NDArray*, std::__1::allocator<mxnet::NDArray*> > const&, bool, bool, bool) + 13445
[bt] (9) 9   libmxnet.so                         0x00000001150d434d MXAutogradBackwardEx + 3293
```

",IssueComment,https://github.com/apache/mxnet/issues/7853#issuecomment-463850347,vandanavk,2019-02-14 23:51:52,7853,[14273],Data bug,0,Error on MXNet v1.5 (built from source). Tested on OSX ``[code]``
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Build",IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-466514574,mxnet-label-bot,2019-02-22 19:19:26,14236,[14274],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Build"
"@mxnet-label-bot add [Build, Bug]",IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-466522603,frankfliu,2019-02-22 19:44:15,14236,[14274],Build bug,0,"@mxnet-label-bot add [Build, Bug]"
Thanks to raising the issue. @xinyu-intel could you help take a look?,IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-467004795,pengzhao-intel,2019-02-25 13:08:58,14236,[14274],Build bug,0,Thanks to raising the issue. @xinyu-intel could you help take a look?
@mxnet-label-bot add [MKLDNN],IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-467005064,pengzhao-intel,2019-02-25 13:09:54,14236,[14274],Build bug,0,@mxnet-label-bot add [MKLDNN]
@samskalicky  you can workaround by using: make -j1,IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-467759361,rongzha1,2019-02-27 07:50:33,14236,[14274],Build bug,0,@samskalicky you can workaround by using: make -j1
"Thanks guys!

Ive tried adding "" | mkldnn "" to the Makefile here:

https://github.com/apache/incubator-mxnet/blob/master/Makefile#L492

that seems to be working. This is similar to another target above:

https://github.com/apache/incubator-mxnet/blob/master/Makefile#L476

Any thoughts about committing this change into the makefile?",IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-467950847,samskalicky,2019-02-27 17:18:34,14236,[14274],Build bug,0,"Thanks guys! Ive tried adding "" | mkldnn "" to the Makefile here: [url]#L492 that seems to be working. This is similar to another target above: [url]#L476 Any thoughts about committing this change into the makefile?"
Why do we need warpctc if mxnet already has native implementation?,IssueComment,https://github.com/apache/mxnet/issues/14236#issuecomment-468116777,apeforest,2019-02-28 02:53:59,14236,[14274],Build bug,0,Why do we need warpctc if mxnet already has native implementation?
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-468896278,mxnet-label-bot,2019-03-02 07:34:26,14301,[14302],Algorithm design bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"Thanks for your report!

reproduce the bug in MXNet fb4f9d5

It is strange that the address of `ctx.requested[softmaxout_enum::kTempSpace]` is 0 in `src/operator/softmax_output-inl.h   `.
`ctx.requested.size()` is 0 in Backward.

It is a bug that `BackwardResource` is not called when `ex.backward(is_train=true)` is called.
I do not know why SoftmaxOutput is not a legacy operator.",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-468901529,wkcn,2019-03-02 08:51:09,14301,[14302],Algorithm design bug,1,Thanks for your report! reproduce the bug in MXNet fb4f9d5 It is strange that the address of [code] is 0 in [code]. [code] is 0 in Backward. It is a bug that [code] is not called when [code] is called. I do not know why SoftmaxOutput is not a legacy operator.
"I too recently saw an issue with Softmax that generated a segfault.  This behavior began with the Softmax operator changes introduced by https://github.com/apache/incubator-mxnet/pull/13699 and occurs when the framework is compiled with USE_MKLDNN=0.  The failing test is with sockeye:
```
test/integration/test_constraints_int.py::test_constraints[--encoder rnn --decoder rnn --num-layers 1 --rnn-cell-type lstm --rnn-num-hidden 8 --num-embed 4  --rnn-attention-type mlp --rnn-attention-num-hidden 8 --loss cross-entropy --optimized-metric perplexity --max-updates 2 --checkpoint-frequency 2 --optimizer adam --initial-learning-rate 0.01 --batch-type sentence  --decode-and-evaluate 0-2-10] ./test.sh: line 3:    62 Segmentation fault      python setup.py test
++ RV=139
```
Perhaps you could verify that your fix corrects this behavior?",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-469080296,DickJC123,2019-03-04 00:04:44,14301,[14302],Algorithm design bug,1,I too recently saw an issue with Softmax that generated a segfault. This behavior began with the Softmax operator changes introduced by [url] and occurs when the framework is compiled with USE_MKLDNN=0. The failing test is with sockeye: ``[code]`` Perhaps you could verify that your fix corrects this behavior?
"@DickJC123 
Hi! I test sockeye in my laptop with MXNet(master) with `make -j 5 USE_OPENCV=1 USE_BLAS=openblas USE_MKLDNN=0 USE_CPP_PACKAGE=1`

All tests pass except for `test/unit/test_inference.py::test_topk_func`
```
test/unit/test_inference.py::test_topk_func[1-5-200] FAILED              [ 60%]
test/unit/test_inference.py::test_topk_func[5-5-200] FAILED              [ 60%]
test/unit/test_inference.py::test_topk_func[1-1-200] PASSED              [ 60%]
test/unit/test_inference.py::test_topk_func[5-1-200] PASSED              [ 60%]
test/unit/test_inference.py::test_topk_func[10-10-100] FAILED            [ 60%]
```

In sockeye, there is no any Softmax with normalization `valid`, so it couldn't trigger the bug in this issue.",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-469131646,wkcn,2019-03-04 06:15:06,14301,[14302],Algorithm design bug,1,"@DickJC123 Hi! I test sockeye in my laptop with MXNet(master) with [code] All tests pass except for [code] ``[code]`[code]valid`, so it couldn't trigger the bug in this issue."
"I can also confirm this issue, it happens only when `normalization-""valid""` and while executing the `Executor.backward` function call. For instance this sample code works fine - 

```
import mxnet as mx                                                                                                                                            
import numpy as np

xpu = mx.cpu()
x_nd = mx.nd.array([[1, 6, 4, 2],[1, 6, 4, 2]], ctx=xpu)    
grad_x = mx.nd.zeros((2,4), ctx=xpu)    
label_nd = mx.nd.array([1,1], ctx=xpu)

x_nd.attach_grad()

with mx.autograd.record():
    y = mx.nd.SoftmaxOutput(data=x_nd, label=label_nd, ignore_label=0, use_ignore=True) #, normalization=""valid"")

y.backward()
print(x_nd.grad)
```

So the bug is with the gradient calculation of softmax output when `normalization=""valid""`",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-469422711,anirudhacharya,2019-03-04 21:09:58,14301,[14302],Algorithm design bug,1,"I can also confirm this issue, it happens only when [code] and while executing the [code] function call. For instance this sample code works fine - ``[code]`[code]normalization=""valid""`"
"@wkcn Sockeye can use 'valid' normalization in its SoftmaxOutput operator use, see [here](https://github.com/awslabs/sockeye/blob/2f44099cd4f488bd8d348d74e9ae85095f72501e/sockeye/loss.py#L112).
The failure you are observing for `test/unit/test_inference.py::test_topk_func` is related to #13862, which is an open problem.",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-469434934,fhieber,2019-03-04 21:48:16,14301,[14302],Algorithm design bug,1,"@wkcn Sockeye can use 'valid' normalization in its SoftmaxOutput operator use, see [here]([url]#L112). The failure you are observing for [code] is related to #13862, which is an open problem."
"@fhieber Sorry that I overlooked it.
@anirudhacharya https://github.com/apache/incubator-mxnet/pull/14302 will address the problem.",IssueComment,https://github.com/apache/mxnet/issues/14301#issuecomment-469457535,wkcn,2019-03-04 23:02:26,14301,[14302],Algorithm design bug,1,@fhieber Sorry that I overlooked it. @anirudhacharya [url] will address the problem.
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 ",IssueComment,https://github.com/apache/mxnet/issues/14233#issuecomment-466387871,mxnet-label-bot,2019-02-22 12:57:30,14233,[14314],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it."
"@mxnet-label-bot add [Gluon, question]",IssueComment,https://github.com/apache/mxnet/issues/14233#issuecomment-466492519,frankfliu,2019-02-22 18:13:26,14233,[14314],Data bug,0,"@mxnet-label-bot add [Gluon, question]"
"Hi
Is there any temporary workaround for that?",IssueComment,https://github.com/apache/mxnet/issues/14233#issuecomment-468725015,mahmoodn,2019-03-01 16:31:06,14233,[14314],Data bug,0,Hi Is there any temporary workaround for that?
"@mahmoodn 
Sorry for late reply. It is a bug since the type of pyrnd.getrandbits(32) is long in python2.

mx.random.seed(int(pyrnd.getrandbits(32))) will work.",IssueComment,https://github.com/apache/mxnet/issues/14233#issuecomment-469027622,wkcn,2019-03-03 14:21:29,14233,[14314],Data bug,0,@mahmoodn Sorry for late reply. It is a bug since the type of pyrnd.getrandbits(32) is long in python2. mx.random.seed(int(pyrnd.getrandbits(32))) will work.
"Thank you for submitting the issue! I'm labeling it so the MXNet community members can help resolve it. @mxnet-label-bot add [Python, Bug, NDArray]",IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-454134161,piyushghai,2019-01-14 19:38:55,13862,[14356],Version compatibility bug,1,"Thank you for submitting the issue! I'm labeling it so the MXNet community members can help resolve it. @mxnet-label-bot add [Python, Bug, NDArray]"
"Could someone test this also with the 1.4 release candidate? If this is present there as well, I'd appreciate a last-minute fix very much! :)",IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-454592661,fhieber,2019-01-15 23:33:25,13862,[14356],Version compatibility bug,1,"Could someone test this also with the 1.4 release candidate? If this is present there as well, I'd appreciate a last-minute fix very much! :)"
"Hoping to get some activity on this.
I bisected the nightly builds to figure out, when this bug was introduced.
- `mxnet==1.3.1b20181005` is fine
- `mxnet==1.3.1b20181010` contains the bug
Assuming that builds are roughly reflecting merge dates of PRs, the bug must be introduced some time between October 5th and October 10th, and is hence also part of 1.4.0.

There is no code change in the ravel.* files or the unravel_index op itself since its addition, so I would guess that some other type of change caused this unwanted regression; maybe unravel_index uses some outdated interface?

The only commits where I could guess that they could have some effect on this:
- https://github.com/apache/incubator-mxnet/commit/443ded4f8ab455a4c4ec0b9d431564b8ccc785ea
- https://github.com/apache/incubator-mxnet/commit/5314cf4742767319ce356bd5154c6885380e0d5c
- https://github.com/apache/incubator-mxnet/commit/f9f74169bb05f85d85dec5991aa5fc9050dec9f6
- https://github.com/apache/incubator-mxnet/commit/ebe6ea8be97506dba7d00b5a25da58433e38caae
- https://github.com/apache/incubator-mxnet/commit/bcd24f85457821f9c0ce17d60e545a252a87a5ae#diff-58bd15446e511bc1b965256ed89ed624
- https://github.com/apache/incubator-mxnet/commit/e1fe7b14f4c31c35f77c9ee3b701a59db86c38c0

Unfortunately I don't have a way to bisect commits directly through source compilation right now.

I would appreciate some support on this. Thanks!",IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-457954221,fhieber,2019-01-27 21:06:06,13862,[14356],Version compatibility bug,1,"Hoping to get some activity on this. I bisected the nightly builds to figure out, when this bug was introduced. - [code] is fine - [code] contains the bug Assuming that builds are roughly reflecting merge dates of PRs, the bug must be introduced some time between October 5th and October 10th, and is hence also part of 1.4.0. There is no code change in the ravel.* files or the unravel_index op itself since its addition, so I would guess that some other type of change caused this unwanted regression; maybe unravel_index uses some outdated interface? The only commits where I could guess that they could have some effect on this: - [url] - [url] - [url] - [url] - [url]#diff-58bd15446e511bc1b965256ed89ed624 - [url] Unfortunately I don't have a way to bisect commits directly through source compilation right now. I would appreciate some support on this. Thanks!"
I can confirm that the problem exists in 1.4.0,IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-469433517,fhieber,2019-03-04 21:43:47,13862,[14356],Version compatibility bug,1,I can confirm that the problem exists in 1.4.0
"Pretty easy to diagnose. 

The type for mshadow::index_t got changed from ""unsigned"" to ""int64_t"" between both versions. This type is used to encode dimensions of shapes internally. 
In 1.3.1, a ""-1""  therefore was interpreted as unsigned(-1) which is basically max_int. In 1.4.0 and later it is interpreted as ""-1"" and that changed the behaviour of the operator. 

It is debatable whether we ever wanted to explicitly allow the use of ""-1"" in the shape argument of the operator. At least I as the original author had this not in mind and also the documentation does not say anything about magic numbers. In fact, the only case where the operator did work (and could ever work) with magic numbers is when ""-1"" is specified as first coordinate.  Nothing else is possible. 
Two solutions:
- We document that it is o.k. to use ""-1"" as first shape coordinate in ravel/unravel_index. And enhance the operator to support this. That does not require any change for ""ravel"" and a simple change in the code shown below. 
- We leave it as is (the code simply does not support this). A potential workaround for customers would be to specify a huge number except ""-1"" for the first dimension which has essentially the same effect. 

```
struct unravel_index {
  template<typename DType>
  MSHADOW_XINLINE static void Map(int i, index_t N, index_t ndim, index_t *shape,
                                  DType *unravelled, DType *ravelled) {
    index_t idx(ravelled[i]);
    #pragma unroll
    for (int j = ndim; j--; ) {
      index_t tmp = idx / shape[j];
      unravelled[i+j*N] = idx - tmp*shape[j];
      idx = tmp;
    }
  }
};
```







",IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-470110394,asmushetzel,2019-03-06 13:39:47,13862,[14356],Version compatibility bug,1,"Pretty easy to diagnose. The type for mshadow::index_t got changed from ""unsigned"" to ""int64_t"" between both versions. This type is used to encode dimensions of shapes internally. In 1.3.1, a ""-1"" therefore was interpreted as unsigned(-1) which is basically max_int. In 1.4.0 and later it is interpreted as ""-1"" and that changed the behaviour of the operator. It is debatable whether we ever wanted to explicitly allow the use of ""-1"" in the shape argument of the operator. At least I as the original author had this not in mind and also the documentation does not say anything about magic numbers. In fact, the only case where the operator did work (and could ever work) with magic numbers is when ""-1"" is specified as first coordinate. Nothing else is possible. Two solutions: - We document that it is o.k. to use ""-1"" as first shape coordinate in ravel/unravel_index. And enhance the operator to support this. That does not require any change for ""ravel"" and a simple change in the code shown below. - We leave it as is (the code simply does not support this). A potential workaround for customers would be to specify a huge number except ""-1"" for the first dimension which has essentially the same effect. ``[code]``"
"Oh thank you so much! This is quite hilarious - we relied on exploiting this undocumented behavior without knowing :)
For Sockeye this means that we can update to 1.4.0 immediately and make this exploit of range overflow explicit by using `shape=(sys.maxsize, y)` in our code instead of `shape=(-1, y)`. We rely on this functionality because we cannot infer the first shape value (batch size) in a HybridBlock in our case. As such, I would vote for going with option 1, explicitly supporting it in the future.",IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-470117793,fhieber,2019-03-06 14:01:19,13862,[14356],Version compatibility bug,1,"Oh thank you so much! This is quite hilarious - we relied on exploiting this undocumented behavior without knowing :) For Sockeye this means that we can update to 1.4.0 immediately and make this exploit of range overflow explicit by using [code] in our code instead of [code]. We rely on this functionality because we cannot infer the first shape value (batch size) in a HybridBlock in our case. As such, I would vote for going with option 1, explicitly supporting it in the future."
PR #14356 will fix this.,IssueComment,https://github.com/apache/mxnet/issues/13862#issuecomment-470459122,asmushetzel,2019-03-07 09:52:14,13862,[14356],Version compatibility bug,1,PR #14356 will fix this.
Is there any use case of trying to create an NDArray of negative size ?,IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-430485950,Vikas-kum,2018-10-17 04:31:15,9166,[14362],Data bug,0,Is there any use case of trying to create an NDArray of negative size ?
@Vikas89 shape -1 means MXNet will automatically infer the shape.,IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-430734351,apeforest,2018-10-17 18:18:17,9166,[14362],Data bug,0,@Vikas89 shape -1 means MXNet will automatically infer the shape.
@nswamy Please add label [NDArray] Thanks,IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-430734808,apeforest,2018-10-17 18:19:43,9166,[14362],Data bug,0,@nswamy Please add label [NDArray] Thanks
@apeforest this is not a bug but a case of improper exception handling. Please update the labels.,IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-470279860,anirudhacharya,2019-03-06 21:16:10,9166,[14362],Data bug,0,@apeforest this is not a bug but a case of improper exception handling. Please update the labels.
@anirudhacharya This is causing a core dump. I think it's a bug that needs to be fixed.,IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-470280743,apeforest,2019-03-06 21:18:48,9166,[14362],Data bug,0,@anirudhacharya This is causing a core dump. I think it's a bug that needs to be fixed.
"with the latest version of mxnet, the above command neither causes core dump or Segmentation fault. 
When I try to pass `-1` as shape in python2 and python3, I get the following stack trace.
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/ndarray/utils.py"", line 67, in zeros
    return _zeros_ndarray(shape, ctx, dtype, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/ndarray/ndarray.py"", line 3822, in zeros
    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)
  File ""<string>"", line 34, in _zeros
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/_ctypes/ndarray.py"", line 92, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [21:21:58] src/storage/./cpu_device_storage.h:74: Failed to allocate CPU Memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x1bc) [0x7f761aea978c]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f761aeaab08]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(+0x396d71d) [0x7f761e26e71d]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(mxnet::storage::NaiveStorageManager<mxnet::storage::CPUDeviceStorage>::Alloc(mxnet::Storage::Handle*)+0xd) [0x7f761e26e74d]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(mxnet::StorageImpl::Alloc(mxnet::Storage::Handle*)+0x5b) [0x7f761e269efb]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(mxnet::NDArray::CheckAndAlloc() const+0x98d) [0x7f761aeab59d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0xd88) [0x7f761db1a308]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x17) [0x7f761db1a9c7]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(std::_Function_handler<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::Engine::PushSync(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, mxnet::FnProperty, int, char const*)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&, mxnet::engine::CallbackOnComplete&&)+0x5e) [0x7f761da9eb8e]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet-1.5.0-py2.7.egg/mxnet/libmxnet.so(mxnet::engine::NaiveEngine::PushAsync(std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, mxnet::FnProperty, int, char const*, bool)+0x20c) [0x7f761e24393c]
```

Based on this, I think this issue can be closed.",IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-470283142,anirudhacharya,2019-03-06 21:26:04,9166,[14362],Data bug,0,"with the latest version of mxnet, the above command neither causes core dump or Segmentation fault. When I try to pass [code] as shape in python2 and python3, I get the following stack trace. ``[code]`` Based on this, I think this issue can be closed."
"@anirudhacharya Thanks for checking this. Could you please help to update the exception with more meaningful message? Ideally, the user should know that negative number should not be used as dimension in this command. ",IssueComment,https://github.com/apache/mxnet/issues/9166#issuecomment-470284370,apeforest,2019-03-06 21:29:27,9166,[14362],Data bug,0,"@anirudhacharya Thanks for checking this. Could you please help to update the exception with more meaningful message? Ideally, the user should know that negative number should not be used as dimension in this command."
Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.,IssueComment,https://github.com/apache/mxnet/issues/13760#issuecomment-451002197,leleamol,2019-01-02 22:15:23,13760,[14403],Data bug,1,Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.
"@mxnet-label-bot add [Python, NDArray, Operator, Bug]",IssueComment,https://github.com/apache/mxnet/issues/13760#issuecomment-451002597,leleamol,2019-01-02 22:17:00,13760,[14403],Data bug,1,"@mxnet-label-bot add [Python, NDArray, Operator, Bug]"
"In `src/operator/tensor/matrix_op-inl.h`, there is no judgement for `e == b` in the function `SetSliceOpOutputDimSize`.",IssueComment,https://github.com/apache/mxnet/issues/13760#issuecomment-452647495,wkcn,2019-01-09 10:27:19,13760,[14403],Data bug,1,"In [code], there is no judgement for [code] in the function [code]."
Empty tensor is currently not possible to be explicitly expressed in MXNet.,IssueComment,https://github.com/apache/mxnet/issues/13760#issuecomment-466583434,szha,2019-02-22 23:24:53,13760,[14403],Data bug,1,Empty tensor is currently not possible to be explicitly expressed in MXNet.
"Since the begin=end case is currently returning ndarray with garbage values, should we throw an exception when we encounter these inputs. I know we may have users depending on this behavior, but the number may be very less because of the garbage values.",IssueComment,https://github.com/apache/mxnet/issues/13760#issuecomment-466607258,anirudh2290,2019-02-23 02:21:48,13760,[14403],Data bug,1,"Since the begin=end case is currently returning ndarray with garbage values, should we throw an exception when we encounter these inputs. I know we may have users depending on this behavior, but the number may be very less because of the garbage values."
"@mxnet-label-bot add [Gluon, question]",IssueComment,https://github.com/apache/mxnet/issues/13616#issuecomment-446657106,vdantu,2018-12-12 16:45:28,13616,[14423],Code bug,0,"@mxnet-label-bot add [Gluon, question]"
"I confirm this behavior, and here is the minimal reproducible example:

```
import mxnet as mx
from mxnet.gluon.nn import HybridSequential, Dense

model = HybridSequential()
model.add(Dense(units=10))

model.initialize()
model.hybridize()

model(mx.random.uniform(shape=(10, 100)))
model.export(""./model"")

block = mx.gluon.nn.SymbolBlock.imports('model-symbol.json', ['data'], 'model-0000.params')
print(block)
```

Result that is printed on the screen looks like:

```
SymbolBlock(

)
```

Funny that both pure Gluon and pure Symbol models are still printing something. For Gluon model I would receive expected output:

```
HybridSequential(
  (0): Dense(100 -> 10, linear)
)
```

For same model using Symbolic API I receive only the name of the last layer (not very useful, but still it is something):

```
<Symbol fc1>
```

I would mark this issue as a bug.",IssueComment,https://github.com/apache/mxnet/issues/13616#issuecomment-450225823,Ishitori,2018-12-27 20:30:26,13616,[14423],Code bug,0,"I confirm this behavior, and here is the minimal reproducible example: ``[code]`[code]`[code]`[code]`[code]`[code]`[code]`` I would mark this issue as a bug."
Maybe we can print the json representation,IssueComment,https://github.com/apache/mxnet/issues/13616#issuecomment-450528716,eric-haibin-lin,2018-12-29 23:38:23,13616,[14423],Code bug,0,Maybe we can print the json representation
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Example",IssueComment,https://github.com/apache/mxnet/issues/14439#issuecomment-473417754,mxnet-label-bot,2019-03-15 19:42:53,14439,[14438],Test bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Example"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/14396#issuecomment-471814862,mxnet-label-bot,2019-03-12 01:47:12,14396,[14451],Code bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"I found that the custom operator has been `Engine::Get()->PushSync`, but it couldn't be executed. (forward and backward).",IssueComment,https://github.com/apache/mxnet/issues/14396#issuecomment-471821570,wkcn,2019-03-12 02:12:42,14396,[14451],Code bug,1,"I found that the custom operator has been [code], but it couldn't be executed. (forward and backward)."
"I rekon there's a limitation of custom op with requires a global lock when executing the python custom op, and it might cause dead lock when combined with subprocess.

@eric-haibin-lin raising for expertise in engine part.",IssueComment,https://github.com/apache/mxnet/issues/14396#issuecomment-471961381,zhreshold,2019-03-12 11:24:47,14396,[14451],Code bug,1,"I rekon there's a limitation of custom op with requires a global lock when executing the python custom op, and it might cause dead lock when combined with subprocess. @eric-haibin-lin raising for expertise in engine part."
"Yes this is because of the dead lock in the subprocess. One way to fix this is to create a start and stop functions in CustomOperator, which should be called from pthread_atfork prepare and child handlers.
Using thread pool to manage CustomOperator threads would make the implementation cleaner. Anyone wants to try and create a PR for this ? @wkcn @arcadiaphy @YutingZhang ?",IssueComment,https://github.com/apache/mxnet/issues/14396#issuecomment-473415672,anirudh2290,2019-03-15 19:35:11,14396,[14451],Code bug,1,"Yes this is because of the dead lock in the subprocess. One way to fix this is to create a start and stop functions in CustomOperator, which should be called from pthread_atfork prepare and child handlers. Using thread pool to manage CustomOperator threads would make the implementation cleaner. Anyone wants to try and create a PR for this ? @wkcn @arcadiaphy @YutingZhang ?"
"After #14363, the threads is created  when running custom operator, so custom operator needs also to be executed in main process to reproduce the bug:

```
from concurrent import futures

import mxnet as mx
import sys

class AdditionOP(mx.operator.CustomOp):
    def __init__(self):
        super(AdditionOP, self).__init__()
    def forward(self, is_train, req, in_data, out_data, aux):
        out_data[0][:] = in_data[0] + in_data[1]
    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
        in_grad[0][:] = out_grad[0]
        in_grad[1][:] = out_grad[0]

@mx.operator.register(""AdditionOP"")
class AdditionOPProp(mx.operator.CustomOpProp):
    def __init__(self):
        super(AdditionOPProp, self).__init__()
    def list_arguments(self):
        return ['a', 'b']
    def list_outputs(self):
        return ['output']
    def infer_shape(self, in_shape):
        return in_shape, [in_shape[0]]
    def create_operator(self, ctx, shapes, dtypes):
        return AdditionOP()

def foo():
    a = mx.nd.array([1, 2, 3])
    b = mx.nd.array([4, 5, 6])

    a.attach_grad()
    b.attach_grad()

    print(""REC"")
    with mx.autograd.record():
        c = mx.nd.Custom(a, b, op_type='AdditionOP')

    dc = mx.nd.array([7, 8, 9])
    c.backward(dc)

    print('Okay :-)')
    print('a + b = c \n {} + {} = {}'.format(a.asnumpy(), b.asnumpy(), c.asnumpy()))

def main():
    foo()  # ensure custom threads created in main process
    ex = futures.ProcessPoolExecutor(1)
    r = ex.submit(foo)
    r.result()

if __name__ == '__main__':
    main()
```",IssueComment,https://github.com/apache/mxnet/issues/14396#issuecomment-473577236,arcadiaphy,2019-03-16 19:26:49,14396,[14451],Code bug,1,"After #14363, the threads is created when running custom operator, so custom operator needs also to be executed in main process to reproduce the bug: ``[code]``"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/14505#issuecomment-475751255,mxnet-label-bot,2019-03-22 19:28:27,14505,[14585],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"Thank you for submitting the issue! I'm labeling it so the MXNet community members can help resolve it.

@mxnet-label-bot add [Gluon, Data-loading, Bug, Call for Contribution]",IssueComment,https://github.com/apache/mxnet/issues/14505#issuecomment-475754380,zachgk,2019-03-22 19:38:25,14505,[14585],Data bug,0,"Thank you for submitting the issue! I'm labeling it so the MXNet community members can help resolve it. @mxnet-label-bot add [Gluon, Data-loading, Bug, Call for Contribution]"
@mzient fix in #14585. Thank you for your solution. ,IssueComment,https://github.com/apache/mxnet/issues/14505#issuecomment-479988230,abhinavs95,2019-04-04 17:22:02,14505,[14585],Data bug,0,@mzient fix in #14585. Thank you for your solution.
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-468943876,mxnet-label-bot,2019-03-02 17:56:56,14304,[14757],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"@mxnet-label-bot Add [Build, CMake
]",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-469455572,piyushghai,2019-03-04 22:55:35,14304,[14757],Build bug,0,"@mxnet-label-bot Add [Build, CMake ]"
+1,IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-476352680,cjolivier01,2019-03-25 19:54:53,14304,[14757],Build bug,0,+1
"updated the first post with the latest changes in the environment

same issue",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-480899982,sl1pkn07,2019-04-08 16:17:21,14304,[14757],Build bug,0,updated the first post with the latest changes in the environment same issue
"~please add [breaking] tag. i cant build mxnet anymore :S~

seems add `-DUSE_F16C=OFF` do the trick",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-482694077,sl1pkn07,2019-04-12 19:25:02,14304,[14757],Build bug,0,~please add [breaking] tag. i cant build mxnet anymore :S~ seems add [code] do the trick
"same issue, pls help",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-484426375,xiongzhangdavid,2019-04-18 09:32:15,14304,[14757],Build bug,0,"same issue, pls help"
"Add:
```
    set(CMAKE_C_FLAGS ""${CMAKE_CXX_FLAGS} -msse3"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -msse2"")
    set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -msse3"")
```
if using CMake",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-484508482,trivialfis,2019-04-18 13:28:28,14304,[14757],Build bug,0,Add: ``[code]`` if using CMake
@trivialfis not works for me,IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-484515585,sl1pkn07,2019-04-18 13:45:15,14304,[14757],Build bug,0,@trivialfis not works for me
"@sl1pkn07 I'm building mxnet with g++@ 7, cmake@3.12 and make (first time trying to build, still trying to fix other issues).  Added the above lines between:

https://github.com/apache/incubator-mxnet/blob/0da4b67ebf5788deef97ecaca5e30cbc9d27660d/CMakeLists.txt#L155

It works for me, not sure what's missing from your environment.",IssueComment,https://github.com/apache/mxnet/issues/14304#issuecomment-484519840,trivialfis,2019-04-18 13:54:51,14304,[14757],Build bug,0,"@sl1pkn07 I'm building mxnet with g++@ 7, [email] and make (first time trying to build, still trying to fix other issues). Added the above lines between: [url]#L155 It works for me, not sure what's missing from your environment."
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Gluon, Example",IssueComment,https://github.com/apache/mxnet/issues/14736#issuecomment-484617692,mxnet-label-bot,2019-04-18 18:00:32,14736,[14781],Test bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Gluon, Example"
"@ssbusc1 Thanks for filing this issue! We will look into this.
@mxnet-label-bot [Bug, Sparse] ",IssueComment,https://github.com/apache/mxnet/issues/12787#issuecomment-428810951,piyushghai,2018-10-11 03:48:13,12787,[14856],Documentation bug,1,"@ssbusc1 Thanks for filing this issue! We will look into this. @mxnet-label-bot [Bug, Sparse]"
"@mxnet-label-bot add [Operator, Bug]",IssueComment,https://github.com/apache/mxnet/issues/12970#issuecomment-432873660,safrooze,2018-10-25 00:36:38,12970,[14919],Test bug,0,"@mxnet-label-bot add [Operator, Bug]"
"Bilinear upsampling required kernel to be passed as input because it use deconvolution under the hood. Clarified the expectation in API documentation.
```python
 >>> x = nd.ones((1,1,3,3))
  >>> x
  [[[[1. 1. 1.]
     [1. 1. 1.]
     [1. 1. 1.]]]]
  <NDArray 1x1x3x3 @cpu(0)>
  >>> w = nd.ones((1,1,4,4))
  >>> w
  [[[[1. 1. 1. 1.]
     [1. 1. 1. 1.]
     [1. 1. 1. 1.]
     [1. 1. 1. 1.]]]]
  <NDArray 1x1x4x4 @cpu(0)>
  >>> nd.UpSampling(x, w, scale=2, sample_type='bilinear', num_filter=1)
  [[[[1. 2. 2. 2. 2. 1.]
     [2. 4. 4. 4. 4. 2.]
     [2. 4. 4. 4. 4. 2.]
     [2. 4. 4. 4. 4. 2.]
     [2. 4. 4. 4. 4. 2.]
     [1. 2. 2. 2. 2. 1.]]]]
  <NDArray 1x1x6x6 @cpu(0)>
```",IssueComment,https://github.com/apache/mxnet/issues/12970#issuecomment-490606970,sandeep-krishnamurthy,2019-05-08 18:52:24,12970,[14919],Test bug,0,Bilinear upsampling required kernel to be passed as input because it use deconvolution under the hood. Clarified the expectation in API documentation. ``[code]``
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: ONNX, Bug",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489272964,mxnet-label-bot,2019-05-03 23:52:44,14875,[14942],Deployment bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: ONNX, Bug"
@Roshrini Could you please take a look?,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489394830,lanking520,2019-05-05 06:23:03,14875,[14942],Deployment bug,0,@Roshrini Could you please take a look?
"@ehsanmok mxnet isn't responsible for this error, it's only ONNX.
update your ONNX version, should work when ONNX==1.2.2",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489583284,AnaRhisT94,2019-05-06 11:06:49,14875,[14942],Deployment bug,0,"@ehsanmok mxnet isn't responsible for this error, it's only ONNX. update your ONNX version, should work when ONNX==1.2.2"
"@AnaRhisT94 No, my ONNX is already the latest v1.5.0. It's when calling [export_model](https://mxnet.apache.org/api/python/contrib/onnx.html?highlight=onnx#module-mxnet.contrib.onnx.mx2onnx.export_model) causes that to happen. `int(None)` is never valid.",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489708835,ehsanmok,2019-05-06 17:42:07,14875,[14942],Deployment bug,0,"@AnaRhisT94 No, my ONNX is already the latest v1.5.0. It's when calling [export_model]([url]#module-mxnet.contrib.onnx.mx2onnx.export_model) causes that to happen. [code] is never valid."
Try to use ONNX 1.2.2,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489823249,AnaRhisT94,2019-05-06 23:27:14,14875,[14942],Deployment bug,0,Try to use ONNX 1.2.2
Same error with ONNX 1.2.2,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-489826598,ehsanmok,2019-05-06 23:44:25,14875,[14942],Deployment bug,0,Same error with ONNX 1.2.2
"@mxnet-label-bot add [Bug]

@ehsanmok I'm looking into this, could you share your script?",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-491915490,vandanavk,2019-05-13 17:32:58,14875,[14942],Deployment bug,0,"@mxnet-label-bot add [Bug] @ehsanmok I'm looking into this, could you share your script?"
"> Same error with ONNX 1.2.2

I see, well just delete the None then?",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-492646691,AnaRhisT94,2019-05-15 13:07:29,14875,[14942],Deployment bug,0,"> Same error with ONNX 1.2.2 I see, well just delete the None then?"
"@vandanavk here is an MVE

```python
from os import path as osp
import numpy as np
import mxnet as mx
from mxnet.contrib import onnx as onnx_mxnet
from mxnet import gluon
from gluoncv import model_zoo, data, utils

OUTPUT = ""./output""
DATA = ""./data/cat.png""
SIZE = 320
MODEL = ""yolo3_mobilenet1.0_coco""
INPUT_SHAPE = (1, 3, SIZE, SIZE)

net = model_zoo.get_model(MODEL, pretrained=True)
net.hybridize()
# pass an img to trigger init after hybridize
x, _ = data.transforms.presets.yolo.load_test(DATA, short=SIZE)
_, _ = net(x)

net.export(osp.join(OUTPUT, MODEL))
sym = osp.join(OUTPUT, MODEL + ""-symbol.json"")
params = osp.join(OUTPUT, MODEL + ""-0000.params"")
onnx_file = osp.join(OUTPUT, MODEL + "".onnx"")

converted_model_path = onnx_mxnet.export_model(sym, params, [INPUT_SHAPE], np.float32, onnx_file, verbose=True)
```",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-492800991,ehsanmok,2019-05-15 19:57:42,14875,[14942],Deployment bug,0,@vandanavk here is an MVE ``[code]``
"@ehsanmok I tried the following code with the PR https://github.com/apache/incubator-mxnet/pull/14942. `ValueError: invalid literal for int() with base 10: 'None'` error doesn't occur anymore but I do see `AttributeError: No conversion function registered for op type _arange yet.`. _arange export can be filed as separate feature request. Please try PR https://github.com/apache/incubator-mxnet/pull/14942 and let me know if it works for you.

```
from os import path as osp
import numpy as np
import mxnet as mx
from mxnet.contrib import onnx as onnx_mxnet
from mxnet import gluon
from gluoncv import model_zoo, data, utils

OUTPUT = ""./""
DATA = ""./cat.jpg""
SIZE = 320
MODEL = ""yolo3_darknet53_coco""
INPUT_SHAPE = (1, 3, SIZE, SIZE)

net = model_zoo.get_model(MODEL, pretrained=True)
net.hybridize()
# pass an img to trigger init after hybridize
x, _ = data.transforms.presets.yolo.load_test(DATA, short=SIZE)
_ = net(x)

net.export(osp.join(OUTPUT, MODEL))
sym = osp.join(OUTPUT, MODEL + ""-symbol.json"")
params = osp.join(OUTPUT, MODEL + ""-0000.params"")
onnx_file = osp.join(OUTPUT, MODEL + "".onnx"")

converted_model_path = onnx_mxnet.export_model(sym, params, [INPUT_SHAPE], np.float32, onnx_file, verbose=True)
```",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-510156252,vandanavk,2019-07-10 17:32:08,14875,[14942],Deployment bug,0,@ehsanmok I tried the following code with the PR [url] [code] error doesn't occur anymore but I do see [code]. _arange export can be filed as separate feature request. Please try PR [url] and let me know if it works for you. ``[code]``
@ehsanmok I met the same problem with you. I saw the issue has been around for 3 months. Did you solve it?,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-512034878,bloatybo,2019-07-16 23:03:15,14875,[14942],Deployment bug,0,@ehsanmok I met the same problem with you. I saw the issue has been around for 3 months. Did you solve it?
"> @ehsanmok I met the same problem with you. I saw the issue has been around for 3 months. Did you solve it?

Can you try the PR https://github.com/apache/incubator-mxnet/pull/14942? I dint see the issue with this PR.",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-513933483,vandanavk,2019-07-22 20:04:39,14875,[14942],Deployment bug,0,> @ehsanmok I met the same problem with you. I saw the issue has been around for 3 months. Did you solve it? Can you try the PR [url] I dint see the issue with this PR.
"I found that there are several ops not supported during converting, including slice_axis(..., end =None), slice_like, repeat, arange. But for a fixed input dimension, these operations can be replaced with normal slice_like, concat. The main problem is in box_nms. ",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-522298438,caiqi,2019-08-18 07:28:31,14875,[14942],Deployment bug,0,"I found that there are several ops not supported during converting, including slice_axis(..., end =None), slice_like, repeat, arange. But for a fixed input dimension, these operations can be replaced with normal slice_like, concat. The main problem is in box_nms."
"Not sure if this is the right place to post, but I used the fixes from  PR #14942, fixed the issue for me but the next error is: 
'AttributeError: No conversion function registered for op type _greater_scalar yet.'

Attempting to export 'ssd_512_mobilenet1.0_voc'
",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-524158528,ntomer,2019-08-23 03:28:07,14875,[14942],Deployment bug,0,"Not sure if this is the right place to post, but I used the fixes from PR #14942, fixed the issue for me but the next error is: 'AttributeError: No conversion function registered for op type _greater_scalar yet.' Attempting to export 'ssd_512_mobilenet1.0_voc'"
@caiqi @ntomer feel free to contribute the ONNX conversion for these missing operators :+1:,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-526781904,vandanavk,2019-08-31 00:06:19,14875,[14942],Deployment bug,0,@caiqi @ntomer feel free to contribute the ONNX conversion for these missing operators :+1:
Does anyone have any update on this ? I am having the same issue ...,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-550039126,djaym7,2019-11-05 21:53:47,14875,[14942],Deployment bug,0,Does anyone have any update on this ? I am having the same issue ...
"the bug is happen in yolo3.py line 161, there is a None param, fix it. and then u will meet the _arange op not regist",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-560980966,ghost,2019-12-03 03:22:28,14875,[14942],Deployment bug,0,"the bug is happen in yolo3.py line 161, there is a None param, fix it. and then u will meet the _arange op not regist"
"> Not sure if this is the right place to post, but I used the fixes from PR #14942, fixed the issue for me but the next error is:
> 'AttributeError: No conversion function registered for op type _greater_scalar yet.'
> 
> Attempting to export 'ssd_512_mobilenet1.0_voc'

Did you find some ways to fix it? I meet it with ""ssd_512_resnet50_v1_voc"" ",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-564367981,Rainweic,2019-12-11 03:50:11,14875,[14942],Deployment bug,0,"> Not sure if this is the right place to post, but I used the fixes from PR #14942, fixed the issue for me but the next error is: > 'AttributeError: No conversion function registered for op type _greater_scalar yet.' > > Attempting to export 'ssd_512_mobilenet1.0_voc' Did you find some ways to fix it? I meet it with ""ssd_512_resnet50_v1_voc"""
"nope, raised a ticket in Amazon but no one is currently working on this ..",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-564773614,djaym7,2019-12-11 23:06:40,14875,[14942],Deployment bug,0,"nope, raised a ticket in Amazon but no one is currently working on this .."
"same problom, when used torch yolov32onnx.py, so easy to convert
I will give up mxnet never look back",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-575012459,mahxn0,2020-01-16 06:58:55,14875,[14942],Deployment bug,0,"same problom, when used torch yolov32onnx.py, so easy to convert I will give up mxnet never look back"
"I met the same problem.
And I tried the solution in PR[#14942](https://github.com/apache/incubator-mxnet/pull/14942), found a new bug.
>  File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_model.py"", line 83, in export_model
    verbose=verbose)
  File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 253, in create_onnx_graph_proto
    idx=idx
  File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 90, in convert_layer
    raise AttributeError(""No conversion function registered for op type %s yet."" % op)
AttributeError: No conversion function registered for op type _arange yet.",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-580600315,chouxianyu,2020-01-31 06:23:36,14875,[14942],Deployment bug,0,"I met the same problem. And I tried the solution in PR[#14942]([url] found a new bug. > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_model.py"", line 83, in export_model verbose=verbose) File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 253, in create_onnx_graph_proto idx=idx File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 90, in convert_layer raise AttributeError(""No conversion function registered for op type %s yet."" % op) AttributeError: No conversion function registered for op type _arange yet."
"> I met the same problem.
> And I tried the solution in PR[#14942](https://github.com/apache/incubator-mxnet/pull/14942), found a new bug.
> 
> > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_model.py"", line 83, in export_model
> > verbose=verbose)
> > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 253, in create_onnx_graph_proto
> > idx=idx
> > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 90, in convert_layer
> > raise AttributeError(""No conversion function registered for op type %s yet."" % op)
> > AttributeError: No conversion function registered for op type _arange yet.

Had same error on Nov 5, 2019.. tried to build make the operator but didnt work..",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-582171608,djaym7,2020-02-04 23:46:37,14875,[14942],Deployment bug,0,"> I met the same problem. > And I tried the solution in PR[#14942]([url] found a new bug. > > > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_model.py"", line 83, in export_model > > verbose=verbose) > > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 253, in create_onnx_graph_proto > > idx=idx > > File ""D:\WorkingSoftware\Anaconda3\lib\site-packages\mxnet\contrib\onnx\mx2onnx\export_onnx.py"", line 90, in convert_layer > > raise AttributeError(""No conversion function registered for op type %s yet."" % op) > > AttributeError: No conversion function registered for op type _arange yet. Had same error on Nov 5, 2019.. tried to build make the operator but didnt work.."
"I'm encountering the same issue.
I fine-tuned an SSD model on a custom dataset (everything working properly), and I'm trying to export it to ONNX in order to run it on Android.
This is what I'm doing:

```
from os import path as osp
import numpy as np
import mxnet as mx
import gluoncv as gcv
from mxnet.contrib import onnx as onnx_mxnet
from mxnet import gluon
from gluoncv import model_zoo, data, utils

ctx = mx.cpu(0)

OUTPUT = 'oxnn/'
DATA = ""./friends.png""
MODEL = ""CML_exported""
INPUT_SHAPE = ((1,3,512,683))

dummy_img, _ = data.transforms.presets.ssd.load_test(DATA, short=512)

CML_classes = [""CML_mug""]
net = gcv.model_zoo.get_model('ssd_512_mobilenet1.0_custom', classes=CML_classes, pretrained_base=False, ctx=ctx)
net.load_parameters(""saved_weights/CML_mobilenet_mug_00/ep_035.params"", ctx=ctx)
net.hybridize()
_ = net(dummy_img)

net.export(osp.join(OUTPUT, MODEL))
sym = osp.join(OUTPUT, MODEL + ""-symbol.json"")
params = osp.join(OUTPUT, MODEL + ""-0000.params"")
onnx_file = osp.join(OUTPUT, MODEL + "".onnx"")

converted_model_path = onnx_mxnet.export_model(sym, params, [INPUT_SHAPE], np.float32, onnx_file, verbose=True)
```

I'm getting the usual:

```
  File ""/home/lews/anaconda3/envs/gluon/lib/python3.7/site-packages/mxnet/contrib/onnx/mx2onnx/_op_translations.py"", line 1502, in convert_slice_axis
    ends = int(attrs.get(""end"", None))
ValueError: invalid literal for int() with base 10: 'None'```

Any updates since last year? Was this somehow fixed?",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-778132510,LewsTherin511,2021-02-12 11:11:29,14875,[14942],Deployment bug,0,"I'm encountering the same issue. I fine-tuned an SSD model on a custom dataset (everything working properly), and I'm trying to export it to ONNX in order to run it on Android. This is what I'm doing: ``[code]`[code]`[code]`` Any updates since last year? Was this somehow fixed?"
"Hi @LewsTherin511, thanks for reaching out! This should be easy to fix; you can expect this to be fixed by tomorrow :)

Meanwhile what version of mxnet are you using and on what os? Our team have been improving onnx lately (on v1.x branch) and here is a simple tool to help update onnx support to your local mxnet https://github.com/apache/incubator-mxnet/pull/19876
We would be very happy to help you export the model and answer any questions",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-780859592,Zha0q1,2021-02-17 21:16:46,14875,[14942],Deployment bug,0,"Hi @LewsTherin511, thanks for reaching out! This should be easy to fix; you can expect this to be fixed by tomorrow :) Meanwhile what version of mxnet are you using and on what os? Our team have been improving onnx lately (on v1.x branch) and here is a simple tool to help update onnx support to your local mxnet [url] We would be very happy to help you export the model and answer any questions"
Actually as @waytrue17 pointed out in https://discuss.mxnet.apache.org/t/exporting-model-to-onnx-or-alternative-way-to-run-on-android/6862 this might have already been fixed. Would you try #19876 to update to the latest onnx support?,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-780860927,Zha0q1,2021-02-17 21:19:00,14875,[14942],Deployment bug,0,Actually as @waytrue17 pointed out in [url] this might have already been fixed. Would you try #19876 to update to the latest onnx support?
"> Actually as @waytrue17 pointed out in https://discuss.mxnet.apache.org/t/exporting-model-to-onnx-or-alternative-way-to-run-on-android/6862 this might have already been fixed. Would you try #19876 to update to the latest onnx support?

Hi, thank you very much for your answer!

I'm currenlty using mxnet-mkl (1.6.0), gluoncv (0.8.0) and onnx (1.8.1).
Actually, I also tried updating the installation, but I see that the only effect was to update to gluoncv (0.9.4.post1).

Anyway, I asked on the forum already, but I have an incredibly lame question. I generally always installed mxnet/gluoncv with pip. In order to try your suggestion, I’m assuming I should follow the instructions for an installation from source and switch to the branch you indicated?
So something like:

* pip install --upgrade mxnet -f https://dist.mxnet.io/python/all
* git clone https://github.com/dmlc/gluon-cv
* git checkout v1.x
* cd gluon-cv && python setup.py install --user

or am I getting this completely wrong?
Thanks!
",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-785219238,LewsTherin511,2021-02-24 16:55:06,14875,[14942],Deployment bug,0,"> Actually as @waytrue17 pointed out in [url] this might have already been fixed. Would you try #19876 to update to the latest onnx support? Hi, thank you very much for your answer! I'm currenlty using mxnet-mkl (1.6.0), gluoncv (0.8.0) and onnx (1.8.1). Actually, I also tried updating the installation, but I see that the only effect was to update to gluoncv (0.9.4.post1). Anyway, I asked on the forum already, but I have an incredibly lame question. I generally always installed mxnet/gluoncv with pip. In order to try your suggestion, I’m assuming I should follow the instructions for an installation from source and switch to the branch you indicated? So something like: * pip install --upgrade mxnet -f [url] * git clone [url] * git checkout v1.x * cd gluon-cv && python setup.py install --user or am I getting this completely wrong? Thanks!"
"No no need to build from source. I know that sucks :)

You can just download my python script and run it anywhere and it should work. What the script does is basically 1) detect you current mxnet installation directory 2) pull the latest changes from mxnet repo 3) copy over and overwrite the onnx module to you current mxnet version. You shouldn't need to do anything besides running the script.

I think it's fine to keep you current mxnet and gluon versions.

Please let us know if you have any questions and feel free to @me.",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-786923536,Zha0q1,2021-02-26 22:18:32,14875,[14942],Deployment bug,0,No no need to build from source. I know that sucks :) You can just download my python script and run it anywhere and it should work. What the script does is basically 1) detect you current mxnet installation directory 2) pull the latest changes from mxnet repo 3) copy over and overwrite the onnx module to you current mxnet version. You shouldn't need to do anything besides running the script. I think it's fine to keep you current mxnet and gluon versions. Please let us know if you have any questions and feel free to @me.
@LewsTherin511,IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-786923600,Zha0q1,2021-02-26 22:18:44,14875,[14942],Deployment bug,0,@LewsTherin511
"Hi! Thanks again for your assistance last time, it worked perfectly! 

However, I noticed something weird.
When I first used your script, I was working on another computer, and everything went ok. Yesterday, I tried exporting to ONNX on another machine, and I got the usual error:
`ValueError: invalid literal for int() with base 10: 'None'
`
The machine I'm having problem with has 
gluoncv (0.10.4.post0)
mxnet-mkl (1.6.0)
onnx (1.9.0)

So, I tried the update script, but running it seems to somehow break the mxnet installation. After the update, whenever I try importing mxnet, I get the error:
`File ""<stdin>"", line 1, in <module>
  File ""/home/lews/anaconda3/envs/gluon/lib/python3.8/site-packages/mxnet/__init__.py"", line 31, in <module>
    from . import contrib
  File ""/home/lews/anaconda3/envs/gluon/lib/python3.8/site-packages/mxnet/contrib/__init__.py"", line 31, in <module>
    from . import onnx
  File ""/home/lews/anaconda3/envs/gluon/lib/python3.8/site-packages/mxnet/contrib/onnx/__init__.py"", line 22, in <module>
    from ...onnx import export_model as export_model_
ModuleNotFoundError: No module named 'mxnet.onnx'
`
and when importing GluonCV I got the error:
`File ""/home/lews/anaconda3/envs/gluon/lib/python3.8/site-packages/gluoncv/__init__.py"", line 33, in <module>
    raise ImportError('Unable to import modules due to missing `mxnet` & `torch`. '
ImportError: Unable to import modules due to missing `mxnet` & `torch`. You should install at least one deep learning framework.`
Clearly, the exporting script still doesn't work, nor does anything else MXNet related.


On the same machine, I tried creating a new virtual environment with:
mxnet (1.8.0.post0)
gluoncv (0.10.4.post0)

as before, everything works ok (generically using MXNet/GluonCV models), and the ONNX export doesn't. The error this time is different:
`AttributeError: No conversion function registered for op type _greater_scalar yet.`
I tried the update script again, and the problems are the same:
*) when importing mxnet
`ModuleNotFoundError: No module named 'mxnet.onnx'
`
*) and, when importing gluoncv:
`ImportError: Unable to import modules due to missing `mxnet` & `torch`. You should install at least one deep learning framework.
`

I still have everything working on the old machine, but I thought it might be useful the problem. :)
",IssueComment,https://github.com/apache/mxnet/issues/14875#issuecomment-887301408,LewsTherin511,2021-07-27 08:04:07,14875,[14942],Deployment bug,0,"Hi! Thanks again for your assistance last time, it worked perfectly! However, I noticed something weird. When I first used your script, I was working on another computer, and everything went ok. Yesterday, I tried exporting to ONNX on another machine, and I got the usual error: [code] The machine I'm having problem with has gluoncv (0.10.4.post0) mxnet-mkl (1.6.0) onnx (1.9.0) So, I tried the update script, but running it seems to somehow break the mxnet installation. After the update, whenever I try importing mxnet, I get the error: [code] and when importing GluonCV I got the error: [code]mxnet[code]torch[code]mxnet[code]torch[code] Clearly, the exporting script still doesn't work, nor does anything else MXNet related. On the same machine, I tried creating a new virtual environment with: mxnet (1.8.0.post0) gluoncv (0.10.4.post0) as before, everything works ok (generically using MXNet/GluonCV models), and the ONNX export doesn't. The error this time is different: [code] I tried the update script again, and the problems are the same: *) when importing mxnet [code] *) and, when importing gluoncv: [code]mxnet[code]torch[code] I still have everything working on the old machine, but I thought it might be useful the problem. :)"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Build",IssueComment,https://github.com/apache/mxnet/issues/14895#issuecomment-489710221,mxnet-label-bot,2019-05-06 17:46:16,14895,[14987],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Build"
"Looking into this, this seems to be true for all the links. since we convert our md docs to html for the docs website, looks like html links are hardcoded",IssueComment,https://github.com/apache/mxnet/issues/14895#issuecomment-489805687,vrakesh,2019-05-06 22:26:24,14895,[14987],Build bug,0,"Looking into this, this seems to be true for all the links. since we convert our md docs to html for the docs website, looks like html links are hardcoded"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15029#issuecomment-494611137,mxnet-label-bot,2019-05-22 00:53:11,15029,[15041],Data bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
The coredump does not happen on MacOS with 16GB memory. It occurrs on ubuntu 16.04,IssueComment,https://github.com/apache/mxnet/issues/15029#issuecomment-494678170,apeforest,2019-05-22 06:59:34,15029,[15041],Data bug,1,The coredump does not happen on MacOS with 16GB memory. It occurrs on ubuntu 16.04
"Here is the stack trace from GDB
```
#52347 0x00007fff9fcb71e9 in std::uniform_int_distribution<int>::operator()<std::mersenne_twister_engine<unsigned long, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul> > (this=0x7fff2a5f4c80, __urng=...)
    at /usr/include/c++/5/bits/uniform_int_dist.h:165
#52348 0x00007fff9fcae86b in mxnet::op::(anonymous namespace)::<lambda(mxnet::index_t)>::operator()(mxnet::index_t) const (__closure=0x7fff2a5f61e0,
    n=-1) at src/operator/random/shuffle_op.cc:50
#52349 0x00007fff9fcb66e3 in __gnu_parallel::__parallel_random_shuffle_drs<long*, void mxnet::op::(anonymous namespace)::Shuffle1D<long, std::mersenne_twister_engine<unsigned long, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul> >(long*, int, std::mersenne_twister_engine<unsigned long, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul>*)::{lambda(int)#1}> () at /usr/include/c++/5/parallel/random_shuffle.h:384
#52350 0x00007ffff34d0638 in __kmp_api_GOMP_parallel_40_alias () from /home/ubuntu/src/mxnet/python/mxnet/../../lib/libiomp5.so
#52351 0x00007fff9fcb3bbd in __gnu_parallel::__parallel_random_shuffle_drs<long int*, mxnet::op::(anonymous namespace)::Shuffle1D(DType*, mxnet::index_t, Rand*) [with DType = long int; Rand = std::mersenne_twister_engine<long unsigned int, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul>; mxnet::index_t = int]::<lambda(mxnet::index_t)> >(long *, long *, std::iterator_traits<long*>::difference_type, __gnu_parallel::_ThreadIndex, mxnet::op::(anonymous namespace)::<lambda(mxnet::index_t)> &) (__begin=0x7fff70010040, __end=0x7fff7013baf8,
    __n=153431, __num_threads=16, __rng=...) at /usr/include/c++/5/parallel/random_shuffle.h:342
#52352 0x00007fff9fcb1a0f in __gnu_parallel::__parallel_random_shuffle<long int*, mxnet::op::(anonymous namespace)::Shuffle1D(DType*, mxnet::index_t, Rand*) [with DType = long int; Rand = std::mersenne_twister_engine<long unsigned int, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul>; mxnet::index_t = int]::<lambda(mxnet::index_t)> >(long *, long *, mxnet::op::(anonymous namespace)::<lambda(mxnet::index_t)>) (__begin=0x7fff70010040, __end=0x7fff7013baf8, __rng=...) at /usr/include/c++/5/parallel/random_shuffle.h:528
#52353 0x00007fff9fcaf2b7 in std::__parallel::random_shuffle<long int*, mxnet::op::(anonymous namespace)::Shuffle1D(DType*, mxnet::index_t, Rand*) [with DType = long int; Rand = std::mersenne_twister_engine<long unsigned int, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul>; mxnet::index_t = int]::<lambda(mxnet::index_t)>&>(long *, long *, mxnet::op::(anonymous namespace)::<lambda(mxnet::index_t)> &) (__begin=0x7fff70010040, __end=0x7fff7013baf8, __rand=...) at /usr/include/c++/5/parallel/algo.h:1681
#52354 0x00007fff9fcae8d3 in mxnet::op::(anonymous namespace)::Shuffle1D<long, std::mersenne_twister_engine<unsigned long, 32ul, 624ul, 397ul, 31ul, 2567483615ul, 11ul, 4294967295ul, 7ul, 2636928640ul, 15ul, 4022730752ul, 18ul, 1812433253ul> > (out=0x7fff70010040, size=153431, prnd=0x20c0470)
    at src/operator/random/shuffle_op.cc:52
#52355 0x00007fff9fcacac9 in mxnet::op::ShuffleForwardCPU (attrs=..., ctx=..., inputs=std::vector of length 1, capacity 1 = {...},
    req=std::vector of length 1, capacity 1 = {...}, outputs=std::vector of length 1, capacity 1 = {...}) at src/operator/random/shuffle_op.cc:98
#52356 0x00007fff9f7e2ced in std::_Function_handler<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&), void (*)(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::_M_invoke(std::_Any_data const&, nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) (__functor=..., __args#0=..., __args#1=...,
    __args#2=std::vector of length 1, capacity 1 = {...}, __args#3=std::vector of length 1, capacity 1 = {...},
    __args#4=std::vector of length 1, capacity 1 = {...}) at /usr/include/c++/5/functional:1871
#52357 0x00007fff9f6bd1c4 in std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> ---Type <return> to continue, or q <return> to quit---
> const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::operator()(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) const (this=0x20c18d8, __args#0=...,
    __args#1=..., __args#2=std::vector of length 1, capacity 1 = {...}, __args#3=std::vector of length 1, capacity 1 = {...},
    __args#4=std::vector of length 1, capacity 1 = {...}) at /usr/include/c++/5/functional:2267
#52358 0x00007fffa25ed60d in mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const (__closure=0x20c1850, rctx=...) at src/imperative/./imperative_utils.h:434
#52359 0x00007fffa25f26e6 in std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&) (__functor=...,
    __args#0=<unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x7af4db2, DIE 0x7bae75a>)
    at /usr/include/c++/5/functional:1871
#52360 0x00007fffa2eb67d8 in std::function<void (mxnet::RunContext)>::operator()(mxnet::RunContext) const (this=0x209eca0, __args#0=...)
    at /usr/include/c++/5/functional:2267
#52361 0x00007fffa2ecc22f in mxnet::engine::ThreadedEngine::<lambda(mxnet::RunContext, mxnet::Engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::Engine::CallbackOnComplete) const (__closure=0x209eca0, ctx=..., on_complete=...) at src/engine/threaded_engine.cc:350
#52362 0x00007fffa2ecd7ae in std::_Function_handler<void(mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::PushSync(mxnet::Engine::SyncFn, mxnet::Context, const std::vector<mxnet::engine::Var*>&, const std::vector<mxnet::engine::Var*>&, mxnet::FnProperty, int, char const*)::<lambda(mxnet::RunContext, mxnet::Engine::CallbackOnComplete)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x95cc75b, DIE 0x96109b3>, <unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x95cc75b, DIE 0x96109b8>) (__functor=..., __args#0=<unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x95cc75b, DIE 0x96109b3>,
    __args#1=<unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x95cc75b, DIE 0x96109b8>)
    at /usr/include/c++/5/functional:1871
#52363 0x00007fffa2eb783a in std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (this=0x1ed4000, __args#0=..., __args#1=...) at /usr/include/c++/5/functional:2267
#52364 0x00007fffa2ebf597 in mxnet::engine::ThreadedEngine::ExecuteOprBlock (this=0x1ed2a50, run_ctx=..., opr_block=0x1ed6000)
    at src/engine/./threaded_engine.h:380
#52365 0x00007fffa2ed672a in mxnet::engine::ThreadedEnginePerDevice::CPUWorker<(dmlc::ConcurrentQueueType)0> (this=0x1ed2a50, ctx=..., block=0x2091600,
    ready_event=std::shared_ptr (count 2, weak 0) 0x1e0ac40) at src/engine/threaded_engine_perdevice.cc:300
#52366 0x00007fffa2ed4a70 in mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}::operator()(dmlc::ManualEvent) const (__closure=0xb0ebb0,
    ready_event=std::shared_ptr (count 2, weak 0) 0x1e0ac40) at src/engine/threaded_engine_perdevice.cc:116
#52367 0x00007fffa2ed93a6 in std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::sha---Type <return> to continue, or q <return> to quit---
red_ptr<dmlc::ManualEvent>&&) (__functor=...,
    __args#0=<unknown type in /home/ubuntu/src/mxnet/python/mxnet/../../lib/libmxnet.so, CU 0x963de09, DIE 0x96be975>)
    at /usr/include/c++/5/functional:1871
#52368 0x00007fffa2ecb3d7 in std::function<void (std::shared_ptr<dmlc::ManualEvent>)>::operator()(std::shared_ptr<dmlc::ManualEvent>) const (
    this=0x2081c38, __args#0=std::shared_ptr (empty) 0x0) at /usr/include/c++/5/functional:2267
#52369 0x00007fffa2ecb34a in std::_Bind_simple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)> (std::shared_ptr<dmlc::ManualEvent>)>::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x2081c28) at /usr/include/c++/5/functional:1531
#52370 0x00007fffa2ecb1de in std::_Bind_simple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)> (std::shared_ptr<dmlc::ManualEvent>)>::operator()() (this=0x2081c28) at /usr/include/c++/5/functional:1520
#52371 0x00007fffa2ecb12e in std::thread::_Impl<std::_Bind_simple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)> (std::shared_ptr<dmlc::ManualEvent>)> >::_M_run() (this=0x2081c10) at /usr/include/c++/5/thread:115
#52372 0x00007fffee543c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#52373 0x00007ffff7bc16ba in start_thread (arg=0x7fff2a5f7700) at pthread_create.c:333
#52374 0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```",IssueComment,https://github.com/apache/mxnet/issues/15029#issuecomment-494682241,apeforest,2019-05-22 07:13:24,15029,[15041],Data bug,1,Here is the stack trace from GDB ``[code]``
"@mxnet-label-bot add [question, bug]
",IssueComment,https://github.com/apache/mxnet/issues/14993#issuecomment-493732222,vdantu,2019-05-19 06:55:47,14993,[15066],Documentation bug,0,"@mxnet-label-bot add [question, bug]"
"Thank you for the fix @zhouhang95. If you are interested in contributing to this open source project, feel free to open a pull request fixing those docs at https://github.com/zachgk/incubator-mxnet/blob/master/python/mxnet/image/image.py#L94.",IssueComment,https://github.com/apache/mxnet/issues/14993#issuecomment-495425215,zachgk,2019-05-23 23:57:19,14993,[15066],Documentation bug,0,"Thank you for the fix @zhouhang95. If you are interested in contributing to this open source project, feel free to open a pull request fixing those docs at [url]#L94."
@mxnet-label-bot add [Doc],IssueComment,https://github.com/apache/mxnet/issues/14993#issuecomment-495778342,leleamol,2019-05-24 20:27:12,14993,[15066],Documentation bug,0,@mxnet-label-bot add [Doc]
@mxnet-label-bot add[Doc],IssueComment,https://github.com/apache/mxnet/issues/14993#issuecomment-495778452,pinaraws,2019-05-24 20:27:29,14993,[15066],Documentation bug,0,@mxnet-label-bot add[Doc]
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15062#issuecomment-495737314,mxnet-label-bot,2019-05-24 18:10:30,15062,[15081],Algorithm design bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
@szha ,IssueComment,https://github.com/apache/mxnet/issues/15062#issuecomment-495814840,eric-haibin-lin,2019-05-24 23:42:10,15062,[15081],Algorithm design bug,0,@szha
"@mxnet-label-bot add[Bug, Gluon]",IssueComment,https://github.com/apache/mxnet/issues/15062#issuecomment-495876374,roywei,2019-05-25 07:27:35,15062,[15081],Algorithm design bug,0,"@mxnet-label-bot add[Bug, Gluon]"
"The problem is in the inconsistent behavior of split, where if the number of splits is 1 it returns a single NDArray instead of a list.",IssueComment,https://github.com/apache/mxnet/issues/15062#issuecomment-496284978,szha,2019-05-27 18:51:39,15062,[15081],Algorithm design bug,0,"The problem is in the inconsistent behavior of split, where if the number of splits is 1 it returns a single NDArray instead of a list."
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Doc",IssueComment,https://github.com/apache/mxnet/issues/15214#issuecomment-500885486,mxnet-label-bot,2019-06-11 15:09:25,15214,[15221],Data bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Doc"
@zhreshold Can you take a look?,IssueComment,https://github.com/apache/mxnet/issues/15214#issuecomment-501074995,zachgk,2019-06-12 00:36:07,15214,[15221],Data bug,1,@zhreshold Can you take a look?
"Your point is right, would you like to submit a fix for it?",IssueComment,https://github.com/apache/mxnet/issues/15214#issuecomment-501080332,zhreshold,2019-06-12 01:06:16,15214,[15221],Data bug,1,"Your point is right, would you like to submit a fix for it?"
I guess that fixes the problem! ,IssueComment,https://github.com/apache/mxnet/issues/15214#issuecomment-501165943,cesans,2019-06-12 08:06:29,15214,[15221],Data bug,1,I guess that fixes the problem!
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15239#issuecomment-501860789,mxnet-label-bot,2019-06-13 20:11:31,15239,[15242],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"@mxnet-label-bot Add [Bug, Sparse, Python]",IssueComment,https://github.com/apache/mxnet/issues/15239#issuecomment-501860882,piyushghai,2019-06-13 20:11:49,15239,[15242],Data bug,0,"@mxnet-label-bot Add [Bug, Sparse, Python]"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506014644,mxnet-label-bot,2019-06-26 19:33:43,15375,[15405],Memory bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
@mxnet-label-bot add [bug],IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506045142,vdantu,2019-06-26 21:06:30,15375,[15405],Memory bug,0,@mxnet-label-bot add [bug]
I agree with you about fixing  the memory leak in the NaiveEngine and thanks for raising that. But i disagree about the need to refactor the release of operator memory in the threaded engine. This was a conscious design decision to use Object pool for operator objects which would be manually removed from and added back to the pool. This removal and addition was supposed to be managed by graph executors and other modules that push operators to the engine. If readability is really the issue that is very subjective and  I don't really see the benefit trump the cost here.,IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506535345,anirudh2290,2019-06-27 22:33:59,15375,[15405],Memory bug,0,I agree with you about fixing the memory leak in the NaiveEngine and thanks for raising that. But i disagree about the need to refactor the release of operator memory in the threaded engine. This was a conscious design decision to use Object pool for operator objects which would be manually removed from and added back to the pool. This removal and addition was supposed to be managed by graph executors and other modules that push operators to the engine. If readability is really the issue that is very subjective and I don't really see the benefit trump the cost here.
"I agree with Anirudh.

I didn’t write the object pool code, but the reason it was done that way was to
avoid a new/delete operation in-line to the profiling, thus causing profiling to introduce possibly extra locks (and associated heap code), depending on the allocation library used, causing the “Observer Effect”, which is not desired in profiling, of all things. Ideally, the naive engine would use the same mechanism (a better change, imho), but since it’s not used as much for production workloads, I suppose they didn’t think it was worth it to code it. ",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506577210,cjolivier01,2019-06-28 02:32:32,15375,[15405],Memory bug,0,"I agree with Anirudh. I didn’t write the object pool code, but the reason it was done that way was to avoid a new/delete operation in-line to the profiling, thus causing profiling to introduce possibly extra locks (and associated heap code), depending on the allocation library used, causing the “Observer Effect”, which is not desired in profiling, of all things. Ideally, the naive engine would use the same mechanism (a better change, imho), but since it’s not used as much for production workloads, I suppose they didn’t think it was worth it to code it."
"@anirudh2290 I proposed refactoring around a unique_ptr I don't think is going to change the way memory is released, maybe you got a different impression of what I was suggesting to do. I don't know why are you oposing a proposed change right away without considering the benefits. Having a pointer released in a call hiearchy 3 levels down leads to bugs and memory leaks, is difficult to reason about it. Best is to use RAII and return a unique_ptr or similar which holds ownership. I don't think the way the release of resources is going to be modified or performance impacted in any way, but If the proposal is blocked from the start makes no sense to even propose a PR, this is not very encouraging.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506843795,larroy,2019-06-28 19:00:27,15375,[15405],Memory bug,0,"@anirudh2290 I proposed refactoring around a unique_ptr I don't think is going to change the way memory is released, maybe you got a different impression of what I was suggesting to do. I don't know why are you oposing a proposed change right away without considering the benefits. Having a pointer released in a call hiearchy 3 levels down leads to bugs and memory leaks, is difficult to reason about it. Best is to use RAII and return a unique_ptr or similar which holds ownership. I don't think the way the release of resources is going to be modified or performance impacted in any way, but If the proposal is blocked from the start makes no sense to even propose a PR, this is not very encouraging."
"@cjolivier01 I didn't understand a word of what you are writing. I'm not proposing to change the object pool, just make NewOp return a smart pointer.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506846181,larroy,2019-06-28 19:09:06,15375,[15405],Memory bug,0,"@cjolivier01 I didn't understand a word of what you are writing. I'm not proposing to change the object pool, just make NewOp return a smart pointer."
"@larroy even less encouraging is having you make a PR and then blocking it. I was trying to save you some time. Ownership of the pointer rests with the module that pushes it. So imperative or graph executor should handle it. If you are really keen on doing this do it in the graph executor. There is no need to touch the engine code here.

> Having a pointer released in a call hierarchy 3 levels down leads to bugs and memory leaks, is difficult to reason about it. 

This is the design, Engine relies on object pool for allocation, and the modules depend on engine to handle this, the modules just tell them when it wants a new operator creator or destroyed.
I think the code is very logical and it makes sense to me. Also, the memory leak you found is nowhere related to the ThreadedEngine code.

I considered the benefits of your approach: ""readability/understandability"" in the first comment.

",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506851797,anirudh2290,2019-06-28 19:31:25,15375,[15405],Memory bug,0,"@larroy even less encouraging is having you make a PR and then blocking it. I was trying to save you some time. Ownership of the pointer rests with the module that pushes it. So imperative or graph executor should handle it. If you are really keen on doing this do it in the graph executor. There is no need to touch the engine code here. > Having a pointer released in a call hierarchy 3 levels down leads to bugs and memory leaks, is difficult to reason about it. This is the design, Engine relies on object pool for allocation, and the modules depend on engine to handle this, the modules just tell them when it wants a new operator creator or destroyed. I think the code is very logical and it makes sense to me. Also, the memory leak you found is nowhere related to the ThreadedEngine code. I considered the benefits of your approach: ""readability/understandability"" in the first comment."
"I will make a small fix to the leak in Naive engine, and a separate PR with a refactor, so we can have a more focused and productive conversation then. Thanks for your feedback.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506863130,larroy,2019-06-28 20:14:03,15375,[15405],Memory bug,0,"I will make a small fix to the leak in Naive engine, and a separate PR with a refactor, so we can have a more focused and productive conversation then. Thanks for your feedback."
"@anirudh2290 it makes sense to you because you know the code, I would encourage you to read about the benfits of RAII. This is orthogonal to having an object pool for allocation, which I was not proposing to change.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506873322,larroy,2019-06-28 20:52:38,15375,[15405],Memory bug,0,"@anirudh2290 it makes sense to you because you know the code, I would encourage you to read about the benfits of RAII. This is orthogonal to having an object pool for allocation, which I was not proposing to change."
"> it makes sense to you because you know the code

So who are you targeting, someone unfamiliar with the code. Everyone needs to spend some time with the code to understand it, but after they spend that time it makes sense.

I know the benefits of RAII, but it doesn't apply to this use case of threaded engine. I am saying there is no need to change threaded engine code here which you are proposing to do. RAII won't benefit here, because engine itself is only responsible for indicating to the object pool that the object has to be deleted. The rest is taken care by the object pool itself. As I said if you really want to add RAII, do it in the graph executor (although I don't think it is really needed there either).",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506877531,anirudh2290,2019-06-28 21:08:46,15375,[15405],Memory bug,0,"> it makes sense to you because you know the code So who are you targeting, someone unfamiliar with the code. Everyone needs to spend some time with the code to understand it, but after they spend that time it makes sense. I know the benefits of RAII, but it doesn't apply to this use case of threaded engine. I am saying there is no need to change threaded engine code here which you are proposing to do. RAII won't benefit here, because engine itself is only responsible for indicating to the object pool that the object has to be deleted. The rest is taken care by the object pool itself. As I said if you really want to add RAII, do it in the graph executor (although I don't think it is really needed there either)."
"Maybe you know the code better and what I propose doesn't make sense, my proposal was to wrap the operator in a unique_ptr with a custom deleter with a virtual apply operator, so delete doesn't need to be called here:

https://github.com/apache/incubator-mxnet/blob/master/src/engine/threaded_engine.cc#L472

But it will still use the object pool, this wouldn't be changed.

Did I understand correctly that you think this is not possible or that is better to explicitly delete the operator in this case?

The leak would not be in the first place if you would always pass around a smart pointer from the operator, that was my point, maybe I'm missing something which really needs the raw pointer around, but couldn't you pass around a unique_ptr that gets freed from the pool when needed? having the pointer deleted manually is not exactly exception safe.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506881339,larroy,2019-06-28 21:23:40,15375,[15405],Memory bug,0,"Maybe you know the code better and what I propose doesn't make sense, my proposal was to wrap the operator in a unique_ptr with a custom deleter with a virtual apply operator, so delete doesn't need to be called here: [url]#L472 But it will still use the object pool, this wouldn't be changed. Did I understand correctly that you think this is not possible or that is better to explicitly delete the operator in this case? The leak would not be in the first place if you would always pass around a smart pointer from the operator, that was my point, maybe I'm missing something which really needs the raw pointer around, but couldn't you pass around a unique_ptr that gets freed from the pool when needed? having the pointer deleted manually is not exactly exception safe."
"First when you talk about leak, you talk about naive, which does things differently from threaded.

Coming to threaded, What will be the scope and lifetime of the unique_ptr, that you will be creating, how will dependency engine know when it has to be deleted ? It has individual view of operators not the full graph view.

I have spent a lot of time here, having this discussion, on something that hardly adds any value. I have said all I have to say.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506886676,anirudh2290,2019-06-28 21:46:33,15375,[15405],Memory bug,0,"First when you talk about leak, you talk about naive, which does things differently from threaded. Coming to threaded, What will be the scope and lifetime of the unique_ptr, that you will be creating, how will dependency engine know when it has to be deleted ? It has individual view of operators not the full graph view. I have spent a lot of time here, having this discussion, on something that hardly adds any value. I have said all I have to say."
"> I have spent a lot of time here, having this discussion, on something that hardly adds any value. I have said all I have to say.

Hi @anirudh2290 
I don't think this is very constructive to conclude your reply with that, if you are asking questions. I was trying to understand your point and explain my proposal. If you don't have time to followup, no need to start a discussion in the first place. I just made a suggestion on this ticket to prevent similar kinds of leaks in other places using this code. I respect that you have your own opinion, but for me having unmanaged pointers in many places with a complex delete pattern is creating an environment when reasoning about the code is hard and prone to bugs. Saying that something hardly adds any value is not very constructive if you don't try to understand the problem first. Cheers.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506900904,larroy,2019-06-28 23:03:42,15375,[15405],Memory bug,0,"> I have spent a lot of time here, having this discussion, on something that hardly adds any value. I have said all I have to say. Hi @anirudh2290 I don't think this is very constructive to conclude your reply with that, if you are asking questions. I was trying to understand your point and explain my proposal. If you don't have time to followup, no need to start a discussion in the first place. I just made a suggestion on this ticket to prevent similar kinds of leaks in other places using this code. I respect that you have your own opinion, but for me having unmanaged pointers in many places with a complex delete pattern is creating an environment when reasoning about the code is hard and prone to bugs. Saying that something hardly adds any value is not very constructive if you don't try to understand the problem first. Cheers."
"Okay, I will say it again: the leak is in naive engine and not the threaded engine. Also, let me say again for the threaded engine, there is no problem here. I would request you to spend time fixing some real issues that mxnet has. 

I am going to close this issue, please feel free to open a new one for the naive engine memory leak.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506902499,anirudh2290,2019-06-28 23:13:51,15375,[15405],Memory bug,0,"Okay, I will say it again: the leak is in naive engine and not the threaded engine. Also, let me say again for the threaded engine, there is no problem here. I would request you to spend time fixing some real issues that mxnet has. I am going to close this issue, please feel free to open a new one for the naive engine memory leak."
Did you read the title of the issue? Why are you closing this? it says the leak is in the NaiveEngine when profiling. I said the leak is in Naive Engine since the beginning.,IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506904136,larroy,2019-06-28 23:24:24,15375,[15405],Memory bug,0,Did you read the title of the issue? Why are you closing this? it says the leak is in the NaiveEngine when profiling. I said the leak is in Naive Engine since the beginning.
"This thread has been derailed a lot from the original issue and the whole discussion has been about threaded engine where there is no issue. I will keep this closed, feel free to open a new issue and keep the discussion focused to naive engine.",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506907195,anirudh2290,2019-06-28 23:48:24,15375,[15405],Memory bug,0,"This thread has been derailed a lot from the original issue and the whole discussion has been about threaded engine where there is no issue. I will keep this closed, feel free to open a new issue and keep the discussion focused to naive engine."
"Attaching valgrind run which shows the leaked memory.

![naive_engine_leak](https://user-images.githubusercontent.com/928489/60377426-c12eee80-99cb-11e9-81fc-a15ffb71a799.png)
",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506912535,larroy,2019-06-29 00:40:18,15375,[15405],Memory bug,0,Attaching valgrind run which shows the leaked memory. ![naive_engine_leak]([url]
"
[valgrind.txt](https://github.com/apache/incubator-mxnet/files/3341240/valgrind.txt)
",IssueComment,https://github.com/apache/mxnet/issues/15375#issuecomment-506912862,larroy,2019-06-29 00:44:42,15375,[15405],Memory bug,0,[valgrind.txt]([url]
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Build",IssueComment,https://github.com/apache/mxnet/issues/15612#issuecomment-513335352,mxnet-label-bot,2019-07-19 18:42:37,15612,[15601],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Build"
"@samskalicky Thanks for reporting this Sam, and thanks for the quick PR as well

@mxnet-label-bot add  [Build, Bug]",IssueComment,https://github.com/apache/mxnet/issues/15612#issuecomment-513370894,vrakesh,2019-07-19 20:42:22,15612,[15601],Build bug,0,"@samskalicky Thanks for reporting this Sam, and thanks for the quick PR as well @mxnet-label-bot add [Build, Bug]"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug, C++",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-511704287,mxnet-label-bot,2019-07-16 07:42:21,15555,[15637],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug, C++"
"@mxnet-label-bot add [Bug, C++]",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-511705934,kostayScr,2019-07-16 07:47:40,15555,[15637],Data bug,0,"@mxnet-label-bot add [Bug, C++]"
@ZhennanQin could you help take a look for this ?,IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512102501,pengzhao-intel,2019-07-17 04:59:52,15555,[15637],Data bug,0,@ZhennanQin could you help take a look for this ?
"@kostayScr I can't reproduce the crash on my linux box. Probably because:
```
        ArgMap args;
	ArgMap auxiliaryState;
	ArgMap bestModelArgs;
	ArgMap bestModelAux;
```
are all empty in your case, so no copy actually happen. Can you provide a real case that can reproduce this issue?",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512137023,ZhennanQin,2019-07-17 07:26:35,15555,[15637],Data bug,0,"@kostayScr I can't reproduce the crash on my linux box. Probably because: ``[code]`` are all empty in your case, so no copy actually happen. Can you provide a real case that can reproduce this issue?"
"Hello, thanks for looking into it.
Here is more complete code:
```
void TestWaitMapMXNetBug()
{
	using namespace mxnet::cpp;
	using ArgMap = map<string, NDArray>;

	const size_t testDataSz = 10 * 1024 * 1024;

	float *testDataOnes = new float[testDataSz];
	float *testDataZeroes = new float[testDataSz];
	for ( size_t i = 0; i < testDataSz; i++ )
	{
		testDataOnes[i] = 1.0f;
	}
	for ( size_t i = 0; i < testDataSz; i++ )
	{
		testDataZeroes[i] = 0.0f;
	}
	//NDArray testOnes( testDataOnes, testDataSz ); //crash
	//NDArray testZeroes( testDataZeroes, testDataSz );
	NDArray testOnes( testDataOnes, Shape( testDataSz ), Context::cpu() );
	NDArray testZeroes( testDataZeroes, Shape( testDataSz ), Context::cpu() );

	auto CheckNDArrayValues = []( const NDArray &arr, const float val ) //check that all values in array equal to val
	{
		auto arrData = arr.GetData();
		for ( size_t i = 0; i < arr.Size(); i++ )
		{
			if ( arrData[i] != val )
				return false;
		}
		return true;
	};

	ArgMap args;
	ArgMap auxiliaryState;
	ArgMap bestModelArgs;
	ArgMap bestModelAux;
		
	auto testName = ""testName""s;
	args[testName] = testOnes;
	bestModelArgs[testName] = testZeroes;

	//initial test
	std::cout << CheckNDArrayValues( args[testName], 1.0f );
	std::cout << CheckNDArrayValues( bestModelArgs[testName], 0.0f );

	auto AssignNDArrayMap = []( decltype( args ) &lhs, const decltype( args ) &rhs )
	{
		for ( auto &[k, arr] : rhs )
			arr.CopyTo( &lhs.at( k ) );
	};
	auto WaitMap = []( auto &map )
	{
		return; //Workaround - commenting this return statement fixes the problem
		for ( auto &[k, v] : map )
		{
			v.WaitAll();
			v.WaitToRead();
			v.WaitToWrite();
		}
	};
	auto SaveAsBestModel = [ &WaitMap, &bestModelArgs, &bestModelAux, &AssignNDArrayMap, &args, &auxiliaryState ]()
	{
		AssignNDArrayMap( bestModelArgs, args );
		AssignNDArrayMap( bestModelAux, auxiliaryState );
		WaitMap( bestModelArgs );
		WaitMap( bestModelAux );
	};
	auto LoadBestModel = [ &WaitMap, &bestModelArgs, &bestModelAux, &AssignNDArrayMap, &args, &auxiliaryState ]()
	{
		AssignNDArrayMap( args, bestModelArgs );
		AssignNDArrayMap( auxiliaryState, bestModelAux );
		WaitMap( args );
		WaitMap( auxiliaryState );
	};
	//copy
	SaveAsBestModel();  //a -> b
	LoadBestModel();	//b -> a

	//Data in args/auxiliaryState is now Corrupt!
	//test for corruption, now both must contain ones
	std::cout << CheckNDArrayValues( bestModelArgs[testName], 1.0f );
	std::cout << CheckNDArrayValues( args[testName], 1.0f );

	//bestModelArgs got corrupt, maybe it's all zeroes?
	std::cout << CheckNDArrayValues( bestModelArgs[testName], 0.0f );
}
```

Works(fails the check) only in Release build mode for me.
Correct output:
```
true
true
true
true
false
```
Factual output:
```
true
true
false
true
false
```
Enabling the waiting on the NDArrays fixes it.
I also want to clarify that there is no crash - just data corruption.",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512166732,kostayScr,2019-07-17 08:57:33,15555,[15637],Data bug,0,"Hello, thanks for looking into it. Here is more complete code: ``[code]`[code]`[code]`[code]`[code]`` Enabling the waiting on the NDArrays fixes it. I also want to clarify that there is no crash - just data corruption."
"@kostayScr Thanks for providing this case. Confirm that this is reproducible on my linux box. I'm trying to root cause this problem. I also found that naive engine doesn't have this problem, so for workaround, you can switch to naive engine by 
```
export MXNET_ENGINE_TYPE=NaiveEngine
```
 ",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512635746,ZhennanQin,2019-07-18 02:08:17,15555,[15637],Data bug,0,"@kostayScr Thanks for providing this case. Confirm that this is reproducible on my linux box. I'm trying to root cause this problem. I also found that naive engine doesn't have this problem, so for workaround, you can switch to naive engine by ``[code]``"
"@kostayScr After some analysis, I think it's not a bug, but a engine feature. Actually, the order of a->b and b->a is guaranteed by engine, but as `CopyTo` is executed asynchronously, you shouldn't access a NDArray until all operations are done which are pending on it. So inside `CheckNDArrayValues`, you need to add `WaitAll()` for the checked NDArray. The code would be like
```
  auto CheckNDArrayValues = [](const NDArray &arr, const float val) {
    arr.WaitAll();
    auto arrData = arr.GetData();
    for (size_t i = 0; i < arr.Size(); i++) {
      if (arrData[i] != val) return false;
    }
    return true;
  };
```
With this change, you can remove `WaitMap` and get expected result.

In summary, you don't need to wait between `a->b` and `b-a` as engine can make the guarantee. But you need to wait for all operations are finished before accessing `a`. ",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512697381,ZhennanQin,2019-07-18 07:21:45,15555,[15637],Data bug,0,"@kostayScr After some analysis, I think it's not a bug, but a engine feature. Actually, the order of a->b and b->a is guaranteed by engine, but as [code] is executed asynchronously, you shouldn't access a NDArray until all operations are done which are pending on it. So inside [code], you need to add [code] for the checked NDArray. The code would be like ``[code]`[code]WaitMap[code]a->b[code]b-a[code]a`."
"@ZhennanQin I see. But the original problem was with Forward/SyncCopyToCpu() rather than GetData(). I had the impression that GetData() will wait as well, like it does in Python I believe. So I just used it instead without checking(it was shorter). It probably needs a large font warning about waiting before calling it.

Trying to make a reproduction snippet currently. It might be more complex than this, in the original code, the 'b' bunch of NDarrays went out of scope at the end, maybe that's it.",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-512743860,kostayScr,2019-07-18 09:40:43,15555,[15637],Data bug,0,"@ZhennanQin I see. But the original problem was with Forward/SyncCopyToCpu() rather than GetData(). I had the impression that GetData() will wait as well, like it does in Python I believe. So I just used it instead without checking(it was shorter). It probably needs a large font warning about waiting before calling it. Trying to make a reproduction snippet currently. It might be more complex than this, in the original code, the 'b' bunch of NDarrays went out of scope at the end, maybe that's it."
"@ZhennanQin I think I managed to pin it down:
```
auto ConvBlock = []( const Symbol &data, int num, int filterCount = 16, int dilation = 1 )
{
	auto convWeight = Symbol::Variable( ""weight_conv"" + std::to_string( num ) );
	auto convBias = Symbol::Variable( ""bias_conv"" + std::to_string( num ) );
	//stride can help perf w/o hurting performance much
	Symbol net = Convolution( data, convWeight, convBias, Shape( 1, 5 ), filterCount, Shape(), Shape( 1, dilation ) );
	net = Activation( net, ActivationActType::kRelu );
	return net;
};
void TestWaitMapMXNetBug()
{
	using namespace mxnet::cpp;
	using ArgMap = map<string, NDArray>;

	const size_t testDataSz = 1 * 1024 * 1024;

	float *testDataFirst = new float[testDataSz];
	float *testDataSecond = new float[testDataSz];
	for ( size_t i = 0; i < testDataSz; i++ ) //fill test data with pattern to later verify
	{
		testDataFirst[i] = i;
	}
	for ( size_t i = 0; i < testDataSz; i++ )
	{
		testDataSecond[i] = testDataSz - i - 1;
	}
	//NDArray testOnes( testDataOnes, testDataSz ); //crash
	//compare array to the first arr.Size() elements of pattern
	auto CheckNDArrayValuesPattern = []( NDArray &arr, const float *pattern, size_t patternSz )
	{
		//auto arrData = arr.GetData();
		std::unique_ptr<float[]> arrData( new float[arr.Size()] );
		arr.SyncCopyToCPU( arrData.get(), arr.Size() ); //Synchronous copy - calls WaitToRead on NDArray,
		assert( arr.Size() <= patternSz );
		auto res = memcmp( arrData.get(), pattern, arr.Size() * sizeof( float ) );
		return res == 0;
	};
	auto CheckNDArrMapValsPattern = [ &CheckNDArrayValuesPattern ]( auto &map_, const float *pattern, size_t patternSz )
	{
		for ( auto &[k, arr] : map_ )
		{
			if ( !CheckNDArrayValuesPattern( arr, pattern, patternSz ) )
				return false;
		}
		return true;
	};

	ArgMap args;
	ArgMap auxiliaryState;
	ArgMap bestModelArgs;
	ArgMap bestModelAux;
		

	auto SetMapPattern = []( auto &map_, const float *pattern, size_t patternSz )
	{
		for ( auto &[k, arr] : map_ )
		{
			if ( patternSz < arr.Size() )
				throw std::logic_error( __FUNCSIG__ );
			arr.SyncCopyFromCPU( pattern, arr.Size() );
		}
	};

	auto ctx = Context::cpu();
	args[""X""] = NDArray( Shape( 200, 1, 1, 1024 * 1), ctx ); //dummy, (batch_size, channel, height, width)
	Symbol data = Symbol::Variable( ""X"" );
	auto net = ConvBlock( data, 0, 16 );
	net = ConvBlock( net, 1, 16 );
	net = ConvBlock( net, 2, 16 );
	net.InferArgsMap( ctx, &args, args );
	auto exec = net.SimpleBind( ctx, args );
	SetMapPattern( args, testDataFirst, testDataSz );

	for ( auto &[k, v] : args )
		bestModelArgs[k] = NDArray( v.GetShape(), ctx );
	SetMapPattern( bestModelArgs, testDataSecond, testDataSz );

	qDebug() << CheckNDArrMapValsPattern( args, testDataFirst, testDataSz );
	qDebug() << CheckNDArrMapValsPattern( bestModelArgs, testDataSecond, testDataSz );

	auto AssignNDArrayMap = []( decltype( args ) &lhs, const decltype( args ) &rhs )
	{
		for ( auto &[k, arr] : rhs )
			arr.CopyTo( &lhs.at( k ) );
	};
	auto WaitMap = []( auto &map )
	{
		return; //Workaround - commenting this return statement fixes the problem
		for ( auto &[k, v] : map )
		{
			v.WaitAll();
			v.WaitToRead();
			v.WaitToWrite();
		}
	};
	auto SaveAsBestModel = [ &WaitMap, &bestModelArgs, &bestModelAux, &AssignNDArrayMap, &args, &auxiliaryState ]()
	{
		AssignNDArrayMap( bestModelArgs, args );
		AssignNDArrayMap( bestModelAux, auxiliaryState );
		WaitMap( bestModelArgs );
		WaitMap( bestModelAux );
	};
	auto LoadBestModel = [ &WaitMap, &bestModelArgs, &bestModelAux, &AssignNDArrayMap, &args, &auxiliaryState ]()
	{
		AssignNDArrayMap( args, bestModelArgs );
		AssignNDArrayMap( auxiliaryState, bestModelAux );
		WaitMap( args );
		WaitMap( auxiliaryState );
	};

	for ( int i = 0; i < 1000; ++i )
		exec->Forward( false );
	{
		//to make NDArray change it's storage type to the MKLDNN one
		vector<float> dummy( exec->outputs[0].Size() );
		exec->outputs[0].SyncCopyToCPU(&dummy);
	}

	//copy
	SaveAsBestModel();  //a -> b


	for ( auto &[k, v] : args )
	{
		//uncommenting this line must cause an exception:
		//qDebug() << k.c_str() << v.GetData();

		//exception info:
		//ndarray.cc:751
		//#if MXNET_USE_MKLDNN == 1
		//CHECK(!IsMKLDNNData()) << ""We can't generate TBlob for MKLDNN data. ""
		//<< ""Please use Reorder2Default() to generate a new NDArray first"";
		//#endif
	}
	//auto dataPtr1 = bestModelArgs.at( ""0"" ).GetData();
	LoadBestModel();	//b -> a

	//Data in args/auxiliaryState is now Corrupt!
	//test for corruption
	qDebug() << CheckNDArrMapValsPattern( args, testDataFirst, testDataSz );
	qDebug() << CheckNDArrMapValsPattern( bestModelArgs, testDataFirst, testDataSz );
}
```

Output should be all true, but it's:
```
true
true
false
false
```

It seems to be related to the data format (storage type) of the NDArrays. See comment in the code about exception - it's what hinted me at how to reproduce. I checked quite a few things before I got it reproducing, here is the uncleaned code, maybe it'll save some time: https://pastebin.com/RcHb6kdb (the commented code didn't reproduce the bug).",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-513219404,kostayScr,2019-07-19 13:00:01,15555,[15637],Data bug,0,"@ZhennanQin I think I managed to pin it down: ``[code]`[code]`[code]`` It seems to be related to the data format (storage type) of the NDArrays. See comment in the code about exception - it's what hinted me at how to reproduce. I checked quite a few things before I got it reproducing, here is the uncleaned code, maybe it'll save some time: [url] (the commented code didn't reproduce the bug)."
"@kostayScr Thanks for providing such detailed case. I can reproduce it locally. Some experiment shows that it's another unexpected behavior about `MKLDNNDataReorderAsync`.  Quote out it from mkldnn_convolution.cc can fix this issue, but inference speed will drop especially for small batch size. I'm trying to fix this without performance impact. Maybe need more days.",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-513708217,ZhennanQin,2019-07-22 09:08:30,15555,[15637],Data bug,0,"@kostayScr Thanks for providing such detailed case. I can reproduce it locally. Some experiment shows that it's another unexpected behavior about [code]. Quote out it from mkldnn_convolution.cc can fix this issue, but inference speed will drop especially for small batch size. I'm trying to fix this without performance impact. Maybe need more days."
"@ZhennanQin Yeah it was a slippery one, many pre-conditions(that's why the code is so long). The WaitMap function works fine as a workaround for me. Doesn't affect performance and results seem to be stable.",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-513760709,kostayScr,2019-07-22 11:55:41,15555,[15637],Data bug,0,"@ZhennanQin Yeah it was a slippery one, many pre-conditions(that's why the code is so long). The WaitMap function works fine as a workaround for me. Doesn't affect performance and results seem to be stable."
"@kostayScr I filed a PR to fix this: https://github.com/apache/incubator-mxnet/pull/15637. Local test works for me. But as you know, the problem is a bit complicated and random, so I wish you can have a try for your  real case to see if problem is really resolved. Thanks in advance.",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-514083096,ZhennanQin,2019-07-23 07:09:09,15555,[15637],Data bug,0,"@kostayScr I filed a PR to fix this: [url] Local test works for me. But as you know, the problem is a bit complicated and random, so I wish you can have a try for your real case to see if problem is really resolved. Thanks in advance."
"@ZhennanQin I applied the patch manually, then tried to reproduce the bug in my code(ran it 5 times). The bug didn't pop up so hopefully it is fixed now. Should I close the issue?",IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-514157620,kostayScr,2019-07-23 10:46:15,15555,[15637],Data bug,0,"@ZhennanQin I applied the patch manually, then tried to reproduce the bug in my code(ran it 5 times). The bug didn't pop up so hopefully it is fixed now. Should I close the issue?"
@kostayScr Thanks for the try. It's up to you that whether you want to trace the bug to be fixed from master:),IssueComment,https://github.com/apache/mxnet/issues/15555#issuecomment-514448116,ZhennanQin,2019-07-24 01:57:41,15555,[15637],Data bug,0,@kostayScr Thanks for the try. It's up to you that whether you want to trace the bug to be fixed from master:)
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Test",IssueComment,https://github.com/apache/mxnet/issues/15636#issuecomment-514070280,mxnet-label-bot,2019-07-23 06:19:53,15636,[15661],Test bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Test"
cc: @ChaiBapchya ,IssueComment,https://github.com/apache/mxnet/issues/15636#issuecomment-515613776,sandeep-krishnamurthy,2019-07-26 22:07:48,15636,[15661],Test bug,0,cc: @ChaiBapchya
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515628670,mxnet-label-bot,2019-07-26 23:32:27,15670,[15671],Build bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
@PatricZhao @ZhennanQin ,IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515629317,Zha0q1,2019-07-26 23:37:07,15670,[15671],Build bug,1,@PatricZhao @ZhennanQin
@mxnet-label-bot add [Bug],IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515629519,Zha0q1,2019-07-26 23:38:27,15670,[15671],Build bug,1,@mxnet-label-bot add [Bug]
"@Zha0q1 Thanks for reporting this. PR https://github.com/apache/incubator-mxnet/pull/15671 is created to fix this, your provided case can pass now. Please have a try for your real case to see if it works.",IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515640507,ZhennanQin,2019-07-27 01:24:31,15670,[15671],Build bug,1,"@Zha0q1 Thanks for reporting this. PR [url] is created to fix this, your provided case can pass now. Please have a try for your real case to see if it works."
@Zha0q1 please help double-check if the bug is fixed.,IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515732277,pengzhao-intel,2019-07-28 04:34:56,15670,[15671],Build bug,1,@Zha0q1 please help double-check if the bug is fixed.
"> @Zha0q1 please help double-check if the bug is fixed.

@pengzhao-intel  Thanks! I will l check and get back to this issue",IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515732305,Zha0q1,2019-07-28 04:35:39,15670,[15671],Build bug,1,> @Zha0q1 please help double-check if the bug is fixed. @pengzhao-intel Thanks! I will l check and get back to this issue
@pengzhao-intel @ZhennanQin My use case works now. Thanks!,IssueComment,https://github.com/apache/mxnet/issues/15670#issuecomment-515804150,Zha0q1,2019-07-28 23:29:03,15670,[15671],Build bug,1,@pengzhao-intel @ZhennanQin My use case works now. Thanks!
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Test",IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-514846259,mxnet-label-bot,2019-07-25 00:20:25,15652,[15691],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Test"
We have it as a switch (see https://github.com/apache/incubator-mxnet/blob/master/Makefile#L255). You should have USE_BLAS=open to use openblas,IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-515161904,zachgk,2019-07-25 18:30:11,15652,[15691],Build bug,0,We have it as a switch (see [url]#L255). You should have USE_BLAS=open to use openblas
"so can we ignore openblas  on 
https://github.com/apache/incubator-mxnet/blob/master/Makefile#L217 ?

Thanks,
Sean.",IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-515163120,gogazago,2019-07-25 18:33:26,15652,[15691],Build bug,0,"so can we ignore openblas on [url]#L217 ? Thanks, Sean."
I might be wrong. @larroy Can you take a look?,IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-515177387,zachgk,2019-07-25 19:14:11,15652,[15691],Build bug,0,I might be wrong. @larroy Can you take a look?
What's the question exactly?  The description on the ticket is not clear.,IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-516179377,larroy,2019-07-29 22:01:26,15652,[15691],Build bug,0,What's the question exactly? The description on the ticket is not clear.
"When you build with make and want to set it to openblas, do you use `USE_BLAS=open` or `USE_BLAS=openblas`?  https://github.com/apache/incubator-mxnet/blob/master/Makefile#L255 seems to indicate it should be `open` while https://github.com/apache/incubator-mxnet/blob/master/Makefile#L217 seems to indicate `openblas`",IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-516182037,zachgk,2019-07-29 22:11:43,15652,[15691],Build bug,0,"When you build with make and want to set it to openblas, do you use [code] or [code]? [url]#L255 seems to indicate it should be [code] while [url]#L217 seems to indicate [code]"
"Yes, this seems to be wrong.",IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-516213621,larroy,2019-07-30 00:39:06,15652,[15691],Build bug,0,"Yes, this seems to be wrong."
@mxnet-label-bot add [Bug],IssueComment,https://github.com/apache/mxnet/issues/15652#issuecomment-516228866,larroy,2019-07-30 02:01:54,15652,[15691],Build bug,0,@mxnet-label-bot add [Bug]
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Build",IssueComment,https://github.com/apache/mxnet/issues/15659#issuecomment-515159883,mxnet-label-bot,2019-07-25 18:24:19,15659,[15697],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Build"
@wuxun-zhang please take a look for this issue.,IssueComment,https://github.com/apache/mxnet/issues/15659#issuecomment-515297883,pengzhao-intel,2019-07-26 03:47:05,15659,[15697],Build bug,0,@wuxun-zhang please take a look for this issue.
"Some background: MKL-DNN itself doesn't support dropout primitive. When MXNet is built with MKL-DNN, it will use functions from VSL to implement dropout. It's kind of legacy code from MKL2017 integration. That's why it cannot be disabled by `MXNET_MKLDNN_ENABLED=0`.",IssueComment,https://github.com/apache/mxnet/issues/15659#issuecomment-515307535,TaoLv,2019-07-26 04:49:24,15659,[15697],Build bug,0,"Some background: MKL-DNN itself doesn't support dropout primitive. When MXNet is built with MKL-DNN, it will use functions from VSL to implement dropout. It's kind of legacy code from MKL2017 integration. That's why it cannot be disabled by [code]."
OK. I'll look into this issue.,IssueComment,https://github.com/apache/mxnet/issues/15659#issuecomment-515312297,wuxun-zhang,2019-07-26 05:16:37,15659,[15697],Build bug,0,OK. I'll look into this issue.
"@matteosal Thanks for reporting this issue. Seems this is not a computational error. I have looked into the `dropout` code, and found that the output `mask` is forced to be calculated and stored with `int*` type instead of original `DType*` or `float*` type. So, when you tried to access the value of `mask`, the wrong answer `1.e-45` will be printed.  Refer to [forward pass](https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/dropout-inl.h#L131) and [backward pass](https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/dropout-inl.h#L152). 
You can build from source by commenting out the `USE_MKL` option in [Makefile](https://github.com/apache/incubator-mxnet/blob/master/Makefile#L150) to work around it temporarily. We'll submit a PR for this after completing the verification locally. ",IssueComment,https://github.com/apache/mxnet/issues/15659#issuecomment-515689407,wuxun-zhang,2019-07-27 14:45:08,15659,[15697],Build bug,0,"@matteosal Thanks for reporting this issue. Seems this is not a computational error. I have looked into the [code] code, and found that the output [code] is forced to be calculated and stored with [code] type instead of original [code] or [code] type. So, when you tried to access the value of [code], the wrong answer [code] will be printed. Refer to [forward pass]([url]#L131) and [backward pass]([url]#L152). You can build from source by commenting out the [code] option in [Makefile]([url]#L150) to work around it temporarily. We'll submit a PR for this after completing the verification locally."
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 ",IssueComment,https://github.com/apache/mxnet/issues/15705#issuecomment-516480641,mxnet-label-bot,2019-07-30 15:53:34,15705,[15707],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it."
"@eric-haibin-lin pointed out https://github.com/apache/incubator-mxnet/pull/14053 is likely at fault
CC: @szha ",IssueComment,https://github.com/apache/mxnet/issues/15705#issuecomment-516485009,leezu,2019-07-30 16:04:04,15705,[15707],Data bug,0,@eric-haibin-lin pointed out [url] is likely at fault CC: @szha
"Yes, it was the cause as in symbol the default value is missing for the axis.",IssueComment,https://github.com/apache/mxnet/issues/15705#issuecomment-516533965,szha,2019-07-30 18:18:05,15705,[15707],Data bug,0,"Yes, it was the cause as in symbol the default value is missing for the axis."
https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/symbol/symbol.py#L2542,IssueComment,https://github.com/apache/mxnet/issues/15705#issuecomment-516533989,szha,2019-07-30 18:18:08,15705,[15707],Data bug,0,[url]#L2542
@mxnet-label-bot add [Bug],IssueComment,https://github.com/apache/mxnet/issues/15705#issuecomment-516637602,vrakesh,2019-07-30 23:46:03,15705,[15707],Data bug,0,@mxnet-label-bot add [Bug]
"Hi @fierceX, do you have any small example that shows this problem? I will look into it.",IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517441738,ptrendx,2019-08-01 20:17:26,15716,[15829],Algorithm design bug,1,"Hi @fierceX, do you have any small example that shows this problem? I will look into it."
"There seem to be 2 problems here. On 1 hand, the ConcatType function seems to be too strict in what it thinks it needs to be correct (and so that error should not be there in the first place as type could be inferred during later stage of InferType pass) - I will make a PR fixing that tomorrow. On the other hand, I don't quite see how you could end up with this situation by just adding AMP so again, a small example would be really nice.",IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517492809,ptrendx,2019-08-01 23:27:31,15716,[15829],Algorithm design bug,1,"There seem to be 2 problems here. On 1 hand, the ConcatType function seems to be too strict in what it thinks it needs to be correct (and so that error should not be there in the first place as type could be inferred during later stage of InferType pass) - I will make a PR fixing that tomorrow. On the other hand, I don't quite see how you could end up with this situation by just adding AMP so again, a small example would be really nice."
@mxnet-label-bot add [Pending Requester Info],IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517493415,vrakesh,2019-08-01 23:30:34,15716,[15829],Algorithm design bug,1,@mxnet-label-bot add [Pending Requester Info]
"Hi @ptrendx ,The following code should be able to reproduce this error.
```python
import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn,rnn
from mxnet.contrib import amp

model = nn.HybridSequential()
model.add(rnn.LSTM(hidden_size=10,num_layers=2,bidirectional=True))
model.add(nn.Dense(2))

model.initialize()
model.hybridize()
model(nd.ones((2,3,4)))

new_model = amp.convert_hybrid_block(model)
```",IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517521798,fierceX,2019-08-02 02:17:44,15716,[15829],Algorithm design bug,1,"Hi @ptrendx ,The following code should be able to reproduce this error. ``[code]``"
Thanks! I will look into this.,IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517749905,ptrendx,2019-08-02 15:45:24,15716,[15829],Algorithm design bug,1,Thanks! I will look into this.
thanks @ptrendx for looking at this. let me know if I can help here.,IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-517857654,anirudh2290,2019-08-02 22:06:27,15716,[15829],Algorithm design bug,1,thanks @ptrendx for looking at this. let me know if I can help here.
"Ok, so after applying PR #15740 I can successfully run the example when using `amp.init`:
```
import mxnet as mx
from mxnet import nd
from mxnet.gluon import nn,rnn
from mxnet.contrib import amp

amp.init()

model = nn.HybridSequential()
model.add(rnn.LSTM(hidden_size=10,num_layers=2,bidirectional=True))
model.add(nn.Dense(2))

model.initialize(ctx=mx.gpu(0))
model.hybridize()
model(nd.ones((2,3,4), ctx=mx.gpu(0)))

# new_model = amp.convert_hybrid_block(model)
```
while the `amp.convert_hybrid_block` still fails with the same error in concat - @anirudh2290, could you take a look at this?",IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-518327880,ptrendx,2019-08-05 17:35:56,15716,[15829],Algorithm design bug,1,"Ok, so after applying PR #15740 I can successfully run the example when using [code]: ``[code]`[code]amp.convert_hybrid_block` still fails with the same error in concat - @anirudh2290, could you take a look at this?"
@ptrendx will take a look.,IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-518347956,anirudh2290,2019-08-05 18:32:21,15716,[15829],Algorithm design bug,1,@ptrendx will take a look.
@ptrendx @fierceX I have added a fix in #15829. Please help review.,IssueComment,https://github.com/apache/mxnet/issues/15716#issuecomment-520038684,anirudh2290,2019-08-09 19:35:36,15716,[15829],Algorithm design bug,1,@ptrendx @fierceX I have added a fix in #15829. Please help review.
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-518755049,mxnet-label-bot,2019-08-06 16:56:31,15767,[15853],Algorithm design bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
@mxnet-label-bot add [Bug],IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-518880434,vdantu,2019-08-06 23:28:37,15767,[15853],Algorithm design bug,1,@mxnet-label-bot add [Bug]
@wuxun-zhang please take a look for this bug.,IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-518945561,pengzhao-intel,2019-08-07 05:27:00,15767,[15853],Algorithm design bug,1,@wuxun-zhang please take a look for this bug.
"@matteosal Thanks for reporting this issue. I can reproduce this issue locally. Firstly, `float64` is not supported yet for current MKL-DNN implementation, so actually mkl-dnn pass should not be executed in this example and there should be missing or imcomplete datatype check somewhere. Additionally, `grad_req` is dependent on `args_grad`, so `grad_req` is always `kNullOp` when `args_grad=None` (see [#L167](https://github.com/apache/incubator-mxnet/blob/master/src/c_api/c_api_executor.cc#L167)). ",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519002670,wuxun-zhang,2019-08-07 08:42:52,15767,[15853],Algorithm design bug,1,"@matteosal Thanks for reporting this issue. I can reproduce this issue locally. Firstly, [code] is not supported yet for current MKL-DNN implementation, so actually mkl-dnn pass should not be executed in this example and there should be missing or imcomplete datatype check somewhere. Additionally, [code] is dependent on [code], so [code] is always [code] when [code] (see [#L167]([url]#L167))."
"I also get the same problem with `RNN`, but setting explicit gradients doesn't help in this case. It seems completely broken on float64:
```
import mxnet as mx

sym = mx.sym.RNN(
	mx.sym.Variable('in'), 
	mx.sym.Variable('par'), 
	mx.sym.Variable('s'), 
	state_size = (2),
	num_layers = 1,
	mode = 'rnn_tanh'
)

dtype = 'float64'
explicit_grad = {
	'in': mx.nd.ones([2, 1, 2], dtype=dtype),
	'par': mx.nd.ones([12], dtype=dtype),
	's': mx.nd.ones([1, 1, 2], dtype=dtype)
}

args_grad = explicit_grad
grad_req = 'write'

ex = sym.bind(mx.cpu(), 
	{
		'in': mx.nd.ones([2, 1, 2], dtype=dtype),
		'par': mx.nd.ones([12], dtype=dtype),
		's': mx.nd.ones([1, 1, 2], dtype=dtype)
	},
	args_grad = args_grad,
	grad_req = grad_req
)
ex.forward()
print(ex.outputs[0])
```

Other RNN modes besides 'rnn_tanh' are also affected.",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519040543,matteosal,2019-08-07 10:33:26,15767,[15853],Algorithm design bug,1,"I also get the same problem with [code], but setting explicit gradients doesn't help in this case. It seems completely broken on float64: ``[code]`` Other RNN modes besides 'rnn_tanh' are also affected."
@wuxun-zhang let's double-check all data type in MKLDNN backend. Maybe fix should be in 1.5.1. @TaoLv ,IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519088145,pengzhao-intel,2019-08-07 13:07:05,15767,[15853],Algorithm design bug,1,@wuxun-zhang let's double-check all data type in MKLDNN backend. Maybe fix should be in 1.5.1. @TaoLv
"Seems that there are no data type check for MKL-DNN stateful RNN implementation (see https://github.com/apache/incubator-mxnet/blob/master/src/operator/rnn.cc#L226). So, when input data is `float64`, mkldnn rnn pass still be executed and then `unknown mkldnn type` error will be got.",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519106862,wuxun-zhang,2019-08-07 13:51:30,15767,[15853],Algorithm design bug,1,"Seems that there are no data type check for MKL-DNN stateful RNN implementation (see [url]#L226). So, when input data is [code], mkldnn rnn pass still be executed and then [code] error will be got."
"> Seems that there are no data type check for MKL-DNN stateful RNN implementation (see https://github.com/apache/incubator-mxnet/blob/master/src/operator/rnn.cc#L226). So, when input data is `float64`, mkldnn rnn pass still be executed and then `unknown mkldnn type` error will be got.

The execution trace of RNN is maked out as below.

https://github.com/apache/incubator-mxnet/blob/71861238743fcd8177afe52d1562d9078ac547de/src/operator/rnn.cc#L254

https://github.com/apache/incubator-mxnet/blob/71861238743fcd8177afe52d1562d9078ac547de/src/operator/nn/mkldnn/mkldnn_base-inl.h#L206-L220",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519126191,xziya,2019-08-07 14:37:34,15767,[15853],Algorithm design bug,1,"> Seems that there are no data type check for MKL-DNN stateful RNN implementation (see [url]#L226). So, when input data is [code], mkldnn rnn pass still be executed and then [code] error will be got. The execution trace of RNN is maked out as below. [url]#L254 [url]#L206-L220"
"It's not all about float64, but about `MKLDNN` subgraph backend. The problem is, recently we enabled MKLDNN subgraph backend by default on master, and this will break the fallback mechanism when handing float64. So for nightly build from master, please use `export MXNET_SUBGRAPH_BACKEND=NONE` to work around shortly, for MXNet v1.5.0, please `unset MXNET_SUBGRAPH_BACKEND`.",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519317089,ZhennanQin,2019-08-08 00:32:57,15767,[15853],Algorithm design bug,1,"It's not all about float64, but about [code] subgraph backend. The problem is, recently we enabled MKLDNN subgraph backend by default on master, and this will break the fallback mechanism when handing float64. So for nightly build from master, please use [code] to work around shortly, for MXNet v1.5.0, please [code]."
@pengzhao-intel @TaoLv v1.5.0 doesn't have this issue. So don't need to fix in v1.5.1.,IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519322278,ZhennanQin,2019-08-08 01:02:40,15767,[15853],Algorithm design bug,1,@pengzhao-intel @TaoLv v1.5.0 doesn't have this issue. So don't need to fix in v1.5.1.
@ZhennanQin Can we add data type check here [#L1663](https://github.com/apache/incubator-mxnet/blob/7186123874/src/executor/graph_executor.cc#L1663) to disable subgraph when input data type is not supported by MKL-DNN?,IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519323609,wuxun-zhang,2019-08-08 01:10:49,15767,[15853],Algorithm design bug,1,@ZhennanQin Can we add data type check here [#L1663]([url]#L1663) to disable subgraph when input data type is not supported by MKL-DNN?
"> @pengzhao-intel @TaoLv v1.5.0 doesn't have this issue. So don't need to fix in v1.5.1.

It's nice and we can try to resolve in 1.6.",IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-519325620,pengzhao-intel,2019-08-08 01:21:27,15767,[15853],Algorithm design bug,1,> @pengzhao-intel @TaoLv v1.5.0 doesn't have this issue. So don't need to fix in v1.5.1. It's nice and we can try to resolve in 1.6.
@matteosal sorry for the delay. The PR is blocked by 3rd party package but it is resolved and will be merged soon.,IssueComment,https://github.com/apache/mxnet/issues/15767#issuecomment-528323795,pengzhao-intel,2019-09-05 11:40:18,15767,[15853],Algorithm design bug,1,@matteosal sorry for the delay. The PR is blocked by 3rd party package but it is resolved and will be merged soon.
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug, Feature",IssueComment,https://github.com/apache/mxnet/issues/15709#issuecomment-516711721,mxnet-label-bot,2019-07-31 06:13:08,15709,[15865],Data bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug, Feature"
@mxnet-label-bot add [Feature request],IssueComment,https://github.com/apache/mxnet/issues/15709#issuecomment-517028008,vrakesh,2019-07-31 21:27:41,15709,[15865],Data bug,1,@mxnet-label-bot add [Feature request]
"Hi,

I would like to work on this. However I'll need some pointer as to how to go about it or even a reference of other operator which supports this should do.",IssueComment,https://github.com/apache/mxnet/issues/15709#issuecomment-520457487,kshitij12345,2019-08-12 14:50:32,15709,[15865],Data bug,1,"Hi, I would like to work on this. However I'll need some pointer as to how to go about it or even a reference of other operator which supports this should do."
"Hi @kshitij12345, thank you for looking into this. Your contribution will be highly appreciated!

Actually this is not an in-place issue and the `transpose` operator in MXNet is not designed to support in-place.

From the error log and the check at https://github.com/apache/incubator-mxnet/blob/master/src/operator/tensor/matrix_op-inl.h#L334, the `req[0]` here is `kNullOp` which means the operator should do nothing before return.

So the logic should look like:
```cpp
if (req[0] == kNullOp) {
  return;
} else {
  CHECK_EQ(req[0], kWriteTo) << ""Transpose does not support kWriteInplace or kAddTo"";
  // ....
}
```",IssueComment,https://github.com/apache/mxnet/issues/15709#issuecomment-520477884,TaoLv,2019-08-12 15:39:20,15709,[15865],Data bug,1,"Hi @kshitij12345, thank you for looking into this. Your contribution will be highly appreciated! Actually this is not an in-place issue and the [code] operator in MXNet is not designed to support in-place. From the error log and the check at [url]#L334, the [code] here is [code] which means the operator should do nothing before return. So the logic should look like: ``[code]``"
"@TaoLv Oh, I see. Thank You for the nice explanation. Will send a PR soon. ",IssueComment,https://github.com/apache/mxnet/issues/15709#issuecomment-520496730,kshitij12345,2019-08-12 16:27:47,15709,[15865],Data bug,1,"@TaoLv Oh, I see. Thank You for the nice explanation. Will send a PR soon."
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15916#issuecomment-521758279,mxnet-label-bot,2019-08-15 19:03:12,15916,[15917],Deployment bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15543#issuecomment-511584112,mxnet-label-bot,2019-07-15 21:52:01,15543,[16053],Algorithm design bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"@zhanghang1989  Thanks for reporting this

@mxnet-label-bot add [Optimizer, Bug] ",IssueComment,https://github.com/apache/mxnet/issues/15543#issuecomment-511590954,vrakesh,2019-07-15 22:17:37,15543,[16053],Algorithm design bug,1,"@zhanghang1989 Thanks for reporting this @mxnet-label-bot add [Optimizer, Bug]"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511169503,mxnet-label-bot,2019-07-14 03:13:04,15535,[16166],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"I was running your code, and it looks like `last_batch_handle` has nothing to do with the error. You'll get the same error whenever your `batch size > 2 * len(data)`. 
```python
data = mx.nd.arange(5)

dtIter = mx.io.NDArrayIter(data, batch_size=11) # everything is fine for batch_size upto 10
for i in dtIter:
    print (i.data)
```
prints:
`IndexError: Slicing stop 6 exceeds limit of 5`",IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511202059,braindotai,2019-07-14 13:17:12,15535,[16166],Data bug,0,"I was running your code, and it looks like [code] has nothing to do with the error. You'll get the same error whenever your [code]. ``[code]`[code]IndexError: Slicing stop 6 exceeds limit of 5`"
"> I was running your code, and it looks like `last_batch_handle` has nothing to do with the error. You'll get the same error whenever your `batch size > 2 * len(data)`.
> 
> ```python
> data = mx.nd.arange(5)
> 
> dtIter = mx.io.NDArrayIter(data, batch_size=11) # everything is fine for batch_size upto 10
> for i in dtIter:
>     print (i.data)
> ```
> 
> prints:
> `IndexError: Slicing stop 6 exceeds limit of 5`

`last_batch_handle='pad'` does matter because the default value for `last_batch_handle` is `'pad'` and all other options (`roll_over` and `discard`) does not result in exception.",IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511213685,turtleizzy,2019-07-14 15:48:31,15535,[16166],Data bug,0,"> I was running your code, and it looks like [code] has nothing to do with the error. You'll get the same error whenever your [code]. > > ``[code]`[code]IndexError: Slicing stop 6 exceeds limit of 5[code]last_batch_handle='pad'[code]last_batch_handle[code]'pad'[code]roll_over[code]discard`) does not result in exception."
"Oh man!! Sorry, I forgot to see what's the default.",IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511215625,braindotai,2019-07-14 16:14:55,15535,[16166],Data bug,0,"Oh man!! Sorry, I forgot to see what's the default."
PS: workaround is to pad manually.,IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511359114,turtleizzy,2019-07-15 11:04:44,15535,[16166],Data bug,0,PS: workaround is to pad manually.
"@mxnet-label-bot add [python, bug]",IssueComment,https://github.com/apache/mxnet/issues/15535#issuecomment-511475696,frankfliu,2019-07-15 16:35:29,15535,[16166],Data bug,0,"@mxnet-label-bot add [python, bug]"
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 ",IssueComment,https://github.com/apache/mxnet/issues/16338#issuecomment-536755898,mxnet-label-bot,2019-09-30 21:14:25,16338,[16397],Data bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it."
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended label(s): Bug",IssueComment,https://github.com/apache/mxnet/issues/16399#issuecomment-539718387,mxnet-label-bot,2019-10-08 21:48:10,16399,[16432],Processor bug,1,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended label(s): Bug"
@szha,IssueComment,https://github.com/apache/mxnet/issues/16399#issuecomment-540848391,leezu,2019-10-11 00:13:21,16399,[16432],Processor bug,1,@szha
"@mxnet-label-bot , add [Bug]",IssueComment,https://github.com/apache/mxnet/issues/16399#issuecomment-541855703,ddavydenko,2019-10-14 18:52:58,16399,[16432],Processor bug,1,"@mxnet-label-bot , add [Bug]"
"There is a simple fix at https://github.com/apache/incubator-mxnet/pull/16432
Just requires review",IssueComment,https://github.com/apache/mxnet/issues/16399#issuecomment-541975033,leezu,2019-10-14 23:39:50,16399,[16432],Processor bug,1,There is a simple fix at [url] Just requires review
"Hey @danielgoncalvesti Thanks for posting this issue
But we need more code for reproducing this issue.
For starters - Which commit+device+language?",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-546542934,ChaiBapchya,2019-10-25 23:25:06,16629,[16690],Build bug,0,Hey @danielgoncalvesti Thanks for posting this issue But we need more code for reproducing this issue. For starters - Which commit+device+language?
@mxnet-label-bot  add [Pending Requester Info],IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-547086483,ChaiBapchya,2019-10-28 18:34:01,16629,[16690],Build bug,0,@mxnet-label-bot add [Pending Requester Info]
"I'm using the latest commit from the master branch. 
My configuration is: 
-MacOS Catalina 10.15
-Docker 2.1.0.4 (stable)
-Python 3.7.4

I short, I enter the docker folder, and then run this command:
`./tool.sh build python cpu`

The installation purpose of this tool on docker is just to create my own binary database. So I have a collection of pictures in a folder and want to generate a binary file like 'database.bin'.",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-547220707,danielgoncalvesti,2019-10-29 01:54:09,16629,[16690],Build bug,0,"I'm using the latest commit from the master branch. My configuration is: -MacOS Catalina 10.15 -Docker 2.1.0.4 (stable) -Python 3.7.4 I short, I enter the docker folder, and then run this command: [code] The installation purpose of this tool on docker is just to create my own binary database. So I have a collection of pictures in a folder and want to generate a binary file like 'database.bin'."
"My config
MacOS High Sierra 10.13.6
Docker 2.1.0.4
Python 3.7.5

`./tool.sh build python cpu`
Was able to reproduce this error. Working on fixing it",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548555705,ChaiBapchya,2019-10-31 20:29:47,16629,[16690],Build bug,0,My config MacOS High Sierra 10.13.6 Docker 2.1.0.4 Python 3.7.5 [code] Was able to reproduce this error. Working on fixing it
@mxnet-label-bot remove [Pending Requester Info],IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548555818,ChaiBapchya,2019-10-31 20:30:03,16629,[16690],Build bug,0,@mxnet-label-bot remove [Pending Requester Info]
"It's not just for Mac (as i tried it for unix too) and error was reproduced.
@mxnet-label-bot add [MKLDNN, Build, Docker]",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548578442,ChaiBapchya,2019-10-31 21:34:55,16629,[16690],Build bug,0,"It's not just for Mac (as i tried it for unix too) and error was reproduced. @mxnet-label-bot add [MKLDNN, Build, Docker]"
"For reference, error
```
[100%] Built target mkldnn
make[2]: Leaving directory `/mxnet/3rdparty/mkldnn/build'
Install the project...
-- Install configuration: ""Release""
-- Installing: /mxnet/3rdparty/mkldnn/build/install/share/doc/mkldnn/LICENSE
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so.1.0
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so.1
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_config.h
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_version.h
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn.h
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_debug.h
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_types.h
-- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn.hpp
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-config.cmake
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-config-version.cmake
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-targets.cmake
-- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-targets-release.cmake
make[1]: Leaving directory `/mxnet/3rdparty/mkldnn/build'
mkdir -p /mxnet/lib
if [ -f ""/mxnet/3rdparty/mkldnn/build/install/lib64/libmkldnn.so.1"" ]; then \
                cp /mxnet/3rdparty/mkldnn/build/install/lib64/libmkldnn* /mxnet/lib; \
                cp /mxnet/3rdparty/mkldnn/build/install/lib64/libmkldnn* /mxnet/3rdparty/mkldnn/build/install/lib/; \
        else \
                cp /mxnet/3rdparty/mkldnn/build/install/lib/libmkldnn* /mxnet/lib; \
        fi
cp: cannot stat '/mxnet/3rdparty/mkldnn/build/install/lib/libmkldnn*': No such file or directory
make: *** [/mxnet/3rdparty/mkldnn/build/install/lib/libmkldnn.so.1] Error 1
The command '/bin/sh -c git clone --recursive https://github.com/dmlc/mxnet && cd mxnet &&     make -j$(nproc) &&     rm -r build' returned a non-zero code: 2
```",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548578824,ChaiBapchya,2019-10-31 21:36:05,16629,[16690],Build bug,0,"For reference, error ``[code]/mxnet/3rdparty/mkldnn/build' Install the project... -- Install configuration: ""Release"" -- Installing: /mxnet/3rdparty/mkldnn/build/install/share/doc/mkldnn/LICENSE -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so.1.0 -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so.1 -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/libmkldnn.so -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_config.h -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_version.h -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn.h -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_debug.h -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn_types.h -- Installing: /mxnet/3rdparty/mkldnn/build/install/include/mkldnn.hpp -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-config.cmake -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-config-version.cmake -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-targets.cmake -- Installing: /mxnet/3rdparty/mkldnn/build/install/lib/x86_64-linux-gnu/cmake/mkldnn/mkldnn-targets-release.cmake make[1]: Leaving directory [code]``"
@xinyu-intel could you help take a look for the MKLDNN library link?,IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548594778,pengzhao-intel,2019-10-31 22:30:40,16629,[16690],Build bug,0,@xinyu-intel could you help take a look for the MKLDNN library link?
"Just to clarify, the issue is not caused by the MKL-DNN 1.0 upgrading. It was reported against the master branch before #16555 was merged. It relates to the cmake installation behavior on different OS distributions. Anyway we will fix it on the latest master branch.",IssueComment,https://github.com/apache/mxnet/issues/16629#issuecomment-548639583,TaoLv,2019-11-01 02:17:03,16629,[16690],Build bug,0,"Just to clarify, the issue is not caused by the MKL-DNN 1.0 upgrading. It was reported against the master branch before #16555 was merged. It relates to the cmake installation behavior on different OS distributions. Anyway we will fix it on the latest master branch."
"Thanks to reporting the issue ~~~
@ZhennanQin  will look into the issue. 
",IssueComment,https://github.com/apache/mxnet/issues/16687#issuecomment-548663177,pengzhao-intel,2019-11-01 04:34:31,16687,[16697],Performance bug,0,Thanks to reporting the issue ~~~ @ZhennanQin will look into the issue.
@ptrendx ,IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-550579206,leezu,2019-11-07 01:34:09,16747,[16781],Data bug,0,@ptrendx
"I suggest turn the fused_op off by default in the 1.6.0 release and announce it as experimental feature, or revert the PR. @szha @eric-haibin-lin @junrushao1994 @DickJC123 @wkcn @reminisce @haojin2  @TaoLv @marcoabreu  What do you think?",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-550583532,sxjscience,2019-11-07 01:53:14,16747,[16781],Data bug,0,"I suggest turn the fused_op off by default in the 1.6.0 release and announce it as experimental feature, or revert the PR. @szha @eric-haibin-lin @junrushao1994 @DickJC123 @wkcn @reminisce @haojin2 @TaoLv @marcoabreu What do you think?"
@zhreshold ,IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-550584121,sxjscience,2019-11-07 01:56:01,16747,[16781],Data bug,0,@zhreshold
"I agree to turn the fused_op off by default until fused_op is stable.
The reason is that users couldn't use the 1.6.0 release if it is not compatible with their code.",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-550971205,wkcn,2019-11-07 08:13:52,16747,[16781],Data bug,0,I agree to turn the fused_op off by default until fused_op is stable. The reason is that users couldn't use the 1.6.0 release if it is not compatible with their code.
+1,IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-550978412,TaoLv,2019-11-07 08:36:48,16747,[16781],Data bug,0,+1
"Isn't right now the period of finding those integration bugs and fixing them for 1.6 release? I will definitely look into this issue and fix it, not sure why you propose to turn the feature off by default?",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-551232209,ptrendx,2019-11-07 19:39:30,16747,[16781],Data bug,0,"Isn't right now the period of finding those integration bugs and fixing them for 1.6 release? I will definitely look into this issue and fix it, not sure why you propose to turn the feature off by default?"
@ptrendx I think we are already in a code-freeze status and the simplest fix is to turn it off by default. We could easily turn it on in 1.6.1 once we have confirmed that it has no impact in all the training scripts (there are plenty of them) and some may take time to run.,IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-551235898,sxjscience,2019-11-07 19:49:39,16747,[16781],Data bug,0,@ptrendx I think we are already in a code-freeze status and the simplest fix is to turn it off by default. We could easily turn it on in 1.6.1 once we have confirmed that it has no impact in all the training scripts (there are plenty of them) and some may take time to run.
"Ok, I sent a clarification email to dev@ as you are not actually the first person to reach out to me with this misunderstanding of code freeze. Code freeze is a period where bugs are found and fixed in order to polish the release and provide the best experience for the end users.

I treat the bugs about fusion with highest priority and will do my best to fix them. If I fail to address all issues before the time to make RC, then I agree it should be turned off by default and marked experimental.",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-551284480,ptrendx,2019-11-07 21:57:16,16747,[16781],Data bug,0,"Ok, I sent a clarification email to dev@ as you are not actually the first person to reach out to me with this misunderstanding of code freeze. Code freeze is a period where bugs are found and fixed in order to polish the release and provide the best experience for the end users. I treat the bugs about fusion with highest priority and will do my best to fix them. If I fail to address all issues before the time to make RC, then I agree it should be turned off by default and marked experimental."
"I agree with @ptrendx, we should try to fix the bugs and ship the features if time allows.",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-551312428,leezu,2019-11-07 23:23:54,16747,[16781],Data bug,0,"I agree with @ptrendx, we should try to fix the bugs and ship the features if time allows."
"I received the clarification email about the meaning of code freeze and I agree with @ptrendx that we should try to fix it these days and consider to turn it off by default if we fail to do so. BTW, what's the expected date for 1.6 RC?",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-551316009,sxjscience,2019-11-07 23:37:39,16747,[16781],Data bug,0,"I received the clarification email about the meaning of code freeze and I agree with @ptrendx that we should try to fix it these days and consider to turn it off by default if we fail to do so. BTW, what's the expected date for 1.6 RC?"
"I created a PR with a fix. @leezu, could you validate it?",IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-552684533,ptrendx,2019-11-12 00:54:49,16747,[16781],Data bug,0,"I created a PR with a fix. @leezu, could you validate it?"
@ptrendx thanks for the fix. Just confirmed it works.,IssueComment,https://github.com/apache/mxnet/issues/16747#issuecomment-553216409,leezu,2019-11-13 03:01:07,16747,[16781],Data bug,0,@ptrendx thanks for the fix. Just confirmed it works.
"I see, this is a newly added type. We will fix this.",IssueComment,https://github.com/apache/mxnet/issues/16723#issuecomment-549639921,ptrendx,2019-11-05 02:48:38,16723,[16796],Data bug,1,"I see, this is a newly added type. We will fix this."
@PatricZhao ,IssueComment,https://github.com/apache/mxnet/issues/16833#issuecomment-554556279,reminisce,2019-11-15 22:36:52,16833,[16837],Data bug,0,@PatricZhao
"@stu1130 @reminisce Thanks for reporting this issue. I can reproduce this from my side. I just figured out the root cause, when input's ndim is 0, it should not execute MKL-DNN pass. Anyway, I will submit a PR to fix this issue soon. Sorry for any inconvenience. @pengzhao-intel 

Also tested below input, and no error.
```
>>> a = np.array([1])
>>> np.expand_dims(a, 0)
mkldnn_verbose,info,Intel MKL-DNN v1.0.4 (commit a0a87d662edeef38d01db4ac5dd25f59a1f0881f)
mkldnn_verbose,info,Detected ISA is Intel AVX-512 with Intel DL Boost
mkldnn_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:a:f0 dst_f32::blocked:a:f0,num:1,1,11.0342
array([[1.]])
>>>
```",IssueComment,https://github.com/apache/mxnet/issues/16833#issuecomment-554600286,wuxun-zhang,2019-11-16 03:38:33,16833,[16837],Data bug,0,"@stu1130 @reminisce Thanks for reporting this issue. I can reproduce this from my side. I just figured out the root cause, when input's ndim is 0, it should not execute MKL-DNN pass. Anyway, I will submit a PR to fix this issue soon. Sorry for any inconvenience. @pengzhao-intel Also tested below input, and no error. ``[code]``"
@stu1130 PR#16837 is filed. Please help check if it fix this issue. Thanks.,IssueComment,https://github.com/apache/mxnet/issues/16833#issuecomment-554602407,wuxun-zhang,2019-11-16 04:13:06,16833,[16837],Data bug,0,@stu1130 PR#16837 is filed. Please help check if it fix this issue. Thanks.
`ExpandDimEx` needs to skip zero-size cases. I will send in a PR for the fix.,IssueComment,https://github.com/apache/mxnet/issues/16850#issuecomment-555375375,reminisce,2019-11-19 07:41:21,16850,[16856],Data bug,1,[code] needs to skip zero-size cases. I will send in a PR for the fix.
MKL-DNN path has been fixed by https://github.com/apache/incubator-mxnet/pull/16837.,IssueComment,https://github.com/apache/mxnet/issues/16850#issuecomment-555378036,TaoLv,2019-11-19 07:50:28,16850,[16856],Data bug,1,MKL-DNN path has been fixed by [url]
"@TaoLv #16837 only deals with zero-dim case, while here it's zero-size which is different.",IssueComment,https://github.com/apache/mxnet/issues/16850#issuecomment-555379679,reminisce,2019-11-19 07:56:13,16850,[16856],Data bug,1,"@TaoLv #16837 only deals with zero-dim case, while here it's zero-size which is different."
"@reminisce Yes, we need also check if any dim is zero.",IssueComment,https://github.com/apache/mxnet/issues/16850#issuecomment-555381262,wuxun-zhang,2019-11-19 08:01:25,16850,[16856],Data bug,1,"@reminisce Yes, we need also check if any dim is zero."
Thank you for clarifying @reminisce . Let me know if anything I can help.,IssueComment,https://github.com/apache/mxnet/issues/16850#issuecomment-555382126,TaoLv,2019-11-19 08:04:21,16850,[16856],Data bug,1,Thank you for clarifying @reminisce . Let me know if anything I can help.
"This should be a release blocker.

cc @ptrendx @reminisce ",IssueComment,https://github.com/apache/mxnet/issues/16647#issuecomment-546744371,szha,2019-10-27 23:05:10,16647,[16895],Version compatibility bug,1,This should be a release blocker. cc @ptrendx @reminisce
@lanking520 assign @szha ,IssueComment,https://github.com/apache/mxnet/issues/16647#issuecomment-555179394,samskalicky,2019-11-18 19:47:51,16647,[16895],Version compatibility bug,1,@lanking520 assign @szha
`NDArray._basic_indexing_contiguous_flat_begin_end` has a bug. I will send in a fix.,IssueComment,https://github.com/apache/mxnet/issues/16887#issuecomment-557442363,reminisce,2019-11-22 08:36:17,16887,[16902],Data bug,1,[code] has a bug. I will send in a fix.
"Another case fails after I changed https://github.com/apache/incubator-mxnet/blob/4da14a22385622c35e9a5c9c3e8a17c07f718cad/python/mxnet/ndarray/ndarray.py#L902. Will need more time to debug this problem.

```python
import mxnet as mx
from mxnet import gluon
mx.npx.set_np()
a = mx.np.random.normal(0, 1, (5, 5, 5, 5))
b = mx.np.random.normal(0, 1, (5, 5, 5, 5))
index1 = (slice(4, 1, -1), slice(2, 1, -1), 3, 0)
index2 = (slice(2, 3, 1), slice(0, 1, 1))

class Foo(gluon.HybridBlock):
    def hybrid_forward(self, F, x, y):
        return (x[()][index1] + y[()][index1])[index2]

net = Foo()
with mx.autograd.record():
    out = net(a, b)
```",IssueComment,https://github.com/apache/mxnet/issues/16887#issuecomment-557701936,sxjscience,2019-11-22 21:31:02,16887,[16902],Data bug,1,Another case fails after I changed [url]#L902. Will need more time to debug this problem. ``[code]``
"I'd suggest tweaking this description slightly.  Instead of ""test_multi_worker_forked_data_loader fails with DEBUG="" I'd suggest ""Failed OpenMP assertion when loading MXNet compiled with DEBUG=1"".

It's happening for me (and Pedro) in non-dataloading scenarios.  Seems to happen as soon as the thread pool is created.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-411031326,KellenSunderland,2018-08-07 11:57:57,10856,[17012],Build bug,0,"I'd suggest tweaking this description slightly. Instead of ""test_multi_worker_forked_data_loader fails with DEBUG="" I'd suggest ""Failed OpenMP assertion when loading MXNet compiled with DEBUG=1"". It's happening for me (and Pedro) in non-dataloading scenarios. Seems to happen as soon as the thread pool is created."
I don't see a root cause listed here.,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-415094876,cjolivier01,2018-08-22 16:30:22,10856,[17012],Build bug,0,I don't see a root cause listed here.
"So, why is __kmp_team_pool == __null assertion hitting?  Intel OpenMP is compatible with process forking, so this is probably a symptom of something else wrong leading up to this.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-415095349,cjolivier01,2018-08-22 16:31:51,10856,[17012],Build bug,0,"So, why is __kmp_team_pool == __null assertion hitting? Intel OpenMP is compatible with process forking, so this is probably a symptom of something else wrong leading up to this."
"This is really problematic, is there a suggestion for anything that we can do to fix this?",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-436361689,larroy,2018-11-06 18:40:06,10856,[17012],Build bug,0,"This is really problematic, is there a suggestion for anything that we can do to fix this?"
"Stuck for 46h running unit tests with the following error message:

```
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6479).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
Assertion failure at kmp_runtime.cpp(6479): __kmp_thread_pool == __null.
```

",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-453742444,larroy,2019-01-12 12:08:18,10856,[17012],Build bug,0,Stuck for 46h running unit tests with the following error message: ``[code]``
"I spent some time looking at this, I can say I understand what's the problem, this happens when the omp engine is initialized after destructors are called on thread creation. Usually involves the pthread_at_fork fiasco in initialize.cc 

Iinitalization is triggered from __nptl_deallocate_tsd (); https://code.woboq.org/userspace/glibc/nptl/pthread_create.c.html#497 

__kmp_team_pool is a volatile which gets changed continously across threads for reuse of the last kmp team.

This happens if omp is initialized twice for some reason, so __kmp_team_pool is not NULL because there's a thread running omp functions, kmp_team_pool can be non-NULL",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-455632496,larroy,2019-01-18 17:53:22,10856,[17012],Build bug,0,"I spent some time looking at this, I can say I understand what's the problem, this happens when the omp engine is initialized after destructors are called on thread creation. Usually involves the pthread_at_fork fiasco in initialize.cc Iinitalization is triggered from __nptl_deallocate_tsd (); [url]#497 __kmp_team_pool is a volatile which gets changed continously across threads for reuse of the last kmp team. This happens if omp is initialized twice for some reason, so __kmp_team_pool is not NULL because there's a thread running omp functions, kmp_team_pool can be non-NULL"
"What's worse is that when executing this test, you can easily create a segmentation fault, by hitting ^C so we are UB land:

```
python tests/python/unittest/test_gluon_data.py
```


```
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6479).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
Assertion failure at kmp_runtime.cpp(6479): __kmp_thread_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6479).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
Assertion failure at kmp_runtime.cpp(6479): __kmp_thread_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6479).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
^C[INFO] Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1709269671 to reproduce.

----------------------------------------------------------------------
Ran 3 tests in 12.296s

OK

Segmentation fault: 11

Stack trace returned 10 entries:
[bt] (0) /home/piotr/devel/mxnet/mxnet/python/mxnet/../../build/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x54) [0x7fef667f38e5]
[bt] (1) /home/piotr/devel/mxnet/mxnet/python/mxnet/../../build/libmxnet.so(+0x4ba60be) [0x7fef69ba40be]
[bt] (2) /lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7fefbeb324b0]
[bt] (3) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0xc7c0d) [0x7fefb8857c0d]
[bt] (4) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0xc7d02) [0x7fefb8857d02]
[bt] (5) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0x6ebca) [0x7fefb87febca]
[bt] (6) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0x56e9d) [0x7fefb87e6e9d]
[bt] (7) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0x5727c) [0x7fefb87e727c]
[bt] (8) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0x576b4) [0x7fefb87e76b4]
[bt] (9) /home/piotr/devel/mxnet/mxnet/build/3rdparty/openmp/runtime/src/libomp.so(+0x56d4f) [0x7fefb87e6d4f]
```",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-455633028,larroy,2019-01-18 17:55:13,10856,[17012],Build bug,0,"What's worse is that when executing this test, you can easily create a segmentation fault, by hitting ^C so we are UB land: ``[code]`[code]`[code]``"
"This is not related to Data-loading, but is an issue with OpenMP. Removing the data-loading label. 

@mxnet-label-bot Update [Backend, Bug]",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-501424790,piyushghai,2019-06-12 19:35:24,10856,[17012],Build bug,0,"This is not related to Data-loading, but is an issue with OpenMP. Removing the data-loading label. @mxnet-label-bot Update [Backend, Bug]"
"Getting crashes in master when using debug version with OpenMP and MKL for development:

```
Assertion failure at kmp_runtime.cpp(6481): __kmp_team_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6481).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
Assertion failure at kmp_runtime.cpp(6481): __kmp_team_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6481).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.
w odict_values([Parameter sequential0_dense0_weight (shape=(3, 0), dtype=float32), Parameter sequential0_dense0_bias (shape=(3,), dtype=float32)])
OMP: Error #15: Initializing libiomp5.so, but found libomp.so already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
fish: “./fc_mx.py” terminated by signal SIGABRT (Abort)
```",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505129021,larroy,2019-06-24 18:37:24,10856,[17012],Build bug,0,Getting crashes in master when using debug version with OpenMP and MKL for development: ``[code]``
"```
(py3_venv) pllarroy@elite:134: ~/d/GradOptMXNet [autograd_fixes]> cat fc_mx.py
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import mxnet as mx
import mxnet.autograd as ag
from mxnet import nd
from mxnet import gluon
import sys


def main():
    x = nd.array([[0.2131, 0.5449, 0.9910],
        [0.1600, 0.1665, 0.1387],
        [0.6242, 0.0409, 0.0663],
        [0.6590, 0.9822, 0.3108],
        [0.8566, 0.3848, 0.8385]])
    x.attach_grad()
    net = gluon.nn.Sequential()
    with net.name_scope():
        net.add(gluon.nn.Dense(3))
        #net.initialize()
    net.initialize(mx.init.Xavier(magnitude=2.24))
    w = net.collect_params().values()
    print(f""w {w}"")
    with ag.record():
        y = net.forward(x)
        x_grad = ag.grad(y, x, create_graph=True, retain_graph=True)[0]
    print(x_grad)
    return 1

if __name__ == '__main__':
    sys.exit(main())
```

Build options, defaults as in https://github.com/apache/incubator-mxnet/blob/master/cmake/cmake_options.yml

The crash happened as I was running the script above.


",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505130046,larroy,2019-06-24 18:40:19,10856,[17012],Build bug,0,"``[code]`` Build options, defaults as in [url] The crash happened as I was running the script above."
"I tried with the PR from Anton, and I also get a crash, in that version we are only linking with MKL's openmp:


```
(py3_venv) pllarroy@elite:1: ~/d/GradOptMXNet [autograd_fixes]> ./fc_mx.py
w odict_values([Parameter sequential0_dense0_weight (shape=(3, 0), dtype=float32), Parameter sequential0_dense0_bias (shape=(3,), dtype=float32)])
OMP: Error #15: Initializing libiomp5.so, but found libiomp5.so already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
fish: “./fc_mx.py” terminated by signal SIGABRT (Abort)
(py3_venv) pllarroy@elite:134: ~/d/GradOptMXNet [autograd_fixes]> ldd /home/ANT.AMAZON.COM/pllarroy/devel/mxnet/python/mxnet/../../build/libmxnet.so | grep -i openmp
(py3_venv) pllarroy@elite:1: ~/d/GradOptMXNet [autograd_fixes]> ldd /home/ANT.AMAZON.COM/pllarroy/devel/mxnet/python/mxnet/../../build/libmxnet.so | grep -i omp
	libiomp5.so => /home/ANT.AMAZON.COM/pllarroy/devel/mxnet/build/mklml/mklml_lnx_2019.0.5.20190502/lib/libiomp5.so (0x00007f60c8446000)
(py3_venv) pllarroy@elite:0: ~/d/GradOptMXNet [autograd_fixes]> ipython
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import mxnet as mx                                                                                                                                                                         
mx.li
In [2]: mx.libinfo.find_lib_path()                                                                                                                                                                 
Out[2]: ['/home/ANT.AMAZON.COM/pllarroy/devel/mxnet/python/mxnet/../../build/libmxnet.so']

In [3]:                                                                                                                                                                                            
Do you really want to exit ([y]/n)? y
```

https://github.com/apache/incubator-mxnet/pull/12160

When compiling without OpenMP the example above doesn't crash.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505134548,larroy,2019-06-24 18:52:35,10856,[17012],Build bug,0,"I tried with the PR from Anton, and I also get a crash, in that version we are only linking with MKL's openmp: ``[code]`` [url] When compiling without OpenMP the example above doesn't crash."
"@mxnet-label-bot add [MKL]
",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505134595,larroy,2019-06-24 18:52:42,10856,[17012],Build bug,0,@mxnet-label-bot add [MKL]
"Adding stack trace. Tried to reproduce in another machine and the crash didn't happen, but get assertion now and then.

[bt.txt](https://github.com/apache/incubator-mxnet/files/3322437/bt.txt)

```
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
Core was generated by `py3_venv/bin/python fc.py'.
Program terminated with signal SIGABRT, Aborted.
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
[Current thread is 1 (Thread 0x7fbd94581700 (LWP 17687))]
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007fbda3583801 in __GI_abort () at abort.c:79
#2  0x00007fbd3dcda8b3 in __kmp_abort_process () at ../../src/kmp_runtime.cpp:492
#3  0x00007fbd3dcc6277 in __kmp_fatal (message=...) at ../../src/kmp_i18n.cpp:894
#4  0x00007fbd3dcd86d5 in __kmp_register_library_startup () at ../../src/kmp_runtime.cpp:6702
#5  0x00007fbd3dcd99d4 in _INTERNAL_25_______src_kmp_runtime_cpp_6dd760e2::__kmp_do_serial_initialize () at ../../src/kmp_runtime.cpp:6821
#6  _INTERNAL_25_______src_kmp_runtime_cpp_6dd760e2::__kmp_do_middle_initialize () at ../../src/kmp_runtime.cpp:7137
#7  __kmp_middle_initialize () at ../../src/kmp_runtime.cpp:7246
#8  0x00007fbd3dcbb44e in __kmp_api_omp_get_num_procs () at ../../src/kmp_ftn_entry.h:615
#9  0x00007fbd35d2b9ce in mkl_serv_get_num_stripes () from /opt/intel/mkl/lib/intel64/libmkl_intel_thread.so
#10 0x00007fbd35e9bdd4 in mkl_blas_sgemm () from /opt/intel/mkl/lib/intel64/libmkl_intel_thread.so
#11 0x00007fbd3d245719 in sgemm_ () from /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so
#12 0x00007fbd3d2a5510 in cblas_sgemm () from /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so
#13 0x00007fbd9e4654f9 in mkldnn::impl::cpu::extended_sgemm (transa=0x7fbd9e93183f ""T"", transb=0x7fbd9e931841 ""N"", M=0x7fbd9457f744, N=0x7fbd9457f740, K=0x7fbd9457f748, 
```",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505162890,larroy,2019-06-24 20:16:39,10856,[17012],Build bug,0,"Adding stack trace. Tried to reproduce in another machine and the crash didn't happen, but get assertion now and then. [bt.txt]([url] ``[code]py3_venv/bin/python fc.py'. Program terminated with signal SIGABRT, Aborted. #0 __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51 [Current thread is 1 (Thread 0x7fbd94581700 (LWP 17687))] #0 __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51 #1 0x00007fbda3583801 in __GI_abort () at abort.c:79 #2 0x00007fbd3dcda8b3 in __kmp_abort_process () at ../../src/kmp_runtime.cpp:492 #3 0x00007fbd3dcc6277 in __kmp_fatal (message=...) at ../../src/kmp_i18n.cpp:894 #4 0x00007fbd3dcd86d5 in __kmp_register_library_startup () at ../../src/kmp_runtime.cpp:6702 #5 0x00007fbd3dcd99d4 in _INTERNAL_25_______src_kmp_runtime_cpp_6dd760e2::__kmp_do_serial_initialize () at ../../src/kmp_runtime.cpp:6821 #6 _INTERNAL_25_______src_kmp_runtime_cpp_6dd760e2::__kmp_do_middle_initialize () at ../../src/kmp_runtime.cpp:7137 #7 __kmp_middle_initialize () at ../../src/kmp_runtime.cpp:7246 #8 0x00007fbd3dcbb44e in __kmp_api_omp_get_num_procs () at ../../src/kmp_ftn_entry.h:615 #9 0x00007fbd35d2b9ce in mkl_serv_get_num_stripes () from /opt/intel/mkl/lib/intel64/libmkl_intel_thread.so #10 0x00007fbd35e9bdd4 in mkl_blas_sgemm () from /opt/intel/mkl/lib/intel64/libmkl_intel_thread.so #11 0x00007fbd3d245719 in sgemm_ () from /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so #12 0x00007fbd3d2a5510 in cblas_sgemm () from /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so #13 0x00007fbd9e4654f9 in mkldnn::impl::cpu::extended_sgemm (transa=0x7fbd9e93183f ""T"", transb=0x7fbd9e931841 ""N"", M=0x7fbd9457f744, N=0x7fbd9457f740, K=0x7fbd9457f748, ```"
"Added a dockerfile for reproduction:

https://github.com/larroy/mxnet_omp",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-505163621,larroy,2019-06-24 20:18:34,10856,[17012],Build bug,0,Added a dockerfile for reproduction: [url]
"This breaks ipython, can't execute external commands anymore:
```

In [2]: !git show
commit 9d7fc7cbee09de2694022995d0601cb4316e4988 (HEAD -> master, upstream/master)
Author: Cody Allen <ceedubs@gmail.com>
Date:   Wed Jul 31 16:09:23 2019 -0700

    Fix Scala Symbolic API some/Some typo (#15687)

diff --git a/docs/api/scala/symbol.md b/docs/api/scala/symbol.md
index aaddc2a8a..f92548e48 100644
--- a/docs/api/scala/symbol.md
+++ b/docs/api/scala/symbol.md
@@ -41,7 +41,7 @@ The following example configures a two-layer neural network.
     val data = Symbol.Variable(""data"")
     val fc1 = Symbol.api.FullyConnected(Some(data), num_hidden = 128, name = ""fc1"")
     val act1 = Symbol.api.Activation(Some(fc1), ""relu"", ""relu1"")
-    val fc2 = Symbol.api.FullyConnected(some(act1), num_hidden = 64, name = ""fc2"")
+    val fc2 = Symbol.api.FullyConnected(Some(act1), num_hidden = 64, name = ""fc2"")
     val net = Symbol.api.SoftmaxOutput(Some(fc2), name = ""out"")
     :type net
     // org.apache.mxnet.Symbol

In [3]: import mxnet as mx
Assertion failure at kmp_runtime.cpp(6481): __kmp_team_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6481).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.

In [4]: !ls
Assertion failure at kmp_runtime.cpp(6481): __kmp_team_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6481).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.

In [5]: !git show
Assertion failure at kmp_runtime.cpp(6481): __kmp_team_pool == __null.
OMP: Error #13: Assertion failure at kmp_runtime.cpp(6481).
OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see https://bugs.llvm.org/.

In [6]: 

```
",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517076662,larroy,2019-08-01 00:59:47,10856,[17012],Build bug,0,"This breaks ipython, can't execute external commands anymore: ``[code]``"
"Using the code provided in this example, the code in the train process is never executed:


https://github.com/apache/incubator-mxnet/issues/14979

I think we can confirm that this a big issue and is producing incorrect results.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517407376,larroy,2019-08-01 18:35:07,10856,[17012],Build bug,0,"Using the code provided in this example, the code in the train process is never executed: [url] I think we can confirm that this a big issue and is producing incorrect results."
"I have a private branch which fixes this issue, but need to verify side effects further.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517417936,larroy,2019-08-01 19:06:13,10856,[17012],Build bug,0,"I have a private branch which fixes this issue, but need to verify side effects further."
https://github.com/larroy/mxnet/tree/openmp_improv,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517418096,larroy,2019-08-01 19:06:38,10856,[17012],Build bug,0,[url]
I revisited this yesterday when I was working on one issue. I see that this assertion hits only when DNDEBUG and -g flags are passed when building 3rdparty/openmp. I tweaked my CMakeLists to remove those DNDEBUG and -g flags to 3rdparty/openmp and those assertions go away. I don't think I will ever get to debugging openmp code so I am good. ,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517424340,anirudh2290,2019-08-01 19:25:45,10856,[17012],Build bug,0,I revisited this yesterday when I was working on one issue. I see that this assertion hits only when DNDEBUG and -g flags are passed when building 3rdparty/openmp. I tweaked my CMakeLists to remove those DNDEBUG and -g flags to 3rdparty/openmp and those assertions go away. I don't think I will ever get to debugging openmp code so I am good.
"well, it's not executing code that should be executed, if you see the linked issue.  Plus producing a crash...  so it does have customer impact...",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517474312,larroy,2019-08-01 22:02:53,10856,[17012],Build bug,0,"well, it's not executing code that should be executed, if you see the linked issue. Plus producing a crash... so it does have customer impact..."
related: https://github.com/apache/incubator-mxnet/issues/14979,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517474497,larroy,2019-08-01 22:03:40,10856,[17012],Build bug,0,related: [url]
you can fix the customer impact by making the workaround i suggested in CMakeLists.txt. Removing pthread_at_fork will cause customer impact for multiprocessing users.,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517475293,anirudh2290,2019-08-01 22:06:33,10856,[17012],Build bug,0,you can fix the customer impact by making the workaround i suggested in CMakeLists.txt. Removing pthread_at_fork will cause customer impact for multiprocessing users.
"I disagree with your statement. Yes, the customer impact is that if fixes several problems. Please specify what problems do you think it introduces to remove pthread_at_fork. Also I would request some benefit of the doubt because I have dive deep into this. Thanks.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517481273,larroy,2019-08-01 22:31:29,10856,[17012],Build bug,0,"I disagree with your statement. Yes, the customer impact is that if fixes several problems. Please specify what problems do you think it introduces to remove pthread_at_fork. Also I would request some benefit of the doubt because I have dive deep into this. Thanks."
"it is my understanding that this is caused by the static init which calls an OMP API at fork time, possibly from the kernel tuning stuff. if so, just make it not re-run the kernel tuning stuff at fork time. 

i have seen no compelling evidence that the atfork call in that linked issue is related to this failure.  in fact, i stated this before in the email discussion and no challenge to that was given . honestly, this is why i don’t bother arguing about this issue anymore. it’s like talking into a black hole...",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517482774,cjolivier01,2019-08-01 22:38:22,10856,[17012],Build bug,0,"it is my understanding that this is caused by the static init which calls an OMP API at fork time, possibly from the kernel tuning stuff. if so, just make it not re-run the kernel tuning stuff at fork time. i have seen no compelling evidence that the atfork call in that linked issue is related to this failure. in fact, i stated this before in the email discussion and no challenge to that was given . honestly, this is why i don’t bother arguing about this issue anymore. it’s like talking into a black hole..."
"@cjolivier01 I think you haven't carefully taken all the data provided into consideration, including my latest responses in the @dev mailing list regarding this issue, which are unanswered. Also I have linked the issue above which shows customer code not being executed. I will send a PR with a fix, feel free to comment on that, but please refrain from making uninformed comments without making any attempt to run code, use a debugger or perform properly documented experiments. I have posted in the thread of this issue my experiments, which show the relation with this:

https://github.com/apache/incubator-mxnet/issues/14979

Note that I don't claim that using llvm's openmp is the root cause of the problem but rather re-entrancy coming from pthread_at_fork handlers. My suggestion is to remove those.

Also I don't understand what you mean by ""just make it not re-run the kernel tuning stuff at fork time."". Please be specific with your suggestions to avoid wasting time in miscommunication.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517483598,larroy,2019-08-01 22:42:11,10856,[17012],Build bug,0,"@cjolivier01 I think you haven't carefully taken all the data provided into consideration, including my latest responses in the @dev mailing list regarding this issue, which are unanswered. Also I have linked the issue above which shows customer code not being executed. I will send a PR with a fix, feel free to comment on that, but please refrain from making uninformed comments without making any attempt to run code, use a debugger or perform properly documented experiments. I have posted in the thread of this issue my experiments, which show the relation with this: [url] Note that I don't claim that using llvm's openmp is the root cause of the problem but rather re-entrancy coming from pthread_at_fork handlers. My suggestion is to remove those. Also I don't understand what you mean by ""just make it not re-run the kernel tuning stuff at fork time."". Please be specific with your suggestions to avoid wasting time in miscommunication."
"> Please specify what problems do you think it introduces to remove pthread_at_fork.

Multiprocessing will stop working after you remove pthread_at_fork at handler. Many of mxnet users like @YutingZhang depend on that. MXNet Model Server uses it @vdantu .

I think you should either do the workaround, or if you want to really debug openmp code, explore the option that @cjolivier01 suggests.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517493013,anirudh2290,2019-08-01 23:28:25,10856,[17012],Build bug,0,"> Please specify what problems do you think it introduces to remove pthread_at_fork. Multiprocessing will stop working after you remove pthread_at_fork at handler. Many of mxnet users like @YutingZhang depend on that. MXNet Model Server uses it @vdantu . I think you should either do the workaround, or if you want to really debug openmp code, explore the option that @cjolivier01 suggests."
Thanks. Do you know why it stops working? Could you elaborate?,IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517493544,larroy,2019-08-01 23:31:11,10856,[17012],Build bug,0,Thanks. Do you know why it stops working? Could you elaborate?
"To rephrase your question , you are asking why we need pthread_atfork ? 

This is to suspend all the active threads prior to forking. Otherwise we can run into issues as documented here: http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_atfork.html",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517495361,anirudh2290,2019-08-01 23:41:24,10856,[17012],Build bug,0,"To rephrase your question , you are asking why we need pthread_atfork ? This is to suspend all the active threads prior to forking. Otherwise we can run into issues as documented here: [url]"
"Ok I got what you meant now. I don't think the kernel tunning is a problem there. I see that there's problems with multiprocessing when removing pthread_at_fork. Please suggest another approach for the issue linked above. Why is kernel tuning related to the crash linked above?  Yes the warning comes from kernel tuning, but the crash??",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517498203,larroy,2019-08-01 23:58:20,10856,[17012],Build bug,0,"Ok I got what you meant now. I don't think the kernel tunning is a problem there. I see that there's problems with multiprocessing when removing pthread_at_fork. Please suggest another approach for the issue linked above. Why is kernel tuning related to the crash linked above? Yes the warning comes from kernel tuning, but the crash??"
"Thanks @anirudh2290 for the record, I think you are right about problems with Multiprocessing that will be caused by removing completely the handlers. I think we need to find a way to avoid the other problematic side-effects that are causing problems and crashes. Suggestions welcome.",IssueComment,https://github.com/apache/mxnet/issues/10856#issuecomment-517499070,larroy,2019-08-02 00:02:53,10856,[17012],Build bug,0,"Thanks @anirudh2290 for the record, I think you are right about problems with Multiprocessing that will be caused by removing completely the handlers. I think we need to find a way to avoid the other problematic side-effects that are causing problems and crashes. Suggestions welcome."
@DickJC123 I've created a proper issue so we can track this problem - thanks for looking into it!,IssueComment,https://github.com/apache/mxnet/issues/17020#issuecomment-563106604,perdasilva,2019-12-09 07:42:55,17020,[17028],Version compatibility bug,1,@DickJC123 I've created a proper issue so we can track this problem - thanks for looking into it!
"As  #15167 is part of 1.6 release, is this a release blocker?",IssueComment,https://github.com/apache/mxnet/issues/17020#issuecomment-563113073,leezu,2019-12-09 08:03:10,17020,[17028],Version compatibility bug,1,"As #15167 is part of 1.6 release, is this a release blocker?"
@apeforest assign @ptrendx ,IssueComment,https://github.com/apache/mxnet/issues/17020#issuecomment-563400541,samskalicky,2019-12-09 19:39:39,17020,[17028],Version compatibility bug,1,@apeforest assign @ptrendx
I was looking into it and it seems like a bug in nvrtc in cuda 9 toolkit. I believe the workaround should be easy - I will have the PR ready today.,IssueComment,https://github.com/apache/mxnet/issues/17020#issuecomment-563449044,ptrendx,2019-12-09 21:31:42,17020,[17028],Version compatibility bug,1,I was looking into it and it seems like a bug in nvrtc in cuda 9 toolkit. I believe the workaround should be easy - I will have the PR ready today.
Would you like to open a PR to update the doc (to recommend version 4)?,IssueComment,https://github.com/apache/mxnet/issues/17061#issuecomment-565883333,leezu,2019-12-16 02:46:51,17061,[17090],Version compatibility bug,0,Would you like to open a PR to update the doc (to recommend version 4)?
"Would you mind taking a look at https://github.com/apache/incubator-mxnet/pull/17228 ? It should fix your issue, but I'm not an R expert.",IssueComment,https://github.com/apache/mxnet/issues/17103#issuecomment-571275838,leezu,2020-01-06 19:24:02,17103,[17228],Documentation bug,0,"Would you mind taking a look at [url] ? It should fix your issue, but I'm not an R expert."
"We may refactor https://github.com/apache/incubator-mxnet/blob/master/cmake/Modules/FindNCCL.cmake to improve autodetection. In the meantime see the variables used for searching. If you set one of them to your nccl base directory, it should find nccl successfully? ",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-571732861,leezu,2020-01-07 19:23:49,17239,[17297],Build bug,0,"We may refactor [url] to improve autodetection. In the meantime see the variables used for searching. If you set one of them to your nccl base directory, it should find nccl successfully?"
I experienced this too ... try using -DUSE_NCCL=1 -DUSE_NCCL_PATH=/usr/local/cuda/include (or as @leezu your NCCL path),IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-572036823,mjsML,2020-01-08 12:59:39,17239,[17297],Build bug,0,I experienced this too ... try using -DUSE_NCCL=1 -DUSE_NCCL_PATH=/usr/local/cuda/include (or as @leezu your NCCL path)
"@mjsML Thanks, using that flag worked for me. @guanxinq or @ChaiBapchya interested in fixing FindNCCL.cmake as suggested? :)",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573390637,apeforest,2020-01-12 07:40:11,17239,[17297],Build bug,0,"@mjsML Thanks, using that flag worked for me. @guanxinq or @ChaiBapchya interested in fixing FindNCCL.cmake as suggested? :)"
"I took a look at this auto-detection issue.

To solve this particular case, I added a check for symlink (if UNIX) - https://github.com/ChaiBapchya/incubator-mxnet/blob/nccl_autodetect/cmake/Modules/FindNCCL.cmake

If this is enough, I can submit a PR.

However, I'm not sure if it is complete. Coz I took a look at https://github.com/apache/incubator-mxnet/blob/master/cmake/Modules/FindCUDAToolkit.cmake
It has a fairly long drawn way of finding the Cuda Toolkit
1. Language / user provided path
2. If cuda_root cmake/env not specified, check
- check symlink 
- check platform default

Is this what's needed? @leezu @apeforest 
In that case it makes sense to ""factor"" out this check as it will be used at 2 places (findCudatoolkit and findNCCL)",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573497187,ChaiBapchya,2020-01-13 03:38:25,17239,[17297],Build bug,0,"I took a look at this auto-detection issue. To solve this particular case, I added a check for symlink (if UNIX) - [url] If this is enough, I can submit a PR. However, I'm not sure if it is complete. Coz I took a look at [url] It has a fairly long drawn way of finding the Cuda Toolkit 1. Language / user provided path 2. If cuda_root cmake/env not specified, check - check symlink - check platform default Is this what's needed? @leezu @apeforest In that case it makes sense to ""factor"" out this check as it will be used at 2 places (findCudatoolkit and findNCCL)"
"@apeforest could you provide some background if NCCL is installed at `/usr/local/cuda/include` by default?

@ChaiBapchya your change seems to rely on `CUDA_TOOLKIT_ROOT_DIR`, but this variable is not among the variables exported by `FindCUDAToolkit`. In fact, you can see it's explicitly unset:

https://github.com/apache/incubator-mxnet/blob/master/cmake/Modules/FindCUDAToolkit.cmake#L708

Instead, let's use the result variables

https://github.com/apache/incubator-mxnet/blob/28e053edb4f2079743458bf087557bcac7e58c62/cmake/Modules/FindCUDAToolkit.cmake#L427-L464

Specifically `CUDAToolkit_INCLUDE_DIRS` and `CUDAToolkit_LIBRARY_DIR`? Or would the nccl library not be at the `CUDAToolkit_LIBRARY_DIR`?

Besides using the `CUDAToolkit` variables as additional defaults to find nccl, the `NCCL_ROOT` variable needs to be examined as per https://cmake.org/cmake/help/latest/policy/CMP0074.html
(which is done correctly currently I think)",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573622552,leezu,2020-01-13 11:37:27,17239,[17297],Build bug,0,"@apeforest could you provide some background if NCCL is installed at [code] by default? @ChaiBapchya your change seems to rely on [code], but this variable is not among the variables exported by [code]. In fact, you can see it's explicitly unset: [url]#L708 Instead, let's use the result variables [url]#L427-L464 Specifically [code] and [code]? Or would the nccl library not be at the [code]? Besides using the [code] variables as additional defaults to find nccl, the [code] variable needs to be examined as per [url] (which is done correctly currently I think)"
"In DLAMI, nccl is installed by default in the cuda directory: /usr/local/cuda/include/nccl.h

However, if user installed nccl manually by themselves, sudo apt install libnccl2 libnccl-dev, you may use the `sudo dpkg-query -L libnccl-dev` to find where it is.
https://askubuntu.com/questions/1134732/where-is-nccl-h

I would suggest @ChaiBapchya to first search `/usr/local/cuda/include/`. If not found, try `sudo dpkg-query -L libnccl-dev` instead. Would that work?",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573836423,apeforest,2020-01-13 19:43:44,17239,[17297],Build bug,0,"In DLAMI, nccl is installed by default in the cuda directory: /usr/local/cuda/include/nccl.h However, if user installed nccl manually by themselves, sudo apt install libnccl2 libnccl-dev, you may use the [code] to find where it is. [url] I would suggest @ChaiBapchya to first search [code]. If not found, try [code] instead. Would that work?"
Thanks @ChaiBapchya for volunteering to work on this!,IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573836947,apeforest,2020-01-13 19:44:59,17239,[17297],Build bug,0,Thanks @ChaiBapchya for volunteering to work on this!
"> If not found, try `sudo dpkg-query -L libnccl-dev` instead.

That's would only work on Debian based platforms and only for one particular way of installing nccl on these systems. I think it's safe to require users to set `NCCL_ROOT` if they manually installed nccl to a different path.

To improve the user experience, we may fall-back to building nccl ourselves if nccl is required and not found. Pytorch does that for example.",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573861547,leezu,2020-01-13 20:44:01,17239,[17297],Build bug,0,"> If not found, try [code] instead. That's would only work on Debian based platforms and only for one particular way of installing nccl on these systems. I think it's safe to require users to set [code] if they manually installed nccl to a different path. To improve the user experience, we may fall-back to building nccl ourselves if nccl is required and not found. Pytorch does that for example."
"Ya. Even when I looked at different autodetection files for cmake used in various other open-source frameworks 
1. Xgboost - https://github.com/dmlc/xgboost/blob/master/cmake/modules/FindNccl.cmake
2. Flashlight - https://github.com/facebookresearch/flashlight/blob/master/cmake/FindNCCL.cmake
3. Pytorch - https://github.com/pytorch/pytorch/blob/master/cmake/Modules/FindNCCL.cmake
4. Caffe - https://github.com/BVLC/caffe/blob/master/cmake/Modules/FindNCCL.cmake
5. Thunder - https://github.com/thuem/THUNDER/blob/master/cmake/FindNCCL.cmake

They have similar approach. Either look for default path, env var (NCCL_ROOT) or /usr/local/cuda

Agree with @leezu I haven't seen ""dpkg-query"" or equivalent ""find"" commands used in cmake. They are more of command line searches. In cmake, there's find_path, find_library which does similar job.

Thanks @apeforest @leezu for chiming in!",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-573875270,ChaiBapchya,2020-01-13 21:18:08,17239,[17297],Build bug,0,"Ya. Even when I looked at different autodetection files for cmake used in various other open-source frameworks 1. Xgboost - [url] 2. Flashlight - [url] 3. Pytorch - [url] 4. Caffe - [url] 5. Thunder - [url] They have similar approach. Either look for default path, env var (NCCL_ROOT) or /usr/local/cuda Agree with @leezu I haven't seen ""dpkg-query"" or equivalent ""find"" commands used in cmake. They are more of command line searches. In cmake, there's find_path, find_library which does similar job. Thanks @apeforest @leezu for chiming in!"
"@ChaiBapchya BTW, unfortunately a lot of CMake usage out in the wild does not meet the modern CMake bar but is leftover from the early days of CMake. While not covering all use-cases of MXNet, sometimes we can refer to https://cliutils.gitlab.io/modern-cmake/ for best practices",IssueComment,https://github.com/apache/mxnet/issues/17239#issuecomment-574211833,leezu,2020-01-14 14:55:12,17239,[17297],Build bug,0,"@ChaiBapchya BTW, unfortunately a lot of CMake usage out in the wild does not meet the modern CMake bar but is leftover from the early days of CMake. While not covering all use-cases of MXNet, sometimes we can refer to [url] for best practices"
"Would it be required to use `broadcast_div` instead of `/` in the last line below:

https://github.com/apache/incubator-mxnet/blob/985a4ca62766ddca22b995a663d258003602f6af/python/mxnet/gluon/loss.py#L923-L932",IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-573624439,leezu,2020-01-13 11:43:08,17275,[17308],Algorithm design bug,0,Would it be required to use [code] instead of [code] in the last line below: [url]#L923-L932
"Hello, thanks for having a look.

Actually, I just realized that shape issue comes from the fact that, in symbolic api: 
`.reshape(-1, 1)` is not the same as `.reshape((-1, 1))`. 

Obviously, the simplest fix for `CosineEmbeddingLoss` would be to call explicitly `F.reshape` or to call `.reshape((-1, 1))`. 

Though, souldn't `ndarray` and `symbolic` api have the same behaviour regarding `.reshape` ?
Does this should be fixed ? Is it a desired behaviour ?

Regards",IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-573903431,dithyrambe,2020-01-13 22:29:35,17275,[17308],Algorithm design bug,0,"Hello, thanks for having a look. Actually, I just realized that shape issue comes from the fact that, in symbolic api: [code] is not the same as [code]. Obviously, the simplest fix for [code] would be to call explicitly [code] or to call [code]. Though, souldn't [code] and [code] api have the same behaviour regarding [code] ? Does this should be fixed ? Is it a desired behaviour ? Regards"
There are many subtle inconsistencies with symbolic and ndarray. Those will be fixed in MXNet 2 by migrating to a numpy compatible interface.,IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-574226156,leezu,2020-01-14 15:22:36,17275,[17308],Algorithm design bug,0,There are many subtle inconsistencies with symbolic and ndarray. Those will be fixed in MXNet 2 by migrating to a numpy compatible interface.
"Nice,

I guess the issue is solved then.
What about the fix ?

Regards",IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-574267259,dithyrambe,2020-01-14 16:47:22,17275,[17308],Algorithm design bug,0,"Nice, I guess the issue is solved then. What about the fix ? Regards"
"If you think it's required to change line 925, 926, 927 above to use tuple `(-1, 1)` please create a PR. Is that what you mean? ",IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-574306610,leezu,2020-01-14 18:20:43,17275,[17308],Algorithm design bug,0,"If you think it's required to change line 925, 926, 927 above to use tuple [code] please create a PR. Is that what you mean?"
"All right, 

Thanks for your help.",IssueComment,https://github.com/apache/mxnet/issues/17275#issuecomment-574348641,dithyrambe,2020-01-14 20:04:10,17275,[17308],Algorithm design bug,0,"All right, Thanks for your help."
"Saw this on email list and got curious...


Looks like problem is probably this commit:
https://github.com/apache/incubator-mxnet/commit/4ed14e2b749743a014121f57b265675fa7b4c06d#diff-875aa4c013dbd73b044531e439e8afdd

Basically `MXAPIHandleException` used to be defined inline in the header file, so all consumers had to do was 
```
#include <mxnet/c_api_error.h>
```

But now as of 3 days ago it is not an inline function anymore. Meaning that consumers need to make sure to link against c_api_error.o to get the symbol.

I don't know enough about the build system that produces these nightly builds (does it use the CMake one or the Makefile one?) ... but my hunch would be that either c_api_error.o is not getting built into libmxnet.so. Or somehow it is, but the order it is presented to the linker is before MXAPIHandleException is used, so that symbol isn't included in libmxnet.so.",IssueComment,https://github.com/apache/mxnet/issues/17292#issuecomment-573948848,stephenrawls,2020-01-14 01:04:29,17292,[17348],Build bug,0,"Saw this on email list and got curious... Looks like problem is probably this commit: [url]#diff-875aa4c013dbd73b044531e439e8afdd Basically [code] used to be defined inline in the header file, so all consumers had to do was ``[code]`` But now as of 3 days ago it is not an inline function anymore. Meaning that consumers need to make sure to link against c_api_error.o to get the symbol. I don't know enough about the build system that produces these nightly builds (does it use the CMake one or the Makefile one?) ... but my hunch would be that either c_api_error.o is not getting built into libmxnet.so. Or somehow it is, but the order it is presented to the linker is before MXAPIHandleException is used, so that symbol isn't included in libmxnet.so."
@stephenrawls currently a Makefile build is used. You can find it at https://github.com/apache/incubator-mxnet/tree/master/tools/staticbuild We are working on migrating to the cmake build in the future though.,IssueComment,https://github.com/apache/mxnet/issues/17292#issuecomment-574223193,leezu,2020-01-14 15:16:37,17292,[17348],Build bug,0,@stephenrawls currently a Makefile build is used. You can find it at [url] We are working on migrating to the cmake build in the future though.
@szha ,IssueComment,https://github.com/apache/mxnet/issues/17292#issuecomment-574832896,eric-haibin-lin,2020-01-15 20:03:09,17292,[17348],Build bug,0,@szha
"Thanks @stephenrawls for the analysis. 
Here are the causes of the problem:

1) Horovod uses MX_API_BEGIN() and MX_API_END() from mxnet/c_api_error.h to catch and throw errors in horovod APIs: https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.cc#L224
2) MX_API_BEGIN() is a macro that calls MXAPIHandleException https://github.com/apache/incubator-mxnet/blob/master/include/mxnet/c_api_error.h#L36
3) Before #17128, MXAPIHandleException is an inline function. And therefore when #17128 introduced a new function call NormalizeError() inside MXAPIHandleException it broke Horovod integration because the symbol of NormalizeError is not whitelist by MXNet distribution.
4) #17298 removed NormalizeError() from MXAPIHandleException and made it not inline. https://github.com/apache/incubator-mxnet/pull/17208/files#diff-875aa4c013dbd73b044531e439e8afddR67. This time the error becomes undefined symbol of MXAPIHandleException.

So to summarize, the problem is not that Horovod requires `MXAPIHandleException` function to be inline. The rootcause is that MXNet did not export the symbol `*MXAPIHandleException*` in its [whitelist](https://github.com/apache/incubator-mxnet/blob/a296dad87438624bc6388e4659db4cb039a7908a/make/config/libmxnet.sym#L15), but only the symbols that are being used inside `MXAPIHandleException` function. It was okay when the function `MXAPIHandleException` is inline, but became a problem when it's not. A good practice is to whitelist symbol `*MXAPIHandleException*` instead of its internals.

@szha I will create a PR to fix this.",IssueComment,https://github.com/apache/mxnet/issues/17292#issuecomment-574848779,apeforest,2020-01-15 20:46:29,17292,[17348],Build bug,0,"Thanks @stephenrawls for the analysis. Here are the causes of the problem: 1) Horovod uses MX_API_BEGIN() and MX_API_END() from mxnet/c_api_error.h to catch and throw errors in horovod APIs: [url]#L224 2) MX_API_BEGIN() is a macro that calls MXAPIHandleException [url]#L36 3) Before #17128, MXAPIHandleException is an inline function. And therefore when #17128 introduced a new function call NormalizeError() inside MXAPIHandleException it broke Horovod integration because the symbol of NormalizeError is not whitelist by MXNet distribution. 4) #17298 removed NormalizeError() from MXAPIHandleException and made it not inline. [url]#diff-875aa4c013dbd73b044531e439e8afddR67. This time the error becomes undefined symbol of MXAPIHandleException. So to summarize, the problem is not that Horovod requires [code] function to be inline. The rootcause is that MXNet did not export the symbol [code] in its [whitelist]([url]#L15), but only the symbols that are being used inside [code] function. It was okay when the function [code] is inline, but became a problem when it's not. A good practice is to whitelist symbol [code] instead of its internals. @szha I will create a PR to fix this."
"@cyrusbehr Could you please try to apply below change to the CMakeList file?
```
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 2612854..3068925 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -288,6 +288,7 @@ if(USE_MKLDNN)
   set(MKLDNN_ARCH_OPT_FLAGS """" CACHE INTERNAL """" FORCE)
   set(MKLDNN_ENABLE_JIT_PROFILING OFF CACHE INTERNAL """" FORCE)
   set(MKLDNN_LIBRARY_TYPE STATIC CACHE INTERNAL """" FORCE)
+  set(MKLDNN_CPU_RUNTIME SEQ CACHE INTERNAL """" FORCE)

   add_subdirectory(3rdparty/mkldnn)
```

Let me know if it works for you. Thanks.",IssueComment,https://github.com/apache/mxnet/issues/17338#issuecomment-575073921,TaoLv,2020-01-16 09:56:08,17338,[17356],Build bug,0,@cyrusbehr Could you please try to apply below change to the CMakeList file? ``[code]`` Let me know if it works for you. Thanks.
"@TaoLv this fixes the issue for me (though I didn't try linking with MKL). I'll reassign this issue to you, as it seems to be due to MKLDNN integration (ie above missing variable).

Ie with your diff applied, `cmake -DUSE_CPP_PACKAGE=1 -DUSE_CUDA=0 -DUSE_MKLDNN=ON -DUSE_OPENCV=0 -DUSE_LAPACK=0 -DUSE_OPENMP=0 -GNinja ..; ninja` works",IssueComment,https://github.com/apache/mxnet/issues/17338#issuecomment-575145811,leezu,2020-01-16 13:15:41,17338,[17356],Build bug,0,"@TaoLv this fixes the issue for me (though I didn't try linking with MKL). I'll reassign this issue to you, as it seems to be due to MKLDNN integration (ie above missing variable). Ie with your diff applied, [code] works"
@leezu Thanks. We need check USE_OPENMP in cmake as what we did for makefile: https://github.com/apache/incubator-mxnet/blob/master/mkldnn.mk#L35. I will file a PR to fix the problem once the patch is verified by the reporter.,IssueComment,https://github.com/apache/mxnet/issues/17338#issuecomment-575150538,TaoLv,2020-01-16 13:27:28,17338,[17356],Build bug,0,@leezu Thanks. We need check USE_OPENMP in cmake as what we did for makefile: [url]#L35. I will file a PR to fix the problem once the patch is verified by the reporter.
"@leezu @TaoLv 
Adding `set(MKLDNN_CPU_RUNTIME SEQ CACHE INTERNAL """" FORCE)` solved the problem. 
Thanks!",IssueComment,https://github.com/apache/mxnet/issues/17338#issuecomment-575278640,cyrusbehr,2020-01-16 18:16:22,17338,[17356],Build bug,0,@leezu @TaoLv Adding [code] solved the problem. Thanks!
@PatricZhao @TaoLv Did you encounter this before? Thanks,IssueComment,https://github.com/apache/mxnet/issues/17352#issuecomment-575407880,apeforest,2020-01-17 00:17:29,17352,[17386],Test bug,0,@PatricZhao @TaoLv Did you encounter this before? Thanks
"Yes, we're aware of this problem as in our local validation system, python tests are split into single run for each test. We didn't dive into the problem as we thought it's a problem of nosetests or how the nosetests file is implemented. I remember these was an similar issue but cannot find it anymore.",IssueComment,https://github.com/apache/mxnet/issues/17352#issuecomment-575466220,TaoLv,2020-01-17 05:03:42,17352,[17386],Test bug,0,"Yes, we're aware of this problem as in our local validation system, python tests are split into single run for each test. We didn't dive into the problem as we thought it's a problem of nosetests or how the nosetests file is implemented. I remember these was an similar issue but cannot find it anymore."
@juliusshufan ,IssueComment,https://github.com/apache/mxnet/issues/17352#issuecomment-575466455,TaoLv,2020-01-17 05:04:41,17352,[17386],Test bug,0,@juliusshufan
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Bug",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-456456061,mxnet-label-bot,2019-01-22 16:06:37,13956,[17509],Code bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Bug"
"@mxnet-label-bot add [Bug, c api]",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-456512797,frankfliu,2019-01-22 18:37:12,13956,[17509],Code bug,0,"@mxnet-label-bot add [Bug, c api]"
"I can add a second issue if desired, but I'd like to point out that the C++ API has the same problem.

https://github.com/apache/incubator-mxnet/blob/master/cpp-package/include/mxnet-cpp/executor.hpp#L77-L83

I am using the raw C API (i.e. not the C predict API) partly to avoid having abort called on me.",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-456895966,stephenrawls,2019-01-23 17:37:43,13956,[17509],Code bug,0,"I can add a second issue if desired, but I'd like to point out that the C++ API has the same problem. [url]#L77-L83 I am using the raw C API (i.e. not the C predict API) partly to avoid having abort called on me."
"I'll note that even the raw C API (i.e. not the C predict API) has this problem in quite a few places, though they seem to be less easy to trigger.

E.g. https://github.com/apache/incubator-mxnet/blob/0a45e1a222637c7dee29511cbfc43e594571933b/src/c_api/c_api_executor.cc#L358",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-457111257,mika-fischer,2019-01-24 08:35:15,13956,[17509],Code bug,0,"I'll note that even the raw C API (i.e. not the C predict API) has this problem in quite a few places, though they seem to be less easy to trigger. E.g. [url]#L358"
"Good point...lots of places to clean up if we want to remove the possibility that abort() gets called on us.

Turns out it's also in the underlying library code implementing e.g. operators:
https://github.com/apache/incubator-mxnet/blob/0d480fbe1534d1c1228bf2c7470018ae06cbac37/src/operator/tensor/init_op.h#L121-L123

I am in the process of putting MxNet into a production environment, and it would be nice if it didn't have the ability to unexpectedly abort() the process it's running in. I guess this is a bigger task than I initially thought.",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-457293713,stephenrawls,2019-01-24 17:56:05,13956,[17509],Code bug,0,"Good point...lots of places to clean up if we want to remove the possibility that abort() gets called on us. Turns out it's also in the underlying library code implementing e.g. operators: [url]#L121-L123 I am in the process of putting MxNet into a production environment, and it would be nice if it didn't have the ability to unexpectedly abort() the process it's running in. I guess this is a bigger task than I initially thought."
"@mika-fischer @stephenrawls thanks for the suggestions. Since this requires potentially large change, I'd suggest that we get more attention on this by writing to [dev@mxnet.apache.org](https://lists.apache.org/list.html?dev@mxnet.apache.org), if you intend to initiate a large change.",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-457892225,szha,2019-01-27 06:13:29,13956,[17509],Code bug,0,"@mika-fischer @stephenrawls thanks for the suggestions. Since this requires potentially large change, I'd suggest that we get more attention on this by writing to [[email]]([url] if you intend to initiate a large change."
"By default, those checks should be throwing instead of calling `abort()`.
https://github.com/dmlc/dmlc-core/blob/master/include/dmlc/logging.h#L25-L26

It will abort only when `defined(_LIBCPP_SGX_NO_IOSTREAMS)`
https://github.com/dmlc/dmlc-core/blob/master/include/dmlc/logging.h#L351
or when `DMLC_LOG_FATAL_THROW` is explicitly set to 0.
https://github.com/dmlc/dmlc-core/blob/master/include/dmlc/logging.h#L362

Since the build logic of mxnet itself uses the default setting, I'd like to see how and in what scenarios you trigger either the first or the second case, after which we can try to come up with the fix necessary.",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-457893146,szha,2019-01-27 06:37:52,13956,[17509],Code bug,0,"By default, those checks should be throwing instead of calling [code]. [url]#L25-L26 It will abort only when [code] [url]#L351 or when [code] is explicitly set to 0. [url]#L362 Since the build logic of mxnet itself uses the default setting, I'd like to see how and in what scenarios you trigger either the first or the second case, after which we can try to come up with the fix necessary."
"Good catch, sorry for the confusion. Can't speak for @mika-fischer, but as for me, I dug through the various layers until I saw `abort()` in the code and didn't pay close enough attention to the #if/#elif preprocessor declarations.

My other complaint about the c++ api is that it doesn't always check the return status of its C API calls. But that issue is much easier to detect and fix, I will put in a separate Issue / Pull Request about that.  (e.g. see here: https://github.com/apache/incubator-mxnet/blob/master/cpp-package/include/mxnet-cpp/operator.hpp#L137-L139)",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-457984886,stephenrawls,2019-01-28 02:59:35,13956,[17509],Code bug,0,"Good catch, sorry for the confusion. Can't speak for @mika-fischer, but as for me, I dug through the various layers until I saw [code] in the code and didn't pay close enough attention to the #if/#elif preprocessor declarations. My other complaint about the c++ api is that it doesn't always check the return status of its C API calls. But that issue is much easier to detect and fix, I will put in a separate Issue / Pull Request about that. (e.g. see here: [url]#L137-L139)"
"Yes, same here. I think I've seen the pip installed mxnet abort on me before and the stack trace in the error message confused us. But in fact an exception is thrown as you say. So this can be closed. Sorry for the noise.",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-458048418,mika-fischer,2019-01-28 09:09:17,13956,[17509],Code bug,0,"Yes, same here. I think I've seen the pip installed mxnet abort on me before and the stack trace in the error message confused us. But in fact an exception is thrown as you say. So this can be closed. Sorry for the noise."
"> After all replacements due to macro expansion and the defined unary operator have been performed, all remaining identifiers and keywords, except for true and false, are replaced with the pp-number 0

https://stackoverflow.com/a/5085425/2560672

Thus it is not true that `DMLC_LOG_FATAL_THROW` needs to be explicitly set to 0. Rather, if `DMLC_LOG_FATAL_THROW` is unset, `DMLC_LOG_FATAL_THROW == 0` will evaluate true during preprocessing at least if the preprocessor implements the standard correctly. Some older preprocessors may not have conformed to standard and this logic may have thus appeared to work.

It certainly does not work on GCC7 or Clang 9..",IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-581265363,leezu,2020-02-03 06:52:35,13956,[17509],Code bug,0,"> After all replacements due to macro expansion and the defined unary operator have been performed, all remaining identifiers and keywords, except for true and false, are replaced with the pp-number 0 [url] Thus it is not true that [code] needs to be explicitly set to 0. Rather, if [code] is unset, [code] will evaluate true during preprocessing at least if the preprocessor implements the standard correctly. Some older preprocessors may not have conformed to standard and this logic may have thus appeared to work. It certainly does not work on GCC7 or Clang 9.."
Thanks for fixing it @leezu ,IssueComment,https://github.com/apache/mxnet/issues/13956#issuecomment-582607251,szha,2020-02-05 20:54:06,13956,[17509],Code bug,0,Thanks for fixing it @leezu
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended label(s): CI, Build",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-530575169,mxnet-label-bot,2019-09-11 21:33:38,16146,[17556],Build bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended label(s): CI, Build"
http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-website-build/,IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-530575255,aaronmarkham,2019-09-11 21:33:53,16146,[17556],Build bug,0,[url]
"It's going to be this PR that triggered the problem:
#15568 ",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-530575536,aaronmarkham,2019-09-11 21:34:42,16146,[17556],Build bug,0,It's going to be this PR that triggered the problem: #15568
"Since #16147 was raised that seems to address this issue and the website publishing has been successful for the last 15 runs, I will close this issue. Please reopen if I am closing in error.",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-531982522,zachgk,2019-09-16 22:27:51,16146,[17556],Build bug,0,"Since #16147 was raised that seems to address this issue and the website publishing has been successful for the last 15 runs, I will close this issue. Please reopen if I am closing in error."
"Seems to be failing again due to a different reason: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwebsite/detail/PR-17302/13/pipeline/
```
[2020-02-07T01:31:11.534Z] Traceback (most recent call last):

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/bin/mkdocs"", line 8, in <module>

[2020-02-07T01:31:11.534Z]     sys.exit(cli())

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/click/core.py"", line 764, in __call__

[2020-02-07T01:31:11.534Z]     return self.main(*args, **kwargs)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/click/core.py"", line 717, in main

[2020-02-07T01:31:11.534Z]     rv = self.invoke(ctx)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/click/core.py"", line 1137, in invoke

[2020-02-07T01:31:11.534Z]     return _process_result(sub_ctx.command.invoke(sub_ctx))

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/click/core.py"", line 956, in invoke

[2020-02-07T01:31:11.534Z]     return ctx.invoke(self.callback, **ctx.params)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/click/core.py"", line 555, in invoke

[2020-02-07T01:31:11.534Z]     return callback(*args, **kwargs)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/__main__.py"", line 162, in build_command

[2020-02-07T01:31:11.534Z]     site_dir=site_dir

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/config/base.py"", line 197, in load_config

[2020-02-07T01:31:11.534Z]     errors, warnings = cfg.validate()

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/config/base.py"", line 115, in validate

[2020-02-07T01:31:11.534Z]     post_failed, post_warnings = self._post_validate()

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/config/base.py"", line 95, in _post_validate

[2020-02-07T01:31:11.534Z]     config_option.post_validation(self, key_name=key)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/config/config_options.py"", line 432, in post_validation

[2020-02-07T01:31:11.534Z]     config[key_name] = theme.Theme(**theme_config)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/theme.py"", line 48, in __init__

[2020-02-07T01:31:11.534Z]     self._load_theme_config(name)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/theme.py"", line 78, in _load_theme_config

[2020-02-07T01:31:11.534Z]     theme_dir = utils.get_theme_dir(name)

[2020-02-07T01:31:11.534Z]   File ""/home/jenkins_slave/.local/lib/python3.5/site-packages/mkdocs/utils/__init__.py"", line 278, in get_theme_dir

[2020-02-07T01:31:11.534Z]     return os.path.dirname(os.path.abspath(theme.load().__file__))

[2020-02-07T01:31:11.534Z]   File ""/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py"", line 2443, in load

[2020-02-07T01:31:11.534Z]     self.require(*args, **kwargs)

[2020-02-07T01:31:11.534Z]   File ""/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py"", line 2466, in require

[2020-02-07T01:31:11.534Z]     items = working_set.resolve(reqs, env, installer, extras=self.extras)

[2020-02-07T01:31:11.534Z]   File ""/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py"", line 792, in resolve

[2020-02-07T01:31:11.534Z]     raise VersionConflict(dist, req).with_context(dependent_req)

[2020-02-07T01:31:11.534Z] pkg_resources.VersionConflict: (Markdown 3.2 (/home/jenkins_slave/.local/lib/python3.5/site-packages), Requirement.parse('markdown<3.2'))

[2020-02-07T01:31:11.534Z] Makefile:19: recipe for target 'all' failed
```",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583200677,haojin2,2020-02-07 02:07:25,16146,[17556],Build bug,0,Seems to be failing again due to a different reason: [url] ``[code]``
"Found here too - http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwebsite/detail/PR-17500/6/pipeline
#17500 ",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583301083,ChaiBapchya,2020-02-07 09:14:15,16146,[17556],Build bug,0,Found here too - [url] #17500
This affects all PRs.,IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583547770,leezu,2020-02-07 18:40:51,16146,[17556],Build bug,0,This affects all PRs.
"Looks like it was working 2 days ago:
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwebsite/detail/PR-17486/3/pipeline/86

What changed since then?",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583613433,samskalicky,2020-02-07 21:09:34,16146,[17556],Build bug,0,Looks like it was working 2 days ago: [url] What changed since then?
"My guess is that the dependencies in the julia docs chain hit this error:
```
ERROR: mkdocs-material 4.6.0 has requirement markdown<3.2, but you'll have markdown 3.2 which is incompatible.
```
It does this:
```
pip install --user pygments mkdocs mkdocs-material python-markdown-math
```
So I'm thinking those all need to be version pinned and roll back mkdocs-material to a version that still works. Or pin markdown to a previous version...

That would be here:
https://github.com/apache/incubator-mxnet/blob/7caffa65e30f37e70796ba165ac5a4265e64974e/julia/docs/Makefile#L23",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583621774,aaronmarkham,2020-02-07 21:29:04,16146,[17556],Build bug,0,My guess is that the dependencies in the julia docs chain hit this error: ``[code]`[code]`[code]`` So I'm thinking those all need to be version pinned and roll back mkdocs-material to a version that still works. Or pin markdown to a previous version... That would be here: [url]#L23
"I'm about to head out to the airport and be traveling, so I don't know if I can test this and help with a patch in a reasonable amount of time... any takers?",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583622171,aaronmarkham,2020-02-07 21:30:00,16146,[17556],Build bug,0,"I'm about to head out to the airport and be traveling, so I don't know if I can test this and help with a patch in a reasonable amount of time... any takers?"
"@aaronmarkham indeed it's a version problem:
`pkg_resources.VersionConflict: (Markdown 3.2 (/home/jenkins_slave/.local/lib/python3.5/site-packages), Requirement.parse('markdown<3.2'))`",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583625138,haojin2,2020-02-07 21:37:01,16146,[17556],Build bug,0,@aaronmarkham indeed it's a version problem: [code]
"working on a fix, will have a PR out soon. Currently verifying my fix",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583626780,haojin2,2020-02-07 21:41:03,16146,[17556],Build bug,0,"working on a fix, will have a PR out soon. Currently verifying my fix"
#17549 is merged but looks like the issue is still there.,IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583805903,TaoLv,2020-02-09 05:15:36,16146,[17556],Build bug,0,#17549 is merged but looks like the issue is still there.
"Oh boy:

```
04:34:06  ERROR: pymdown-extensions 6.3 has requirement Markdown>=3.2, but you'll have markdown 3.1 which is incompatible.
04:34:06  ERROR: mkdocs-material 4.6.2 has requirement markdown>=3.2, but you'll have markdown 3.1 which is incompatible.
```

So these dependencies are fighting over this markdown version. One says it must be 3.2 or greater the other says it must be less.

I have an idea:
If someone has an older setup building locally that works - try that and export the pip versions and we pin it like that. Or maybe there's some old docker image that has this setup? 

Also, to get unblocked, we can disable Julia docs from the website build flow.
",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583870316,aaronmarkham,2020-02-09 17:21:02,16146,[17556],Build bug,0,"Oh boy: ``[code]`` So these dependencies are fighting over this markdown version. One says it must be 3.2 or greater the other says it must be less. I have an idea: If someone has an older setup building locally that works - try that and export the pip versions and we pin it like that. Or maybe there's some old docker image that has this setup? Also, to get unblocked, we can disable Julia docs from the website build flow."
"So it worked for me on my test. Not sure why exactly, but I ended up with slightly older versions of things. From my log:
```
mkdocs==1.0.4
mkdocs-material==4.6.0
pymdown-extensions==6.2.1 
python-markdown-math==0.6
```
I think maybe adding these will do the trick. Considering that my current setup isn't failing, I'm not entirely sure how to test this out, but it's worth a try.

## Side note
Now, I tried to run `pip freeze` but the Docker image that I had left was (I guess) before these things ran. The logs state:

```
2020-02-08 00:43:31,273 - root - INFO - Waiting for status of container 73b619cf6eea for 600 s.
2020-02-08 00:43:31,467 - root - INFO - Container exit status: {'Error': None, 'StatusCode': 0}
2020-02-08 00:43:31,467 - root - INFO - Container exited with success 👍
2020-02-08 00:43:31,467 - root - INFO - Executed command for reproduction:

ci/build.py --docker-registry mxnetci --platform ubuntu_cpu_julia /work/runtime_functions.sh build_julia_docs

2020-02-08 00:43:31,468 - root - INFO - Stopping container: 73b619cf6eea
2020-02-08 00:43:31,469 - root - INFO - Removing container: 73b619cf6eea
```
So it removed what I had as a working state. How unfortunate!
Anyone know what flag (or whatever) I could have set when running build.py so that I'd get to keep the last version of the container around for testing?


",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583906642,aaronmarkham,2020-02-09 23:21:50,16146,[17556],Build bug,0,"So it worked for me on my test. Not sure why exactly, but I ended up with slightly older versions of things. From my log: ``[code]`[code]pip freeze[code]`[code]`` So it removed what I had as a working state. How unfortunate! Anyone know what flag (or whatever) I could have set when running build.py so that I'd get to keep the last version of the container around for testing?"
"Btw, this should have been a new issue, rather than re-opening an unrelated issue... sure, the title matches, but the errors were not the same. Anyhoo, I hope I fixed it with the above PR. If it doesn't work, then maybe the jenkins nodes are caching and they need to be restarted. If I can run the CI build and have it pass, then it I'd expect it to run in production! Otherwise, we're missing an important test capability.",IssueComment,https://github.com/apache/mxnet/issues/16146#issuecomment-583921512,aaronmarkham,2020-02-10 01:30:00,16146,[17556],Build bug,0,"Btw, this should have been a new issue, rather than re-opening an unrelated issue... sure, the title matches, but the errors were not the same. Anyhoo, I hope I fixed it with the above PR. If it doesn't work, then maybe the jenkins nodes are caching and they need to be restarted. If I can run the CI build and have it pass, then it I'd expect it to run in production! Otherwise, we're missing an important test capability."
"Hi @D-Roberts, I am the author of `np.random.gamma`. It seems that the rejection sampling got trapped in an infinity loop because of the invalid parameter (scale = 0), as no samples could be accepted according to the standard formula. 

I will create a fix soon.

However,  https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.gamma.html the doc for official NumPy says ""The scale of the gamma distribution. Should be greater than zero."", 
So IMO, returning zero is kind of like undefined behavior.
Do you have any practical examples that involves setting scale to be zero? If not, I would consider raising an ValueError exception rather than returning zero.",IssueComment,https://github.com/apache/mxnet/issues/17529#issuecomment-582907092,xidulu,2020-02-06 13:32:30,17529,[17575],Data bug,1,"Hi @D-Roberts, I am the author of [code]. It seems that the rejection sampling got trapped in an infinity loop because of the invalid parameter (scale = 0), as no samples could be accepted according to the standard formula. I will create a fix soon. However, [url] the doc for official NumPy says ""The scale of the gamma distribution. Should be greater than zero."", So IMO, returning zero is kind of like undefined behavior. Do you have any practical examples that involves setting scale to be zero? If not, I would consider raising an ValueError exception rather than returning zero."
"Hi @xidulu .

I actually agree with raising a ValueError. The only reason I tried these edge cases is because I found some of these inconsistencies in Numpy, where the documentation says that the parameter should be > 0 but then a value is outputed for 0 instead of an Error message.",IssueComment,https://github.com/apache/mxnet/issues/17529#issuecomment-582956614,D-Roberts,2020-02-06 15:24:10,17529,[17575],Data bug,1,"Hi @xidulu . I actually agree with raising a ValueError. The only reason I tried these edge cases is because I found some of these inconsistencies in Numpy, where the documentation says that the parameter should be > 0 but then a value is outputed for 0 instead of an Error message."
@ChaiBapchya @connorgoggins Would you like to take a look at this?,IssueComment,https://github.com/apache/mxnet/issues/17640#issuecomment-589386472,apeforest,2020-02-20 22:36:10,17640,[17642],Code bug,0,@ChaiBapchya @connorgoggins Would you like to take a look at this?
@apeforest working to reproduce now.,IssueComment,https://github.com/apache/mxnet/issues/17640#issuecomment-589387277,connorgoggins,2020-02-20 22:38:11,17640,[17642],Code bug,0,@apeforest working to reproduce now.
"Reproduced on CPU, debugging now.",IssueComment,https://github.com/apache/mxnet/issues/17640#issuecomment-589401143,connorgoggins,2020-02-20 23:10:56,17640,[17642],Code bug,0,"Reproduced on CPU, debugging now."
Implemented fix here: [[OpPerf] Fixed Python profiler bug](https://github.com/apache/incubator-mxnet/pull/17642) @apeforest ,IssueComment,https://github.com/apache/mxnet/issues/17640#issuecomment-589411606,connorgoggins,2020-02-20 23:38:37,17640,[17642],Code bug,0,Implemented fix here: [[OpPerf] Fixed Python profiler bug]([url] @apeforest
@leezu @apeforest Have you met problems when building horovod with cmake-built MXNet?,IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-588403782,sxjscience,2020-02-19 19:28:06,17628,[17647],Build bug,0,@leezu @apeforest Have you met problems when building horovod with cmake-built MXNet?
"A temporary fix is to manually copy files inside the `build` directory to `include/mkldnn`. I've confirmed that this solves the problem in horovod installation.
```
cp build/3rdparty/mkldnn/include/dnnl_config.h include/mkldnn
cp build/3rdparty/mkldnn/include/dnnl_version.h include/mkldnn
```
",IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-588406327,sxjscience,2020-02-19 19:32:57,17628,[17647],Build bug,0,A temporary fix is to manually copy files inside the [code] directory to [code]. I've confirmed that this solves the problem in horovod installation. ``[code]``
"I will take a look at this. I remember previously the recommended build system of mxnet was makefile when cooperating with horovod.
@sxjscience is this the only issue when using cmake to build mxnet and linking it to horovod?",IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-588559555,TaoLv,2020-02-20 01:15:03,17628,[17647],Build bug,0,I will take a look at this. I remember previously the recommended build system of mxnet was makefile when cooperating with horovod. @sxjscience is this the only issue when using cmake to build mxnet and linking it to horovod?
"That’s currently the only one I found for linking to horovod. However I just ran setup.py in horovod and hasn’t run distributed training yet. I’m having some other issues in the cmake-built MXNet which I’m still investigating.

Get Outlook for iOS<https://aka.ms/o0ukef>
________________________________
From: Tao Lv <notifications@github.com>
Sent: Wednesday, February 19, 2020 5:15:05 PM
To: apache/incubator-mxnet <incubator-mxnet@noreply.github.com>
Cc: Xingjian SHI <xshiab@connect.ust.hk>; Mention <mention@noreply.github.com>
Subject: Re: [apache/incubator-mxnet] Missing dnnl_config.h when installing horovod (#17628)


I will take a look at this. I remember previously the recommended build system of mxnet was makefile when cooperating with horovod.
@sxjscience<https://github.com/sxjscience> is this the only issue when using cmake to build mxnet and linking it to horovod?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/apache/incubator-mxnet/issues/17628?email_source=notifications&email_token=ABHQH3QTVXH2GQ6L4GGODWTRDXKRTA5CNFSM4KYACRAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMKLJQY#issuecomment-588559555>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABHQH3VFOAX6T6T4WKRZDY3RDXKRTANCNFSM4KYACRAA>.
",IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-588600364,sxjscience,2020-02-20 04:13:59,17628,[17647],Build bug,0,"That’s currently the only one I found for linking to horovod. However I just ran setup.py in horovod and hasn’t run distributed training yet. I’m having some other issues in the cmake-built MXNet which I’m still investigating. Get Outlook for iOS<[url] ________________________________ From: Tao Lv <[email]> Sent: Wednesday, February 19, 2020 5:15:05 PM To: apache/incubator-mxnet <[email]> Cc: Xingjian SHI <[email]>; Mention <[email]> Subject: Re: [apache/incubator-mxnet] Missing dnnl_config.h when installing horovod (#17628) I will take a look at this. I remember previously the recommended build system of mxnet was makefile when cooperating with horovod. @sxjscience<[url] is this the only issue when using cmake to build mxnet and linking it to horovod? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub<[url]#issuecomment-588559555>, or unsubscribe<[url]"
"@TaoLv what's your status on this issue?
I think this issue is currently blocking Makefile build removal so we should fix it soon. (You mentioned you plan to work on it, so please let us know if you don't plan to work on it soon.)",IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-589465517,leezu,2020-02-21 02:15:09,17628,[17647],Build bug,0,"@TaoLv what's your status on this issue? I think this issue is currently blocking Makefile build removal so we should fix it soon. (You mentioned you plan to work on it, so please let us know if you don't plan to work on it soon.)"
@leezu I‘m working on a fix.,IssueComment,https://github.com/apache/mxnet/issues/17628#issuecomment-589471582,TaoLv,2020-02-21 02:43:18,17628,[17647],Build bug,0,@leezu I‘m working on a fix.
looking into it. thanks for reporting.,IssueComment,https://github.com/apache/mxnet/issues/17638#issuecomment-589325837,yzhliu,2020-02-20 21:24:56,17638,[17674],Data bug,1,looking into it. thanks for reporting.
"I find it could be further simplified:

```python
import mxnet as mx
import numpy as np
from numpy.testing import assert_allclose
from mxnet.gluon import HybridBlock
mx.npx.set_np()

class Foo(HybridBlock):
    def __init__(self, prefix=None, params=None):
        super(Foo, self).__init__(prefix=prefix, params=params)

    def hybrid_forward(self, F, valid_length):
        mask = (F.np.ones((10,)) < valid_length).astype(np.float32)
        return mask

foo = Foo()
foo.hybridize()
out = foo(mx.np.ones((10,), ctx=mx.gpu()))
print(out)
```",IssueComment,https://github.com/apache/mxnet/issues/17638#issuecomment-589329816,sxjscience,2020-02-20 21:34:45,17638,[17674],Data bug,1,I find it could be further simplified: ``[code]``
"```python
import mxnet as mx
import numpy as np
import os
from numpy.testing import assert_allclose
from mxnet.gluon import HybridBlock
mx.npx.set_np()

os.environ['DMLC_LOG_STACK_TRACE_DEPTH'] = '30'

class Foo(HybridBlock):
    def __init__(self, prefix=None, params=None):
        super(Foo, self).__init__(prefix=prefix, params=params)

    def hybrid_forward(self, F, valid_length):
        mask = (F.np.ones((10,)) < valid_length).astype(np.float32)
        return mask

foo = Foo()
foo.hybridize()
out = foo(mx.np.ones((10,), ctx=mx.gpu()))
print(out)
```

```
MXNetError: Traceback (most recent call last):
  [bt] (12) libmxnet.so(MXInvokeCachedOpEx+0x60) [0x7fce362ab420]
  [bt] (11) libmxnet.so(MXInvokeCachedOp+0x42e) [0x7fce362aad4e]
  [bt] (10) libmxnet.so(mxnet::CachedOp::Forward(std::shared_ptr<mxnet::CachedOp> const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0xc77) [0x7fce36421d27]
  [bt] (9) libmxnet.so(mxnet::CachedOp::DynamicForward(mxnet::Context const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, bool)+0x20c) [0x7fce3641973c]
  [bt] (8) libmxnet.so(mxnet::CachedOp::SetForwardGraph(mxnet::CachedOp::GraphInfo*, bool, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x965) [0x7fce36418525]
  [bt] (7) libmxnet.so(mxnet::imperative::MXPlanMemory(nnvm::Graph*, std::vector<int, std::allocator<int> >&&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::pair<unsigned int, unsigned int> const&, std::pair<unsigned int, unsigned int> const&, bool)+0x1e8) [0x7fce3642f598]
  [bt] (6) libmxnet.so(nnvm::ApplyPass(nnvm::Graph, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0x213) [0x7fce362d0ab3]
  [bt] (5) libmxnet.so(nnvm::ApplyPasses(nnvm::Graph, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)+0xecd) [0x7fce3a0b87bd]
  [bt] (4) libmxnet.so(std::_Function_handler<nnvm::Graph (nnvm::Graph), nnvm::Graph (*)(nnvm::Graph)>::_M_invoke(std::_Any_data const&, nnvm::Graph&&)+0x10a) [0x7fce365e6b9a]
  [bt] (3) libmxnet.so(+0x1f279d0) [0x7fce366169d0]
  [bt] (2) libmxnet.so(+0x1f26a6e) [0x7fce36615a6e]
  [bt] (1) libmxnet.so(+0x1f2424d) [0x7fce3661324d]
  [bt] (0) libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x72) [0x7fce3622b982]
  File ""../src/nnvm/plan_memory.cc"", line 58
MXNetError: unknown type_flag=7
```",IssueComment,https://github.com/apache/mxnet/issues/17638#issuecomment-589367294,sxjscience,2020-02-20 22:07:37,17638,[17674],Data bug,1,``[code]`[code]`[code]``
"@ZheyuYe The C++ side implementation of the shape inferring logic is here: https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/src/operator/nn/layer_norm.cc#L39-L66

The python side is here: https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/python/mxnet/gluon/nn/basic_layers.py#L609-L614

The problem should be to check the shape of gamma and beta:
https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/src/operator/nn/layer_norm.cc#L56-L57

Would you try to investigate the issue? You can append `std::cout << in_shape->at(layernorm::kGamma)`, which should not be empty when `in_channel` is given.

I think one way to solve the prioblem is to use the same `SHAPE_ASSIGN_CHECK` as here:
https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/src/operator/numpy/np_where_op.cc#L42
",IssueComment,https://github.com/apache/mxnet/issues/17654#issuecomment-589741619,sxjscience,2020-02-21 16:59:33,17654,[17683],Code bug,1,"@ZheyuYe The C++ side implementation of the shape inferring logic is here: [url]#L39-L66 The python side is here: [url]#L609-L614 The problem should be to check the shape of gamma and beta: [url]#L56-L57 Would you try to investigate the issue? You can append [code], which should not be empty when [code] is given. I think one way to solve the prioblem is to use the same [code] as here: [url]#L42"
"Sure, please open a PR.",IssueComment,https://github.com/apache/mxnet/issues/17704#issuecomment-591810287,leezu,2020-02-27 06:45:05,17704,[17709],Algorithm design bug,0,"Sure, please open a PR."
"~~~
┌─┤[$]|[sl1pkn07]|[sL1pKn07]|[~/aplicaciones/incubator-mxnet]|
└───╼  find . -name .gitignore
./python/.gitignore
./python/mxnet/gluon/.gitignore
./amalgamation/.gitignore
./julia/models/Inception/.gitignore
./julia/.gitignore
./julia/examples/char-lstm/.gitignore
./julia/docs/.gitignore
./scala-package/.mvn/wrapper/.gitignore
./scala-package/.gitignore
./example/ssd/tools/caffe_converter/.gitignore
./example/neural-style/.gitignore
./example/gluon/lipnet/.gitignore
./example/cnn_text_classification/.gitignore
./example/recommenders/.gitignore
./.gitignore
./tools/coreml/pip_package/.gitignore
./tools/bandwidth/.gitignore
./tools/caffe_converter/.gitignore
./contrib/clojure-package/.gitignore
./contrib/clojure-package/src/org/apache/clojure_mxnet/gen/.gitignore
./contrib/clojure-package/examples/multi-label/.gitignore
./contrib/clojure-package/examples/imclassification/.gitignore
./contrib/clojure-package/examples/neural-style/.gitignore
./contrib/clojure-package/examples/bert/.gitignore
./contrib/clojure-package/examples/pre-trained-models/.gitignore
./contrib/clojure-package/examples/rnn/.gitignore
./contrib/clojure-package/examples/captcha/.gitignore
./contrib/clojure-package/examples/cnn-text-classification/.gitignore
./contrib/clojure-package/examples/tutorial/.gitignore
./contrib/clojure-package/examples/profiler/.gitignore
./contrib/clojure-package/examples/visualization/.gitignore
./contrib/clojure-package/examples/infer/objectdetector/.gitignore
./contrib/clojure-package/examples/infer/predictor/.gitignore
./contrib/clojure-package/examples/infer/imageclassifier/.gitignore
./contrib/clojure-package/examples/gan/.gitignore
./3rdparty/mshadow/guide/.gitignore
./3rdparty/mshadow/guide/exp-template/.gitignore
./3rdparty/mshadow/guide/mshadow-ps/.gitignore
./3rdparty/mshadow/.gitignore
./3rdparty/mshadow/mshadow-ps/.gitignore
./perl-package/.gitignore
./tests/nightly/.gitignore
./tests/cpp/.gitignore
./tests/.gitignore
./docker/.gitignore
./R-package/.gitignore
./docs/static_site/.gitignore
./docs/static_site/src/.gitignore
./docs/.gitignore
./docs/python_docs/python/.gitignore
./docs/python_docs/themes/.gitignore
~~~

also this .gitignore ?",IssueComment,https://github.com/apache/mxnet/issues/17704#issuecomment-591866571,sl1pkn07,2020-02-27 09:20:01,17704,[17709],Algorithm design bug,0,~~~ ┌─┤[$]|[sl1pkn07]|[sL1pKn07]|[~/aplicaciones/incubator-mxnet]| └───╼ find . -name .gitignore ./python/.gitignore ./python/mxnet/gluon/.gitignore ./amalgamation/.gitignore ./julia/models/Inception/.gitignore ./julia/.gitignore ./julia/examples/char-lstm/.gitignore ./julia/docs/.gitignore ./scala-package/.mvn/wrapper/.gitignore ./scala-package/.gitignore ./example/ssd/tools/caffe_converter/.gitignore ./example/neural-style/.gitignore ./example/gluon/lipnet/.gitignore ./example/cnn_text_classification/.gitignore ./example/recommenders/.gitignore ./.gitignore ./tools/coreml/pip_package/.gitignore ./tools/bandwidth/.gitignore ./tools/caffe_converter/.gitignore ./contrib/clojure-package/.gitignore ./contrib/clojure-package/src/org/apache/clojure_mxnet/gen/.gitignore ./contrib/clojure-package/examples/multi-label/.gitignore ./contrib/clojure-package/examples/imclassification/.gitignore ./contrib/clojure-package/examples/neural-style/.gitignore ./contrib/clojure-package/examples/bert/.gitignore ./contrib/clojure-package/examples/pre-trained-models/.gitignore ./contrib/clojure-package/examples/rnn/.gitignore ./contrib/clojure-package/examples/captcha/.gitignore ./contrib/clojure-package/examples/cnn-text-classification/.gitignore ./contrib/clojure-package/examples/tutorial/.gitignore ./contrib/clojure-package/examples/profiler/.gitignore ./contrib/clojure-package/examples/visualization/.gitignore ./contrib/clojure-package/examples/infer/objectdetector/.gitignore ./contrib/clojure-package/examples/infer/predictor/.gitignore ./contrib/clojure-package/examples/infer/imageclassifier/.gitignore ./contrib/clojure-package/examples/gan/.gitignore ./3rdparty/mshadow/guide/.gitignore ./3rdparty/mshadow/guide/exp-template/.gitignore ./3rdparty/mshadow/guide/mshadow-ps/.gitignore ./3rdparty/mshadow/.gitignore ./3rdparty/mshadow/mshadow-ps/.gitignore ./perl-package/.gitignore ./tests/nightly/.gitignore ./tests/cpp/.gitignore ./tests/.gitignore ./docker/.gitignore ./R-package/.gitignore ./docs/static_site/.gitignore ./docs/static_site/src/.gitignore ./docs/.gitignore ./docs/python_docs/python/.gitignore ./docs/python_docs/themes/.gitignore ~~~ also this .gitignore ?
"It's valid to have .gitignore in subdirectories, so it's not necessary to change all. `cpp-package/include/mxnet-cpp/.gitignore` is copied together with the include files, so there is a good reason to fix `cpp-package/include/mxnet-cpp/.gitignore`. You could move it to `cpp-package/.gitignore` for example.",IssueComment,https://github.com/apache/mxnet/issues/17704#issuecomment-592091298,leezu,2020-02-27 17:51:24,17704,[17709],Algorithm design bug,0,"It's valid to have .gitignore in subdirectories, so it's not necessary to change all. [code] is copied together with the include files, so there is a good reason to fix [code]. You could move it to [code] for example."
done,IssueComment,https://github.com/apache/mxnet/issues/17704#issuecomment-592124328,sl1pkn07,2020-02-27 19:04:11,17704,[17709],Algorithm design bug,0,done
"Please post the output of `cmake -DUSE_CUDA=1 -DUSE_MKLDNN=1 -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DBUILD_CYTHON_MODULES=1 -DUSE_DIST_KVSTORE=1 ..`.

It seems protobuf is not found correctly on your system. If this is the case and if protobuf is required, we can change the `CMakeLists.txt` to error out in case of not finding protobuf.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-574228943,leezu,2020-01-14 15:28:29,17299,[17732],Build bug,0,"Please post the output of [code]. It seems protobuf is not found correctly on your system. If this is the case and if protobuf is required, we can change the [code] to error out in case of not finding protobuf."
"Besides protobuf, zmq is also a required dependency. However, cmake currently treats it as optional, leading to build error

`../3rdparty/ps-lite/src/./zmq_van.h:8:10: fatal error: zmq.h: No such file or directory
 #include <zmq.h>`

if `-DUSE_DIST_KVSTORE=1` is used.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-574230385,leezu,2020-01-14 15:31:05,17299,[17732],Build bug,0,"Besides protobuf, zmq is also a required dependency. However, cmake currently treats it as optional, leading to build error [code] if [code] is used."
"> Please post the output of `cmake -DUSE_CUDA=1 -DUSE_MKLDNN=1 -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DBUILD_CYTHON_MODULES=1 -DUSE_DIST_KVSTORE=1 ..`.
> 
> It seems protobuf is not found correctly on your system. If this is the case and if protobuf is required, we can change the `CMakeLists.txt` to error out in case of not finding protobuf.

This is the output of cmake:

-- The C compiler identification is GNU 7.4.0
-- The CXX compiler identification is GNU 7.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- CMAKE_CROSSCOMPILING FALSE
-- CMAKE_HOST_SYSTEM_PROCESSOR x86_64
-- CMAKE_SYSTEM_PROCESSOR x86_64
-- CMAKE_SYSTEM_NAME Linux
-- CMake version '3.13.3' using generator 'Unix Makefiles'
-- The CUDA compiler identification is NVIDIA 10.0.130
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- works
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Performing Test SUPPORT_CXX11
-- Performing Test SUPPORT_CXX11 - Success
-- Performing Test SUPPORT_CXX0X
-- Performing Test SUPPORT_CXX0X - Success
-- Performing Test SUPPORT_MSSE3
-- Performing Test SUPPORT_MSSE3 - Success
-- Performing Test SUPPORT_MSSE2
-- Performing Test SUPPORT_MSSE2 - Success
-- Determining F16C support
-- Performing Test COMPILER_SUPPORT_MF16C
-- Performing Test COMPILER_SUPPORT_MF16C - Success
-- F16C enabled
-- CMAKE_BUILD_TYPE is unset, defaulting to Release
-- MKL-DNN compat: set DNNL_BUILD_EXAMPLES to MKLDNN_BUILD_EXAMPLES with value `OFF`
-- MKL-DNN compat: set DNNL_BUILD_TESTS to MKLDNN_BUILD_TESTS with value `OFF`
-- MKL-DNN compat: set DNNL_ENABLE_JIT_PROFILING to MKLDNN_ENABLE_JIT_PROFILING with value `OFF`
-- MKL-DNN compat: set DNNL_LIBRARY_TYPE to MKLDNN_LIBRARY_TYPE with value `STATIC`
-- MKL-DNN compat: set DNNL_ARCH_OPT_FLAGS to MKLDNN_ARCH_OPT_FLAGS with value ``
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Found OpenMP_C: -fopenmp (found version ""4.5"") 
-- Found OpenMP_CXX: -fopenmp (found version ""4.5"") 
-- Found OpenMP: TRUE (found version ""4.5"")  
-- GPU support is disabled
-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) 
-- Found Git: /usr/bin/git (found version ""2.17.1"") 
-- Intel(R) VTune(TM) Amplifier JIT profiling disabled
-- Could NOT find MKL (missing: MKL_INCLUDE_DIR MKL_RT_LIBRARY) 
-- Found OpenBLAS libraries: /usr/local/lib/libopenblas.so
-- Found OpenBLAS include: /usr/local/include
-- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") 
-- Could NOT find Jemalloc (missing: JEMALLOC_LIBRARY JEMALLOC_INCLUDE_DIR) 
-- Found OpenCV: /usr/local (found version ""4.0.0"") found components:  core highgui imgproc imgcodecs 
-- OpenCV 4.0.0 found (/usr/local/lib/cmake/opencv4)
--  OpenCV_LIBS=opencv_core;opencv_highgui;opencv_imgproc;opencv_imgcodecs
-- Found OpenMP_C: -fopenmp  
-- Found OpenMP_CXX: -fopenmp  
-- Found OpenMP: TRUE   
-- Performing Test OPENMP_HAVE_WERROR_FLAG
-- Performing Test OPENMP_HAVE_WERROR_FLAG - Success
-- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG
-- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG - Success
-- Performing Test OPENMP_HAVE_STD_CPP11_FLAG
-- Performing Test OPENMP_HAVE_STD_CPP11_FLAG - Success
-- Found PythonInterp: /home/ubuntu/anaconda3/bin/python (found version ""3.6.6"") 
-- Cannot find llvm-lit.
-- Please put llvm-lit in your PATH, set OPENMP_LLVM_LIT_EXECUTABLE to its full path, or point OPENMP_LLVM_TOOLS_DIR to its directory.
CMake Warning at 3rdparty/openmp/cmake/OpenMPTesting.cmake:22 (message):
  The check targets will not be available!
Call Stack (most recent call first):
  3rdparty/openmp/cmake/OpenMPTesting.cmake:40 (find_standalone_test_dependencies)
  3rdparty/openmp/CMakeLists.txt:49 (include)


-- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG
-- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG - Success
-- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG
-- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG - Success
-- Performing Test LIBOMP_HAVE_X_CPP_FLAG
-- Performing Test LIBOMP_HAVE_X_CPP_FLAG - Success
-- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG
-- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG
-- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG
-- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG
-- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG
-- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG
-- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG
-- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG - Success
-- Performing Test LIBOMP_HAVE_MSSE2_FLAG
-- Performing Test LIBOMP_HAVE_MSSE2_FLAG - Success
-- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG
-- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG - Success
-- Performing Test LIBOMP_HAVE_MMIC_FLAG
-- Performing Test LIBOMP_HAVE_MMIC_FLAG - Failed
-- Performing Test LIBOMP_HAVE_M32_FLAG
-- Performing Test LIBOMP_HAVE_M32_FLAG - Failed
-- Performing Test LIBOMP_HAVE_X_FLAG
-- Performing Test LIBOMP_HAVE_X_FLAG - Success
-- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG
-- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG - Success
-- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG
-- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG - Success
-- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG
-- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG - Success
-- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG
-- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG - Success
-- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG
-- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG - Success
-- Performing Test LIBOMP_HAVE_FINI_FLAG
-- Performing Test LIBOMP_HAVE_FINI_FLAG - Success
-- Found Perl: /usr/bin/perl (found version ""5.26.1"") 
-- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS
-- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS - Success
-- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS
-- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS - Success
-- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE
-- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE - Success
-- Looking for include files windows.h, psapi.h
-- Looking for include files windows.h, psapi.h - not found
-- Looking for EnumProcessModules in psapi
-- Looking for EnumProcessModules in psapi - not found
-- LIBOMP: Operating System     -- Linux
-- LIBOMP: Target Architecture  -- x86_64
-- LIBOMP: Build Type           -- Release
-- LIBOMP: Library Kind         -- SHARED
-- LIBOMP: Library Type         -- normal
-- LIBOMP: Fortran Modules      -- FALSE
-- LIBOMP: Build                -- 20140926
-- LIBOMP: Use Stats-gathering  -- FALSE
-- LIBOMP: Use Debugger-support -- FALSE
-- LIBOMP: Use ITT notify       -- TRUE
-- LIBOMP: Use OMPT-support     -- TRUE
-- LIBOMP: Use OMPT-optional  -- TRUE
-- LIBOMP: Use Adaptive locks   -- TRUE
-- LIBOMP: Use quad precision   -- TRUE
-- LIBOMP: Use TSAN-support     -- FALSE
-- LIBOMP: Use Hwloc library    -- FALSE
-- Looking for sqrt in m
-- Looking for sqrt in m - found
-- Looking for __atomic_load_1
-- Looking for __atomic_load_1 - not found
-- Looking for __atomic_load_1 in atomic
-- Looking for __atomic_load_1 in atomic - found
-- check-libomp does nothing.
-- check-ompt does nothing.
-- check-openmp does nothing.
USE_LAPACK is ON
-- Could NOT find Jemalloc (missing: JEMALLOC_LIBRARY JEMALLOC_INCLUDE_DIR) 
CMake Warning at 3rdparty/googletest/googletest/CMakeLists.txt:47 (project):
  VERSION keyword not followed by a value or was followed by a value that
  expanded to nothing.


-- Found GTest: gtest  
-- Found CUDNN: /usr/local/cuda/lib64/libcudnn.so  
-- Looking for clock_gettime in rt
-- Looking for clock_gettime in rt - found
-- Looking for fopen64
-- Looking for fopen64 - not found
-- Looking for C++ include cxxabi.h
-- Looking for C++ include cxxabi.h - found
-- Looking for nanosleep
-- Looking for nanosleep - found
-- Looking for backtrace
-- Looking for backtrace - found
-- backtrace facility detected in default set of libraries
-- Found Backtrace: /usr/include  
-- Check if the system is big endian
-- Searching 16 bit integer
-- Looking for sys/types.h
-- Looking for sys/types.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for stddef.h
-- Looking for stddef.h - found
-- Check size of unsigned short
-- Check size of unsigned short - done
-- Using unsigned short
-- Check if the system is big endian - little endian
-- /home/ubuntu/incubator-mxnet/3rdparty/dmlc-core/cmake/build_config.h.in -> include/dmlc/build_config.h
-- Found OpenMP_C: -fopenmp  
-- Found OpenMP_CXX: -fopenmp  
-- Automatic GPU detection failed. Building for common architectures.
-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;7.0;7.0+PTX;7.5;7.5+PTX
-- CUDA: Using the following NVCC architecture flags -gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75
-- Found CUDAToolkit: /usr/local/cuda/include (found version ""10.0.130"") 
-- Could NOT find ZMQ (missing: ZMQ_LIBRARY ZMQ_INCLUDE_DIR) 
CMake Warning at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.13/Modules/FindProtobuf.cmake:495 (message):
  Protobuf compiler version 3.8.0 doesn't match library version 3.0.0
Call Stack (most recent call first):
  3rdparty/ps-lite/cmake/ProtoBuf.cmake:4 (find_package)
  3rdparty/ps-lite/CMakeLists.txt:22 (include)


-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version ""3.0.0"") 
-- Found PROTOBUF Compiler: /home/ubuntu/anaconda3/bin/protoc
-- Found PythonInterp: /usr/bin/python2 (found suitable exact version ""2.7.15"") 
-- Cython modules for python2 will be built
-- Found PythonInterp: /home/ubuntu/anaconda3/bin/python3 (found suitable exact version ""3.6.6"") 
-- Cython modules for python3 will be built
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ubuntu/incubator-mxnet/build",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575129847,ChaokunChang,2020-01-16 12:28:04,17299,[17732],Build bug,0,"> Please post the output of [code]. > > It seems protobuf is not found correctly on your system. If this is the case and if protobuf is required, we can change the [code] to error out in case of not finding protobuf. This is the output of cmake: -- The C compiler identification is GNU 7.4.0 -- The CXX compiler identification is GNU 7.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- CMAKE_CROSSCOMPILING FALSE -- CMAKE_HOST_SYSTEM_PROCESSOR x86_64 -- CMAKE_SYSTEM_PROCESSOR x86_64 -- CMAKE_SYSTEM_NAME Linux -- CMake version '3.13.3' using generator 'Unix Makefiles' -- The CUDA compiler identification is NVIDIA 10.0.130 -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- works -- Detecting CUDA compiler ABI info -- Detecting CUDA compiler ABI info - done -- Performing Test SUPPORT_CXX11 -- Performing Test SUPPORT_CXX11 - Success -- Performing Test SUPPORT_CXX0X -- Performing Test SUPPORT_CXX0X - Success -- Performing Test SUPPORT_MSSE3 -- Performing Test SUPPORT_MSSE3 - Success -- Performing Test SUPPORT_MSSE2 -- Performing Test SUPPORT_MSSE2 - Success -- Determining F16C support -- Performing Test COMPILER_SUPPORT_MF16C -- Performing Test COMPILER_SUPPORT_MF16C - Success -- F16C enabled -- CMAKE_BUILD_TYPE is unset, defaulting to Release -- MKL-DNN compat: set DNNL_BUILD_EXAMPLES to MKLDNN_BUILD_EXAMPLES with value [code] -- MKL-DNN compat: set DNNL_BUILD_TESTS to MKLDNN_BUILD_TESTS with value [code] -- MKL-DNN compat: set DNNL_ENABLE_JIT_PROFILING to MKLDNN_ENABLE_JIT_PROFILING with value [code] -- MKL-DNN compat: set DNNL_LIBRARY_TYPE to MKLDNN_LIBRARY_TYPE with value [code] -- MKL-DNN compat: set DNNL_ARCH_OPT_FLAGS to MKLDNN_ARCH_OPT_FLAGS with value `` -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found OpenMP_C: -fopenmp (found version ""4.5"") -- Found OpenMP_CXX: -fopenmp (found version ""4.5"") -- Found OpenMP: TRUE (found version ""4.5"") -- GPU support is disabled -- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) -- Found Git: /usr/bin/git (found version ""2.17.1"") -- Intel(R) VTune(TM) Amplifier JIT profiling disabled -- Could NOT find MKL (missing: MKL_INCLUDE_DIR MKL_RT_LIBRARY) -- Found OpenBLAS libraries: /usr/local/lib/libopenblas.so -- Found OpenBLAS include: /usr/local/include -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") -- Could NOT find Jemalloc (missing: JEMALLOC_LIBRARY JEMALLOC_INCLUDE_DIR) -- Found OpenCV: /usr/local (found version ""4.0.0"") found components: core highgui imgproc imgcodecs -- OpenCV 4.0.0 found (/usr/local/lib/cmake/opencv4) -- OpenCV_LIBS=opencv_core;opencv_highgui;opencv_imgproc;opencv_imgcodecs -- Found OpenMP_C: -fopenmp -- Found OpenMP_CXX: -fopenmp -- Found OpenMP: TRUE -- Performing Test OPENMP_HAVE_WERROR_FLAG -- Performing Test OPENMP_HAVE_WERROR_FLAG - Success -- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG -- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG - Success -- Performing Test OPENMP_HAVE_STD_CPP11_FLAG -- Performing Test OPENMP_HAVE_STD_CPP11_FLAG - Success -- Found PythonInterp: /home/ubuntu/anaconda3/bin/python (found version ""3.6.6"") -- Cannot find llvm-lit. -- Please put llvm-lit in your PATH, set OPENMP_LLVM_LIT_EXECUTABLE to its full path, or point OPENMP_LLVM_TOOLS_DIR to its directory. CMake Warning at 3rdparty/openmp/cmake/OpenMPTesting.cmake:22 (message): The check targets will not be available! Call Stack (most recent call first): 3rdparty/openmp/cmake/OpenMPTesting.cmake:40 (find_standalone_test_dependencies) 3rdparty/openmp/CMakeLists.txt:49 (include) -- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG -- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG - Success -- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG -- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG - Success -- Performing Test LIBOMP_HAVE_X_CPP_FLAG -- Performing Test LIBOMP_HAVE_X_CPP_FLAG - Success -- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG -- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG -- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG -- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG -- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG -- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG -- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG -- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG -- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG -- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG -- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG -- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG -- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG -- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG -- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG - Success -- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG -- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG -- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG - Failed -- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG -- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG - Failed -- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG -- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG - Success -- Performing Test LIBOMP_HAVE_MSSE2_FLAG -- Performing Test LIBOMP_HAVE_MSSE2_FLAG - Success -- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG -- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG - Success -- Performing Test LIBOMP_HAVE_MMIC_FLAG -- Performing Test LIBOMP_HAVE_MMIC_FLAG - Failed -- Performing Test LIBOMP_HAVE_M32_FLAG -- Performing Test LIBOMP_HAVE_M32_FLAG - Failed -- Performing Test LIBOMP_HAVE_X_FLAG -- Performing Test LIBOMP_HAVE_X_FLAG - Success -- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG -- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG - Success -- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG -- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG - Success -- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG -- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG - Success -- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG -- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG - Success -- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG -- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG - Success -- Performing Test LIBOMP_HAVE_FINI_FLAG -- Performing Test LIBOMP_HAVE_FINI_FLAG - Success -- Found Perl: /usr/bin/perl (found version ""5.26.1"") -- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS -- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS - Success -- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS -- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS - Success -- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE -- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE - Success -- Looking for include files windows.h, psapi.h -- Looking for include files windows.h, psapi.h - not found -- Looking for EnumProcessModules in psapi -- Looking for EnumProcessModules in psapi - not found -- LIBOMP: Operating System -- Linux -- LIBOMP: Target Architecture -- x86_64 -- LIBOMP: Build Type -- Release -- LIBOMP: Library Kind -- SHARED -- LIBOMP: Library Type -- normal -- LIBOMP: Fortran Modules -- FALSE -- LIBOMP: Build -- 20140926 -- LIBOMP: Use Stats-gathering -- FALSE -- LIBOMP: Use Debugger-support -- FALSE -- LIBOMP: Use ITT notify -- TRUE -- LIBOMP: Use OMPT-support -- TRUE -- LIBOMP: Use OMPT-optional -- TRUE -- LIBOMP: Use Adaptive locks -- TRUE -- LIBOMP: Use quad precision -- TRUE -- LIBOMP: Use TSAN-support -- FALSE -- LIBOMP: Use Hwloc library -- FALSE -- Looking for sqrt in m -- Looking for sqrt in m - found -- Looking for __atomic_load_1 -- Looking for __atomic_load_1 - not found -- Looking for __atomic_load_1 in atomic -- Looking for __atomic_load_1 in atomic - found -- check-libomp does nothing. -- check-ompt does nothing. -- check-openmp does nothing. USE_LAPACK is ON -- Could NOT find Jemalloc (missing: JEMALLOC_LIBRARY JEMALLOC_INCLUDE_DIR) CMake Warning at 3rdparty/googletest/googletest/CMakeLists.txt:47 (project): VERSION keyword not followed by a value or was followed by a value that expanded to nothing. -- Found GTest: gtest -- Found CUDNN: /usr/local/cuda/lib64/libcudnn.so -- Looking for clock_gettime in rt -- Looking for clock_gettime in rt - found -- Looking for fopen64 -- Looking for fopen64 - not found -- Looking for C++ include cxxabi.h -- Looking for C++ include cxxabi.h - found -- Looking for nanosleep -- Looking for nanosleep - found -- Looking for backtrace -- Looking for backtrace - found -- backtrace facility detected in default set of libraries -- Found Backtrace: /usr/include -- Check if the system is big endian -- Searching 16 bit integer -- Looking for sys/types.h -- Looking for sys/types.h - found -- Looking for stdint.h -- Looking for stdint.h - found -- Looking for stddef.h -- Looking for stddef.h - found -- Check size of unsigned short -- Check size of unsigned short - done -- Using unsigned short -- Check if the system is big endian - little endian -- /home/ubuntu/incubator-mxnet/3rdparty/dmlc-core/cmake/build_config.h.in -> include/dmlc/build_config.h -- Found OpenMP_C: -fopenmp -- Found OpenMP_CXX: -fopenmp -- Automatic GPU detection failed. Building for common architectures. -- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;7.0;7.0+PTX;7.5;7.5+PTX -- CUDA: Using the following NVCC architecture flags -gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75 -- Found CUDAToolkit: /usr/local/cuda/include (found version ""10.0.130"") -- Could NOT find ZMQ (missing: ZMQ_LIBRARY ZMQ_INCLUDE_DIR) CMake Warning at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.13/Modules/FindProtobuf.cmake:495 (message): Protobuf compiler version 3.8.0 doesn't match library version 3.0.0 Call Stack (most recent call first): 3rdparty/ps-lite/cmake/ProtoBuf.cmake:4 (find_package) 3rdparty/ps-lite/CMakeLists.txt:22 (include) -- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version ""3.0.0"") -- Found PROTOBUF Compiler: /home/ubuntu/anaconda3/bin/protoc -- Found PythonInterp: /usr/bin/python2 (found suitable exact version ""2.7.15"") -- Cython modules for python2 will be built -- Found PythonInterp: /home/ubuntu/anaconda3/bin/python3 (found suitable exact version ""3.6.6"") -- Cython modules for python3 will be built -- Configuring done -- Generating done -- Build files have been written to: /home/ubuntu/incubator-mxnet/build"
"Looking at your output, you can see

```
 -- Could NOT find ZMQ (missing: ZMQ_LIBRARY ZMQ_INCLUDE_DIR)
```

You need to install ZMQ. This should have been a fatal error preventing build.

```
CMake Warning at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.13/Modules/FindProtobuf.cmake:495 (message):
Protobuf compiler version 3.8.0 doesn't match library version 3.0.0
Call Stack (most recent call first):
3rdparty/ps-lite/cmake/ProtoBuf.cmake:4 (find_package)
3rdparty/ps-lite/CMakeLists.txt:22 (include)

-- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version ""3.0.0"")
-- Found PROTOBUF Compiler: /home/ubuntu/anaconda3/bin/protoc
```


Your system is not setup correctly. You're mixing system protobuf and some conda protobuf.
Conda messes a lot with your system. You shouldn't use it if you want to compile MXNet.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575132131,leezu,2020-01-16 12:35:04,17299,[17732],Build bug,0,"Looking at your output, you can see ``[code]`[code]`[code]`` Your system is not setup correctly. You're mixing system protobuf and some conda protobuf. Conda messes a lot with your system. You shouldn't use it if you want to compile MXNet."
"> Looking at your output, you can see
> 
> ```
>  -- Could NOT find ZMQ (missing: ZMQ_LIBRARY ZMQ_INCLUDE_DIR)
> ```
> 
> You need to install ZMQ. This should have been a fatal error preventing build.
> 
> ```
> CMake Warning at /usr/local/lib/python3.6/dist-packages/cmake/data/share/cmake-3.13/Modules/FindProtobuf.cmake:495 (message):
> Protobuf compiler version 3.8.0 doesn't match library version 3.0.0
> Call Stack (most recent call first):
> 3rdparty/ps-lite/cmake/ProtoBuf.cmake:4 (find_package)
> 3rdparty/ps-lite/CMakeLists.txt:22 (include)
> 
> -- Found Protobuf: /usr/lib/x86_64-linux-gnu/libprotobuf.so;-lpthread (found version ""3.0.0"")
> -- Found PROTOBUF Compiler: /home/ubuntu/anaconda3/bin/protoc
> ```
> 
> Your system is not setup correctly. You're mixing system protobuf and some conda protobuf.
> Conda messes a lot with your system. You shouldn't use it if you want to compile MXNet.

Thank you a lot. Do you have any suggestions to disable Conda protobuf without uninstalling? (I didn't use conda meaningly.)",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575139822,ChaokunChang,2020-01-16 12:58:36,17299,[17732],Build bug,0,"> Looking at your output, you can see > > ``[code]`[code]`[code]`` > > Your system is not setup correctly. You're mixing system protobuf and some conda protobuf. > Conda messes a lot with your system. You shouldn't use it if you want to compile MXNet. Thank you a lot. Do you have any suggestions to disable Conda protobuf without uninstalling? (I didn't use conda meaningly.)"
"> Thank you a lot. Do you have any suggestions to disable Conda protobuf without uninstalling? (I didn't use conda meaningly.)

I suppose you activated conda after logging in to your machine. Something like `source activate mxnet_p36`. If you did so, try not doing so.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575143725,leezu,2020-01-16 13:10:05,17299,[17732],Build bug,0,"> Thank you a lot. Do you have any suggestions to disable Conda protobuf without uninstalling? (I didn't use conda meaningly.) I suppose you activated conda after logging in to your machine. Something like [code]. If you did so, try not doing so."
"> so

I have never used `source activate mxnet_p3` command before (In fact I even didn't know what is it ...). I thought I didn't use conda because there is no something like `(base)` before `ubuntu@ip-xx-xx-xx-xx:~$ `",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575147222,ChaokunChang,2020-01-16 13:19:05,17299,[17732],Build bug,0,> so I have never used [code] command before (In fact I even didn't know what is it ...). I thought I didn't use conda because there is no something like [code] before [code]
"It means you are using the `(base)` conda environment.

Maybe it's activated by default on the Deep Learning AMI. You can check the `.bashrc` file and delete any lines like `source activate`.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575184498,leezu,2020-01-16 14:47:04,17299,[17732],Build bug,0,It means you are using the [code] conda environment. Maybe it's activated by default on the Deep Learning AMI. You can check the [code] file and delete any lines like [code].
"close in mistake, reopen",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-575419361,ChaokunChang,2020-01-17 01:07:39,17299,[17732],Build bug,0,"close in mistake, reopen"
"@ChaokunChang Sorry, I misread your comment at https://github.com/apache/incubator-mxnet/issues/17299#issuecomment-575147222

I understand you're not using conda. Unfortunately the environment you're using (DLAMI) activates some conda features by default (for example you're using `/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip` and `/home/ubuntu/anaconda3/bin/protoc`). This is a problem with DLAMI.

I recommend you use Base-DLAMI instead. We'll report this problem to DLAMI team.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-577933355,leezu,2020-01-23 23:56:45,17299,[17732],Build bug,0,"@ChaokunChang Sorry, I misread your comment at [url]#issuecomment-575147222 I understand you're not using conda. Unfortunately the environment you're using (DLAMI) activates some conda features by default (for example you're using [code] and [code]). This is a problem with DLAMI. I recommend you use Base-DLAMI instead. We'll report this problem to DLAMI team."
@leezu I encountered the same problem trying to install mxnet with USE_DIST_KVSTORE ON in the latest AWS DLAMI. I think we need to solve this problem for a better user experience. ,IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-579540070,apeforest,2020-01-29 00:36:48,17299,[17732],Build bug,0,@leezu I encountered the same problem trying to install mxnet with USE_DIST_KVSTORE ON in the latest AWS DLAMI. I think we need to solve this problem for a better user experience.
"@apeforest may be able to add a workaround for DLAMI at https://github.com/dmlc/ps-lite/blob/master/cmake/ProtoBuf.cmake

I think the problem here is that DLAMI has a strange setup including `/home/ubuntu/anaconda3/bin/protoc` by default on PATH. What do you think?",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-579541363,leezu,2020-01-29 00:42:34,17299,[17732],Build bug,0,@apeforest may be able to add a workaround for DLAMI at [url] I think the problem here is that DLAMI has a strange setup including [code] by default on PATH. What do you think?
"@leezu Just FYI, if I use make with the USE_DIST_KVSTORE=ON in config.mk, then the build is fine.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-579941781,apeforest,2020-01-29 20:21:30,17299,[17732],Build bug,0,"@leezu Just FYI, if I use make with the USE_DIST_KVSTORE=ON in config.mk, then the build is fine."
"@apeforest thank you. cmake build fails due to DLAMI violating some assumption taken when writing https://github.com/dmlc/ps-lite/blob/master/cmake/ProtoBuf.cmake in 2017 (cc @yajiedesign).
I'll look into finding a workaround it in a few days",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-579972416,leezu,2020-01-29 21:40:01,17299,[17732],Build bug,0,@apeforest thank you. cmake build fails due to DLAMI violating some assumption taken when writing [url] in 2017 (cc @yajiedesign). I'll look into finding a workaround it in a few days
"I verified this error is due to DLAMI shipping protoc version 3.8.0 but headers from version 3.0.0.

`google/protobuf/port_def.inc` is included in headers generated by protoc version >= 3.7.0, but of course if your system comes only with headers from 3.0.0 compilation will fail.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-592740807,leezu,2020-02-28 21:38:27,17299,[17732],Build bug,0,"I verified this error is due to DLAMI shipping protoc version 3.8.0 but headers from version 3.0.0. [code] is included in headers generated by protoc version >= 3.7.0, but of course if your system comes only with headers from 3.0.0 compilation will fail."
"You can workaround the DLAMI bugs by running with

`cmake -DProtobuf_PROTOC_EXECUTABLE=/usr/bin/protoc [...]`",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-592747607,leezu,2020-02-28 21:57:24,17299,[17732],Build bug,0,You can workaround the DLAMI bugs by running with [code]
Reported an issue upstream requesting if cmake may have any strategy to handle broken systems such as DLAMI https://gitlab.kitware.com/cmake/cmake/issues/20403,IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-592751090,leezu,2020-02-28 22:07:27,17299,[17732],Build bug,0,Reported an issue upstream requesting if cmake may have any strategy to handle broken systems such as DLAMI [url]
"With respect to the Makefile build @apeforest, the reason is that Makefile based build doesn't use system protobuf, but downloads and compiles `protobuf-cpp-3.5.1.tar.gz` as part of the build.",IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-592754645,leezu,2020-02-28 22:18:11,17299,[17732],Build bug,0,"With respect to the Makefile build @apeforest, the reason is that Makefile based build doesn't use system protobuf, but downloads and compiles [code] as part of the build."
https://github.com/dmlc/ps-lite/pull/170,IssueComment,https://github.com/apache/mxnet/issues/17299#issuecomment-592809157,leezu,2020-02-29 01:49:19,17299,[17732],Build bug,0,[url]
"It seg-faults for other inputs
```
>>> a=mx.nd.random.randn(2,1024)
>>> mx.nd.unravel_index(a,shape=(1024,1024))

Segmentation fault: 11
```",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-579950885,ChaiBapchya,2020-01-29 20:44:51,17472,[17748],Memory bug,1,It seg-faults for other inputs ``[code]``
"I can confirm it here too (mxnet 1.5.1 on Linux)

```
free(): invalid pointer
Aborted (core dumped)
```


",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-593006992,oleg-trott,2020-02-29 23:49:35,17472,[17748],Memory bug,1,I can confirm it here too (mxnet 1.5.1 on Linux) ``[code]``
"I can reproduce the bug.
I check the code of `unravel_index`, which only support 1D input.",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-593477646,wkcn,2020-03-02 16:05:07,17472,[17748],Memory bug,1,"I can reproduce the bug. I check the code of [code], which only support 1D input."
"I have fixed the bug, and I will submit the PR.",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-593733513,wkcn,2020-03-03 02:25:58,17472,[17748],Memory bug,1,"I have fixed the bug, and I will submit the PR."
@mxnet-bot test if you comment on an issue,IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-664049306,ChaiBapchya,2020-07-26 22:39:44,17472,[17748],Memory bug,1,@mxnet-bot test if you comment on an issue
"Undefined action detected. 
Permissible actions are : run ci [all], run ci [job1, job2] 
Example : @mxnet-bot run ci [all] 
Example : @mxnet-bot run ci [centos-cpu, clang]",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-664049309,mxnet-bot,2020-07-26 22:39:45,17472,[17748],Memory bug,1,"Undefined action detected. Permissible actions are : run ci [all], run ci [job1, job2] Example : @mxnet-bot run ci [all] Example : @mxnet-bot run ci [centos-cpu, clang]"
@mxnet-bot test if it detects that this is an issue and not a PR,IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-664052814,ChaiBapchya,2020-07-26 23:14:14,17472,[17748],Memory bug,1,@mxnet-bot test if it detects that this is an issue and not a PR
"Hey @ChaiBapchya 
 @mxnet-bot can only be invoked on a PR.",IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-664052823,mxnet-bot,2020-07-26 23:14:18,17472,[17748],Memory bug,1,Hey @ChaiBapchya @mxnet-bot can only be invoked on a PR.
Please ignore previous comments. I just used them for testing out a bot functionality.,IssueComment,https://github.com/apache/mxnet/issues/17472#issuecomment-664053550,ChaiBapchya,2020-07-26 23:21:02,17472,[17748],Memory bug,1,Please ignore previous comments. I just used them for testing out a bot functionality.
"that is because `git clone --recursive` pull openmp, and the cmake script don't take care if use system openMP or not

https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/CMakeLists.txt#L390-L430

unfortunately, openmp dont have pkg-config file, or cmake files for interact with cmake

seems need a own cmake module for search it",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589565378,sl1pkn07,2020-02-21 09:10:37,17641,[17751],Build bug,1,"that is because [code] pull openmp, and the cmake script don't take care if use system openMP or not [url]#L390-L430 unfortunately, openmp dont have pkg-config file, or cmake files for interact with cmake seems need a own cmake module for search it"
Same issue: https://github.com/apache/incubator-mxnet/issues/17366,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589568416,TaoLv,2020-02-21 09:18:34,17641,[17751],Build bug,1,Same issue: [url]
"this case also same effects with intel-dnnl.  

some distros already provide a package intel-dnnl, but mxnet force download the sources again",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589578659,sl1pkn07,2020-02-21 09:45:39,17641,[17751],Build bug,1,"this case also same effects with intel-dnnl. some distros already provide a package intel-dnnl, but mxnet force download the sources again"
"@cjolivier01 you previously vetoed changing the omp configuration in cmake build, due to a race condition that had not been fixed. As that has been fixed, are you OK with proceeding to prefer system OMP for the CMake build by default? Or what is your recommendation?

Static build should still statically build omp.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589749070,leezu,2020-02-21 17:18:03,17641,[17751],Build bug,1,"@cjolivier01 you previously vetoed changing the omp configuration in cmake build, due to a race condition that had not been fixed. As that has been fixed, are you OK with proceeding to prefer system OMP for the CMake build by default? Or what is your recommendation? Static build should still statically build omp."
"@sl1pkn07 given the rapid development of intel-dnnl, MXNet expects a fixed version of intel-dnnl. It's quite unlikely that the system provides that particular version, but patches to improve detection are welcome. Do you want to contribute a PR? But let's track this in a separate issue.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589750326,leezu,2020-02-21 17:21:25,17641,[17751],Build bug,1,"@sl1pkn07 given the rapid development of intel-dnnl, MXNet expects a fixed version of intel-dnnl. It's quite unlikely that the system provides that particular version, but patches to improve detection are welcome. Do you want to contribute a PR? But let's track this in a separate issue."
"1) What is pulling in libiomp5.so ?
2) Since when is libomp being linked in statically?  I am not aware of this ever being the case.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589809022,cjolivier01,2020-02-21 19:51:32,17641,[17751],Build bug,1,1) What is pulling in libiomp5.so ? 2) Since when is libomp being linked in statically? I am not aware of this ever being the case.
"btw, cmake files have min cmake at 3.13, but default 18.04 cmake install is cmake 3.10.  Does anyone know what the deal is with 3.13?  Ubuntu 18.04 is a pretty widely-used release...
I changed back to 3.10 and it seems to build ok.
",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589812034,cjolivier01,2020-02-21 19:59:26,17641,[17751],Build bug,1,"btw, cmake files have min cmake at 3.13, but default 18.04 cmake install is cmake 3.10. Does anyone know what the deal is with 3.13? Ubuntu 18.04 is a pretty widely-used release... I changed back to 3.10 and it seems to build ok."
"> @cjolivier01 you previously vetoed changing the omp configuration in cmake build, due to a race condition that had not been fixed. As that has been fixed, are you OK with proceeding to prefer system OMP for the CMake build by default? Or what is your recommendation?
> 
> Static build should still statically build omp.

Not actually.  Due to no legitimate reason to remove it.  ",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589817764,cjolivier01,2020-02-21 20:15:26,17641,[17751],Build bug,1,"> @cjolivier01 you previously vetoed changing the omp configuration in cmake build, due to a race condition that had not been fixed. As that has been fixed, are you OK with proceeding to prefer system OMP for the CMake build by default? Or what is your recommendation? > > Static build should still statically build omp. Not actually. Due to no legitimate reason to remove it."
"> What is pulling in libiomp5.so ?

MKL

> btw, cmake files have min cmake at 3.13, but default 18.04 cmake install is cmake 3.10. Does anyone know what the deal is with 3.13? Ubuntu 18.04 is a pretty widely-used release...
> I changed back to 3.10 and it seems to build ok.

Just `pip install cmake` as per the doc https://mxnet.apache.org/get_started/ubuntu_setup. It'd be harder to explain when users require 3.13 and when 3.X, or 3.Y, than to uniformly require a recent version. There are various bugs fixed in 3.13 that affect MXNet use-cases (eg cuda, https://cmake.org/cmake/help/latest/policy/CMP0077.html for llvm openmp subproject)

> Not actually. Due to no legitimate reason to remove it.

Speed up developer build. No need to build llvm openmp if system openmp is present.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589827331,leezu,2020-02-21 20:43:09,17641,[17751],Build bug,1,"> What is pulling in libiomp5.so ? MKL > btw, cmake files have min cmake at 3.13, but default 18.04 cmake install is cmake 3.10. Does anyone know what the deal is with 3.13? Ubuntu 18.04 is a pretty widely-used release... > I changed back to 3.10 and it seems to build ok. Just [code] as per the doc [url] It'd be harder to explain when users require 3.13 and when 3.X, or 3.Y, than to uniformly require a recent version. There are various bugs fixed in 3.13 that affect MXNet use-cases (eg cuda, [url] for llvm openmp subproject) > Not actually. Due to no legitimate reason to remove it. Speed up developer build. No need to build llvm openmp if system openmp is present."
"openmp is like a 4-5-second build.  

On my desktop machine it's < 3:
real    0m2.940s
user    0m42.446s
sys     0m5.442s



",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589832039,cjolivier01,2020-02-21 20:57:48,17641,[17751],Build bug,1,openmp is like a 4-5-second build. On my desktop machine it's < 3: real 0m2.940s user 0m42.446s sys 0m5.442s
"I installed mkl, but it does not appear to pick it up.  is there a way to force it?",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589832383,cjolivier01,2020-02-21 20:58:39,17641,[17751],Build bug,1,"I installed mkl, but it does not appear to pick it up. is there a way to force it?"
"Actually, i don;t see this behavior when it does pull in mkl/pulling in the other omp (this is Ubuntu 18.04):
```
[chriso@chriso-ripper:~/src/mxnet/build (master)]ldd libmxnet.so 
        linux-vdso.so.1 (0x00007ffcbdf3b000)
        libmkl_rt.so => /opt/intel/mkl/lib/intel64/libmkl_rt.so (0x00007fb399dd8000)
        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fb399bd0000)
        libomp.so => /home/chriso/src/mxnet/build/3rdparty/openmp/runtime/src/libomp.so (0x00007fb3998ea000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fb3996e6000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fb3994c7000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fb39913e000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fb398da0000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fb398b88000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fb398797000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fb3a078c000)
```
I don;t show libmkl_rt.so pulling in libiomp5:
```
[chriso@chriso-ripper:~/src/mxnet/build (master)]ldd /opt/intel/mkl/lib/intel64/libmkl_rt.so
        linux-vdso.so.1 (0x00007ffd6c5cc000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc85058d000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc85019c000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fc850e71000)
```",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589836549,cjolivier01,2020-02-21 21:11:13,17641,[17751],Build bug,1,"Actually, i don;t see this behavior when it does pull in mkl/pulling in the other omp (this is Ubuntu 18.04): ``[code]`[code]`[code]``"
"I think `libmkl_rt` may `dlopen` `libiomp` as per https://github.com/intel/mkl-dnn/issues/230#issuecomment-451082066, but I haven't looked into this further yet",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589844040,leezu,2020-02-21 21:32:22,17641,[17751],Build bug,1,"I think [code] may [code] [code] as per [url]#issuecomment-451082066, but I haven't looked into this further yet"
"Linking in any version of omp statically would probably be a bad idea, since startup order would be important.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589848880,cjolivier01,2020-02-21 21:41:07,17641,[17751],Build bug,1,"Linking in any version of omp statically would probably be a bad idea, since startup order would be important."
"Clearly it does not:
[chriso@chriso-ripper:~/src/mxnet/build (master)]ldd /opt/intel/mkl/lib/intel64/libmkl_rt.so
        linux-vdso.so.1 (0x00007ffd6c5cc000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc85058d000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc85019c000)
        /lib64/ld-linux-x86-64.so.2 (0x00007fc850e71000)",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589849228,cjolivier01,2020-02-21 21:42:06,17641,[17751],Build bug,1,Clearly it does not: [chriso@chriso-ripper:~/src/mxnet/build (master)]ldd /opt/intel/mkl/lib/intel64/libmkl_rt.so linux-vdso.so.1 (0x00007ffd6c5cc000) libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc85058d000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc85019c000) /lib64/ld-linux-x86-64.so.2 (0x00007fc850e71000)
"Yes, that's why `dlopen`.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589851161,leezu,2020-02-21 21:47:51,17641,[17751],Build bug,1,"Yes, that's why [code]."
and with `readelf -a <lib> | grep NEEDED` ?,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589852431,sl1pkn07,2020-02-21 21:51:52,17641,[17751],Build bug,1,and with [code] ?
"can you supply a  script to reproduc this error? I am not able to reproduce.

",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589853130,cjolivier01,2020-02-21 21:53:59,17641,[17751],Build bug,1,can you supply a script to reproduc this error? I am not able to reproduce.
i'm using system openmp and no mkl-dnnl. sorry @icemelon9?,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589855137,sl1pkn07,2020-02-21 22:00:15,17641,[17751],Build bug,1,i'm using system openmp and no mkl-dnnl. sorry @icemelon9?
@sl1pkn07 please open a separate issue for your problem. This issue is about MKL.,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589858953,leezu,2020-02-21 22:12:09,17641,[17751],Build bug,1,@sl1pkn07 please open a separate issue for your problem. This issue is about MKL.
@icemelon9 please provide the a reproducer to trigger the error message.,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-589859111,leezu,2020-02-21 22:12:40,17641,[17751],Build bug,1,@icemelon9 please provide the a reproducer to trigger the error message.
"Sorry about the late response. Here's the script to reproduce the error message.
```python
import numpy as np
import mxnet as mx

a = mx.nd.array(np.random.uniform(size=(1024, 128)).astype('float32'))
b = mx.nd.array(np.random.uniform(size=(128, 1024)).astype('float32'))
c = mx.nd.dot(a, b)
c.wait_to_read()
```

The following shows shared library used by libmxnet on my machine. 
```
mxnet git:(master) ldd build/libmxnet.so
        linux-vdso.so.1 (0x00007ffd8b467000)
        libmkl_rt.so => /opt/intel/mkl/lib/intel64/libmkl_rt.so (0x00007f1abac81000)
        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f1abaa79000)
        libopencv_imgcodecs.so.3.2 => /usr/lib/x86_64-linux-gnu/libopencv_imgcodecs.so.3.2 (0x00007f1aba840000)
        libopencv_imgproc.so.3.2 => /usr/lib/x86_64-linux-gnu/libopencv_imgproc.so.3.2 (0x00007f1aba2ef000)
        libopencv_core.so.3.2 => /usr/lib/x86_64-linux-gnu/libopencv_core.so.3.2 (0x00007f1ab9eb4000)
        libomp.so => /home/ubuntu/repo/mxnet/build/3rdparty/openmp/runtime/src/libomp.so (0x00007f1ab9bce000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f1ab99ca000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f1ab97ab000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f1ab9422000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f1ab9084000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f1ab8e6c000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1ab8a7b000)
        ...
```
```
mxnet git:(master) readelf -a build/libmxnet.so| grep NEEDED
 0x0000000000000001 (NEEDED)             Shared library: [libmkl_rt.so]
 0x0000000000000001 (NEEDED)             Shared library: [librt.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgcodecs.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgproc.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_core.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libomp.so]
 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
```",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590174397,icemelon,2020-02-24 05:41:08,17641,[17751],Build bug,1,Sorry about the late response. Here's the script to reproduce the error message. ``[code]`[code]`[code]`[code]`[code]``
"```
 0x0000000000000001 (NEEDED)             Shared library: [libmkl_rt.so]
 0x0000000000000001 (NEEDED)             Shared library: [librt.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgcodecs.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_imgproc.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libopencv_core.so.3.2]
 0x0000000000000001 (NEEDED)             Shared library: [libomp.so]
 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]
 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]
 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]
```
",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590449594,cjolivier01,2020-02-24 17:24:33,17641,[17751],Build bug,1,``[code]``
"```python
(pytorch) [chriso@chriso-ripper:~/src/mxnet (master)]python
Python 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> import mxnet as mx
>>> 
>>> a = mx.nd.array(np.random.uniform(size=(1024, 128)).astype('float32'))
>>> b = mx.nd.array(np.random.uniform(size=(128, 1024)).astype('float32'))
>>> c = mx.nd.dot(a, b)
>>> c.wait_to_read()
>>> exit()
```
Stll can't reproduce.
Can you send entire cmake config log?",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590450010,cjolivier01,2020-02-24 17:25:22,17641,[17751],Build bug,1,``[code]`` Stll can't reproduce. Can you send entire cmake config log?
"CI can also reproduce this issue. I switched CI to testing CMake builds instead of Makefile build in https://github.com/apache/incubator-mxnet/pull/17645 and the Python MKLDNN + MKL Pipeline fails with this issue: [Log of test failure](http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-cpu/detail/PR-17645/30/pipeline/297) and [Raw log of test failure](http://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/mxnet-validation/pipelines/unix-cpu/branches/PR-17645/runs/30/nodes/297/steps/775/log/?start=0)

and [Raw log of build](http://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/mxnet-validation/pipelines/unix-cpu/branches/PR-17645/runs/30/nodes/48/steps/169/log/?start=0)

@cjolivier01 the build log contains the output of cmake configuration.


That pipeline relies on the following build

```
build_ubuntu_cpu_mkldnn_mkl() {
    set -ex
    cd /work/build
    cmake \
        -DCMAKE_BUILD_TYPE=""RelWithDebInfo"" \
        -DUSE_MKL_IF_AVAILABLE=ON \
        -DBLAS=""MKL"" \
        -DUSE_TVM_OP=ON \
        -DUSE_CUDA=OFF \
        -DUSE_CPP_PACKAGE=ON \
        -G Ninja /work/mxnet
    ninja
}
```",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590465449,leezu,2020-02-24 17:55:49,17641,[17751],Build bug,1,CI can also reproduce this issue. I switched CI to testing CMake builds instead of Makefile build in [url] and the Python MKLDNN + MKL Pipeline fails with this issue: [Log of test failure]([url] and [Raw log of test failure]([url] and [Raw log of build]([url] @cjolivier01 the build log contains the output of cmake configuration. That pipeline relies on the following build ``[code]``
"Here is the cmake log.
```
build git:(master) ✗ cmake -DUSE_CUDA=0 -DUSE_CUDNN=0 -DUSE_MKLDNN=1 -DCMAKE_BUILD_TYPE=Release -GNinja ..
-- The C compiler identification is GNU 7.4.0
-- The CXX compiler identification is GNU 7.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- CMAKE_CROSSCOMPILING FALSE
-- CMAKE_HOST_SYSTEM_PROCESSOR x86_64
-- CMAKE_SYSTEM_PROCESSOR x86_64
-- CMAKE_SYSTEM_NAME Linux
-- CMake version '3.16.4' using generator 'Ninja'
-- Performing Test SUPPORT_CXX11
-- Performing Test SUPPORT_CXX11 - Success
-- Performing Test SUPPORT_CXX0X
-- Performing Test SUPPORT_CXX0X - Success
-- MKL-DNN compat: set DNNL_BUILD_EXAMPLES to MKLDNN_BUILD_EXAMPLES with value `OFF`
-- MKL-DNN compat: set DNNL_BUILD_TESTS to MKLDNN_BUILD_TESTS with value `OFF`
-- MKL-DNN compat: set DNNL_ENABLE_JIT_PROFILING to MKLDNN_ENABLE_JIT_PROFILING with value `OFF`
-- MKL-DNN compat: set DNNL_LIBRARY_TYPE to MKLDNN_LIBRARY_TYPE with value `STATIC`
-- MKL-DNN compat: set DNNL_ARCH_OPT_FLAGS to MKLDNN_ARCH_OPT_FLAGS with value ``
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found OpenMP_C: -fopenmp (found version ""4.5"")
-- Found OpenMP_CXX: -fopenmp (found version ""4.5"")
-- Found OpenMP: TRUE (found version ""4.5"")
-- GPU support is disabled
-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE)
-- Found Git: /usr/bin/git (found version ""2.17.1"")
-- Intel(R) VTune(TM) Amplifier JIT profiling disabled
-- Found MKL: /opt/intel/mkl/include
-- Found MKL (include: /opt/intel/mkl/include, lib: /opt/intel/mkl/lib/intel64/libmkl_rt.so
-- Found OpenCV: /usr (found version ""3.2.0"") found components: core highgui imgproc imgcodecs
-- OpenCV 3.2.0 found (/usr/share/OpenCV)
--  OpenCV_LIBS=opencv_core;opencv_highgui;opencv_imgproc;opencv_imgcodecs
-- Performing Test OPENMP_HAVE_WERROR_FLAG
-- Performing Test OPENMP_HAVE_WERROR_FLAG - Success
-- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG
-- Performing Test OPENMP_HAVE_STD_GNUPP11_FLAG - Success
-- Performing Test OPENMP_HAVE_STD_CPP11_FLAG
-- Performing Test OPENMP_HAVE_STD_CPP11_FLAG - Success
-- Found PythonInterp: /home/ubuntu/anaconda3/envs/tvm/bin/python (found version ""3.6.9"")
-- Cannot find llvm-lit.
-- Please put llvm-lit in your PATH, set OPENMP_LLVM_LIT_EXECUTABLE to its full path, or point OPENMP_LLVM_TOOLS_DIR to its directory.
CMake Warning at 3rdparty/openmp/cmake/OpenMPTesting.cmake:22 (message):
  The check targets will not be available!
Call Stack (most recent call first):
  3rdparty/openmp/cmake/OpenMPTesting.cmake:40 (find_standalone_test_dependencies)
  3rdparty/openmp/CMakeLists.txt:49 (include)


-- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG
-- Performing Test LIBOMP_HAVE_FNO_EXCEPTIONS_FLAG - Success
-- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG
-- Performing Test LIBOMP_HAVE_FNO_RTTI_FLAG - Success
-- Performing Test LIBOMP_HAVE_X_CPP_FLAG
-- Performing Test LIBOMP_HAVE_X_CPP_FLAG - Success
-- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG
-- Performing Test LIBOMP_HAVE_WCAST_QUAL_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_FUNCTION_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_LOCAL_TYPEDEF_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VALUE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNUSED_VARIABLE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SWITCH_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_COVERED_SWITCH_DEFAULT_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG
-- Performing Test LIBOMP_HAVE_WNO_DEPRECATED_REGISTER_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SIGN_COMPARE_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_GNU_ANONYMOUS_STRUCT_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG
-- Performing Test LIBOMP_HAVE_WNO_UNKNOWN_PRAGMAS_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG
-- Performing Test LIBOMP_HAVE_WNO_MISSING_FIELD_INITIALIZERS_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG
-- Performing Test LIBOMP_HAVE_WNO_MISSING_BRACES_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG
-- Performing Test LIBOMP_HAVE_WNO_COMMENT_FLAG - Success
-- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG
-- Performing Test LIBOMP_HAVE_WNO_SELF_ASSIGN_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG
-- Performing Test LIBOMP_HAVE_WNO_VLA_EXTENSION_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG
-- Performing Test LIBOMP_HAVE_WNO_FORMAT_PEDANTIC_FLAG - Failed
-- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG
-- Performing Test LIBOMP_HAVE_WSTRINGOP_OVERFLOW_FLAG - Success
-- Performing Test LIBOMP_HAVE_MSSE2_FLAG
-- Performing Test LIBOMP_HAVE_MSSE2_FLAG - Success
-- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG
-- Performing Test LIBOMP_HAVE_FTLS_MODEL_FLAG - Success
-- Performing Test LIBOMP_HAVE_MMIC_FLAG
-- Performing Test LIBOMP_HAVE_MMIC_FLAG - Failed
-- Performing Test LIBOMP_HAVE_M32_FLAG
-- Performing Test LIBOMP_HAVE_M32_FLAG - Failed
-- Performing Test LIBOMP_HAVE_X_FLAG
-- Performing Test LIBOMP_HAVE_X_FLAG - Success
-- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG
-- Performing Test LIBOMP_HAVE_WARN_SHARED_TEXTREL_FLAG - Success
-- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG
-- Performing Test LIBOMP_HAVE_AS_NEEDED_FLAG - Success
-- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG
-- Performing Test LIBOMP_HAVE_VERSION_SCRIPT_FLAG - Success
-- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG
-- Performing Test LIBOMP_HAVE_STATIC_LIBGCC_FLAG - Success
-- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG
-- Performing Test LIBOMP_HAVE_Z_NOEXECSTACK_FLAG - Success
-- Performing Test LIBOMP_HAVE_FINI_FLAG
-- Performing Test LIBOMP_HAVE_FINI_FLAG - Success
-- Found Perl: /usr/bin/perl (found version ""5.26.1"")
-- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS
-- Performing Test LIBOMP_HAVE_VERSION_SYMBOLS - Success
-- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS
-- Performing Test LIBOMP_HAVE___BUILTIN_FRAME_ADDRESS - Success
-- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE
-- Performing Test LIBOMP_HAVE_WEAK_ATTRIBUTE - Success
-- Looking for include files windows.h, psapi.h
-- Looking for include files windows.h, psapi.h - not found
-- Looking for EnumProcessModules in psapi
-- Looking for EnumProcessModules in psapi - not found
-- LIBOMP: Operating System     -- Linux
-- LIBOMP: Target Architecture  -- x86_64
-- LIBOMP: Build Type           -- Release
-- LIBOMP: Library Kind         -- SHARED
-- LIBOMP: Library Type         -- normal
-- LIBOMP: Fortran Modules      -- FALSE
-- LIBOMP: Build                -- 20140926
-- LIBOMP: Use Stats-gathering  -- FALSE
-- LIBOMP: Use Debugger-support -- FALSE
-- LIBOMP: Use ITT notify       -- TRUE
-- LIBOMP: Use OMPT-support     -- TRUE
-- LIBOMP: Use OMPT-optional  -- TRUE
-- LIBOMP: Use Adaptive locks   -- TRUE
-- LIBOMP: Use quad precision   -- TRUE
-- LIBOMP: Use TSAN-support     -- FALSE
-- LIBOMP: Use Hwloc library    -- FALSE
-- Looking for sqrt in m
-- Looking for sqrt in m - found
-- Looking for __atomic_load_1
-- Looking for __atomic_load_1 - not found
-- Looking for __atomic_load_1 in atomic
-- Looking for __atomic_load_1 in atomic - found
-- check-libomp does nothing.
-- check-ompt does nothing.
-- check-openmp does nothing.
USE_LAPACK is ON
CMake Warning at 3rdparty/googletest/googletest/CMakeLists.txt:47 (project):
  VERSION keyword not followed by a value or was followed by a value that
  expanded to nothing.


-- Found GTest: gtest
-- Looking for clock_gettime in rt
-- Looking for clock_gettime in rt - found
-- Looking for fopen64
-- Looking for fopen64 - not found
-- Looking for C++ include cxxabi.h
-- Looking for C++ include cxxabi.h - found
-- Looking for nanosleep
-- Looking for nanosleep - found
-- Looking for backtrace
-- Looking for backtrace - found
-- backtrace facility detected in default set of libraries
-- Found Backtrace: /usr/include
-- Check if the system is big endian
-- Searching 16 bit integer
-- Looking for sys/types.h
-- Looking for sys/types.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for stddef.h
-- Looking for stddef.h - found
-- Check size of unsigned short
-- Check size of unsigned short - done
-- Using unsigned short
-- Check if the system is big endian - little endian
-- /home/ubuntu/repo/mxnet/3rdparty/dmlc-core/cmake/build_config.h.in -> include/dmlc/build_config.h
-- Performing Test SUPPORT_MSSE2
-- Performing Test SUPPORT_MSSE2 - Success
-- Found OpenMP_C: -fopenmp
-- Found OpenMP_CXX: -fopenmp
-- Found OpenMP: TRUE
-- Performing Test SUPPORT_MSSE3
-- Performing Test SUPPORT_MSSE3 - Success
-- Determining F16C support
-- Performing Test COMPILER_SUPPORT_MF16C
-- Performing Test COMPILER_SUPPORT_MF16C - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ubuntu/repo/mxnet/build
```",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590467401,icemelon,2020-02-24 18:00:23,17641,[17751],Build bug,1,Here is the cmake log. ``[code]OFF[code]OFF[code]OFF[code]STATIC[code][code]``
This is with latest 2020 version of mkl?,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590472526,cjolivier01,2020-02-24 18:12:29,17641,[17751],Build bug,1,This is with latest 2020 version of mkl?
"Yes, I installed Intel MKL 2020.0-166",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590473106,icemelon,2020-02-24 18:13:46,17641,[17751],Build bug,1,"Yes, I installed Intel MKL 2020.0-166"
"This behavior seems suspicious because the claim would also suggest that the problem would exist for all similar build cases (ie my machine now) as well as for clang, which would pull in libomp by default, and not libiomp5.",IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590476688,cjolivier01,2020-02-24 18:21:47,17641,[17751],Build bug,1,"This behavior seems suspicious because the claim would also suggest that the problem would exist for all similar build cases (ie my machine now) as well as for clang, which would pull in libomp by default, and not libiomp5."
does it occur with opencv turned off?,IssueComment,https://github.com/apache/mxnet/issues/17641#issuecomment-590486220,cjolivier01,2020-02-24 18:42:54,17641,[17751],Build bug,1,does it occur with opencv turned off?
Imperative version works fine. Probably something wrong with the `_Symbol.__getitem__`.,IssueComment,https://github.com/apache/mxnet/issues/17766#issuecomment-594959894,reminisce,2020-03-05 00:21:59,17766,[17770],Data bug,0,Imperative version works fine. Probably something wrong with the [code].
"Yes, I'm about to fix it.",IssueComment,https://github.com/apache/mxnet/issues/17766#issuecomment-594960137,sxjscience,2020-03-05 00:22:31,17766,[17770],Data bug,0,"Yes, I'm about to fix it."
"Aha! I think, I found it! While the `mshadow` implementation was fixed in PR #6834, the [`fused_op` implementation](https://github.com/apache/incubator-mxnet/blob/ea2dabaae5e1453572c9105f77322977bd14c108/src/operator/fusion/fused_op-inl.h#L553), that was introduced in PR #15167 uses the plain `ln(1 + exp(x))` formula.",IssueComment,https://github.com/apache/mxnet/issues/17844#issuecomment-599539183,RuRo,2020-03-16 13:34:43,17844,[17849],Processor bug,1,"Aha! I think, I found it! While the [code] implementation was fixed in PR #6834, the [[code] implementation]([url]#L553), that was introduced in PR #15167 uses the plain [code] formula."
"I think we need to specify the Ruby version 2.6.3 in the Gemfile, and regenerate the Gemfile.lock.
https://devcenter.heroku.com/articles/ruby-versions
I don't know what is requiring 2.6.3. But setting it as that should keep us from getting 2.6.5 instead and getting past this error.",IssueComment,https://github.com/apache/mxnet/issues/17918#issuecomment-604802528,aaronmarkham,2020-03-27 03:58:59,17918,[17927],Build bug,0,"I think we need to specify the Ruby version 2.6.3 in the Gemfile, and regenerate the Gemfile.lock. [url] I don't know what is requiring 2.6.3. But setting it as that should keep us from getting 2.6.5 instead and getting past this error."
"The fix was accomplished this way:

Step 1: Setup rvm
1. Install packages
```
sudo apt-get install software-properties-common
sudo apt-add-repository -y ppa:rael-gc/rvm
sudo apt-get update
sudo apt-get install rvm
echo 'source ""/etc/profile.d/rvm.sh""' >> ~/.bashrc
```
2. Logout. Login.
3. Test running `rvm`.
4. Change directories to `incubator-mxnet/docs/static_site/src` **important**

Step 2: Setup Ruby
1. Install Ruby 2.6.5
```
rvm install '2.6.5'
```
2. Use Ruby 2.6.5
```
rvm use '2.6.5'
```

Step 3: Setup `bundler2`
```
gem update --system
gem install bundler:2.0.2
```

Step 4: update the Gemfile
1. Edit Gemfile and add `ruby ""2.6.5""` under `source ""https://rubygems.org""`
2. Update the Gemfile.lock file
```
bundler update --bundler
```

Then well after I went down a rabbit hole of updating Jekyll, I realized the 2.6.3 setting was coming from the Dockerfile. So anytime this happens again... you have to check:
* the dockerfile for jekyll
* the _config.yml files
* the Gemlock file
* and regen the Gemlock.lock file
",IssueComment,https://github.com/apache/mxnet/issues/17918#issuecomment-605290574,aaronmarkham,2020-03-27 20:00:53,17918,[17927],Build bug,0,"The fix was accomplished this way: Step 1: Setup rvm 1. Install packages ``[code]`[code]rvm[code]incubator-mxnet/docs/static_site/src[code]`[code]`[code]`[code]`[code]bundler2[code]`[code]`[code]ruby ""2.6.5""[code]source ""[url]""[code]`[code]`` Then well after I went down a rabbit hole of updating Jekyll, I realized the 2.6.3 setting was coming from the Dockerfile. So anytime this happens again... you have to check: * the dockerfile for jekyll * the _config.yml files * the Gemlock file * and regen the Gemlock.lock file"
"I can reproduce this with mxnet-cu101==1.6 on a G4 instancen. This appears to be a bug with operator fusion. If you export `MXNET_USE_FUSION=0` the program works as expect.

```
% MXNET_USE_FUSION=0 python3 test.py                                           24s ~ ip-172-31-32-170
hybridizing
[[0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5]
 [0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5]
 [0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5]]
% MXNET_USE_FUSION=1 python3 test.py                                                                                                                                                  7s ~ ip-172-31-32-170
hybridizing
[[0.  1.  2.  3.  4.  5.  6.  7.  8. ]
 [9.  0.  1.  2.  3.  4.  5.  6.  7. ]
 [8.  9.  0.  0.5 1.  1.5 2.  2.5 3. ]]
```

CC @ptrendx ",IssueComment,https://github.com/apache/mxnet/issues/17914#issuecomment-604559218,leezu,2020-03-26 17:18:01,17914,[17937],Processor bug,1,I can reproduce this with mxnet-cu101==1.6 on a G4 instancen. This appears to be a bug with operator fusion. If you export [code] the program works as expect. ``[code]`` CC @ptrendx
I will look into this - I'm not sure if this is + though that is problematic. It looks more like a problem with handling of slice_axis. I will investigate.,IssueComment,https://github.com/apache/mxnet/issues/17914#issuecomment-604604059,ptrendx,2020-03-26 18:32:34,17914,[17937],Processor bug,1,I will look into this - I'm not sure if this is + though that is problematic. It looks more like a problem with handling of slice_axis. I will investigate.
"Ok, I see where the problem comes from - there is a bug in handling negative values for parameters in the code generator - in your example if you change axis to 1 and end to 9 you will get the right answer. I will fix that and submit PR shortly.",IssueComment,https://github.com/apache/mxnet/issues/17914#issuecomment-605298056,ptrendx,2020-03-27 20:20:58,17914,[17937],Processor bug,1,"Ok, I see where the problem comes from - there is a bug in handling negative values for parameters in the code generator - in your example if you change axis to 1 and end to 9 you will get the right answer. I will fix that and submit PR shortly."
"Also, a deeper dive into the problem shows that the issue appears when one layer is reused for 5,6,7 times:
```bash
wget https://gist.githubusercontent.com/sxjscience/0bd336c921396b3c66331354e1866886/raw/80a428980fd91110455e847c1a02aef4ae2cba7f/grad_req_addto_bug.py -O grad_req_addto_bug.py
for nrepeat in 1 2 3 4 5 6 7 8 9 10
do
    echo ""nrepeat=${nrepeat}""
    echo ""with addto""
    python grad_req_addto_bug.py --addto --nrepeat ${nrepeat}
    echo ""without addto""
    python grad_req_addto_bug.py --nrepeat ${nrepeat}
done
```
Result:
```
nrepeat=1
with addto
foo0_dense0_weight 86.363464
foo0_dense0_bias 19.548544
without addto
foo0_dense0_weight 86.363464
foo0_dense0_bias 19.548544
nrepeat=2
with addto
foo0_dense0_weight 21.480562
foo0_dense0_bias 19.870453
without addto
foo0_dense0_weight 21.480562
foo0_dense0_bias 19.870453
nrepeat=3
with addto
foo0_dense0_weight 4.64952
foo0_dense0_bias 19.938385
without addto
foo0_dense0_weight 4.64952
foo0_dense0_bias 19.938385
nrepeat=4
with addto
foo0_dense0_weight 0.94337225
foo0_dense0_bias 19.947392
without addto
foo0_dense0_weight 0.94337225
foo0_dense0_bias 19.947392
nrepeat=5
with addto
foo0_dense0_weight 1.7802463
foo0_dense0_bias 315.05945
without addto
foo0_dense0_weight 0.19310227
foo0_dense0_bias 19.947094
nrepeat=6
with addto
foo0_dense0_weight 0.6738244
foo0_dense0_bias 630.11865
without addto
foo0_dense0_weight 0.041728128
foo0_dense0_bias 19.946844
nrepeat=7
with addto
foo0_dense0_weight 0.26325437
foo0_dense0_bias 1260.2372
without addto
foo0_dense0_weight 0.009131842
foo0_dense0_bias 19.946758
nrepeat=8
with addto
foo0_dense0_weight 0.0020059107
foo0_dense0_bias 19.946749
without addto
foo0_dense0_weight 0.0020059107
foo0_dense0_bias 19.946749
nrepeat=9
with addto
foo0_dense0_weight 0.00045126013
foo0_dense0_bias 19.946749
without addto
foo0_dense0_weight 0.00045126013
foo0_dense0_bias 19.946749
nrepeat=10
with addto
foo0_dense0_weight 0.00010413639
foo0_dense0_bias 19.946749
without addto
foo0_dense0_weight 0.00010413639
foo0_dense0_bias 19.946749
```

This shows that it's only wrong when `nrepeat` is 5,6,7",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610206398,sxjscience,2020-04-07 06:43:54,17989,[17995],Algorithm design bug,1,"Also, a deeper dive into the problem shows that the issue appears when one layer is reused for 5,6,7 times: ``[code]`[code]`[code]`[code]nrepeat` is 5,6,7"
"Also, adding zero_grad before the `mx.autograd.record()` won't solve this problem. I've revised the script in gist and you may try the new version:
```bash
wget https://gist.githubusercontent.com/sxjscience/0bd336c921396b3c66331354e1866886/raw/d618ba69cbecf04d3013db77af86c29d62fe0336/grad_req_addto_bug.py -O grad_req_addto_bug.py
python grad_req_addto_bug.py --addto
python grad_req_addto_bug.py

```


```log
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --addto

foo0_dense0_weight 1.7802463
foo0_dense0_bias 315.05945
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py
foo0_dense0_weight 0.19310227
foo0_dense0_bias 19.947094
```",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610208876,sxjscience,2020-04-07 06:50:38,17989,[17995],Algorithm design bug,1,"Also, adding zero_grad before the [code] won't solve this problem. I've revised the script in gist and you may try the new version: ``[code]`[code]`[code]``"
"I discovered this bug when trying to use different parameters of ALBERT. In the original ALBERT, the number of layers are 12 or 24. Both of them won't trigger the bug, so it took me some time to localize the issue.

```
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --addto --nrepeat 12

foo0_dense0_weight 5.412447e-06
foo0_dense0_bias 19.946749
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --nrepeat 12
foo0_dense0_weight 5.412447e-06
foo0_dense0_bias 19.946749
```

```
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --addto --nrepeat 24

foo0_dense0_weight 5.706055e-14
foo0_dense0_bias 19.946749
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --nrepeat 24
foo0_dense0_weight 5.706055e-14
```

Also, the bug occurs in the hybridized case.

```
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py --addto --nrepeat 5 --hybridize

foo0_dense0_weight 1.7802463
foo0_dense0_bias 315.05945
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug.py  --nrepeat 5 --hybridize
foo0_dense0_weight 0.19310227
foo0_dense0_bias 19.947094
```

It also appears in the legacy `mx.nd` interface:

```python
import mxnet as mx
from mxnet.gluon import nn, HybridBlock
import numpy as np
import argparse

np.random.seed(123)
mx.random.seed(123)


parser = argparse.ArgumentParser(
        description='Grad req bug minimal example')
parser.add_argument('--addto', action='store_true')
parser.add_argument('--hybridize', action='store_true')
parser.add_argument('--nrepeat', type=int, default=5)
args = parser.parse_args()

class Foo(HybridBlock):
    def __init__(self, prefix=None, params=None):
        super().__init__(prefix=prefix, params=params)
        with self.name_scope():
            self.layer = nn.Dense(16)

    def hybrid_forward(self, F, dat):
        out = dat
        for _ in range(args.nrepeat):
            out = self.layer(out)
        return out

foo = Foo()
if args.hybridize:
   foo.hybridize()
foo.initialize(ctx=mx.gpu())

if args.addto:
    for p in foo.collect_params().values():
        p.grad_req = 'add'



dat = mx.nd.random.normal(0, 1, (32, 16), ctx=mx.gpu())
og = mx.nd.random.normal(0, 1, (32, 16), ctx=mx.gpu())
with mx.autograd.record():
    out = foo(dat)
    loss = (out * og).sum()
    loss.backward()
for k, v in foo.collect_params().items():
    print(k, mx.nd.norm(v.grad()))


```

```bash
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug_nd.py  --nrepeat 5 --hybridize
foo0_dense0_weight 
[0.16300175]
<NDArray 1 @gpu(0)>
foo0_dense0_bias 
[27.344622]
<NDArray 1 @gpu(0)>
ubuntu@ip-172-31-27-255:~$ python grad_req_addto_bug_nd.py  --nrepeat 5 --hybridize --addto
foo0_dense0_weight 
[1.3425881]
<NDArray 1 @gpu(0)>
foo0_dense0_bias 
[424.70026]
<NDArray 1 @gpu(0)>
```
",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610213687,sxjscience,2020-04-07 07:03:15,17989,[17995],Algorithm design bug,1,"I discovered this bug when trying to use different parameters of ALBERT. In the original ALBERT, the number of layers are 12 or 24. Both of them won't trigger the bug, so it took me some time to localize the issue. ``[code]`[code]`[code]`[code]`[code]`[code]mx.nd[code]`[code]`[code]`[code]``"
"Just verified that there is no problem when `ctx=mx.cpu()` . Also, I've found a simpler script to reproduce the problem:

```python

import mxnet as mx
import numpy as np
mx.npx.set_np()


for ctx in [mx.cpu(), mx.gpu()]:
    for nrepeat in range(1, 10):
        stored_grad = dict()
        for grad_req in ['write', 'add']:
            a = mx.np.array(1, ctx=ctx)
            b = mx.np.array(2, ctx=ctx)
            if grad_req == 'write':
                a.attach_grad(grad_req='write')
            elif grad_req == 'add':
                a.attach_grad(grad_req='add')
            a.grad[()] = 0
            with mx.autograd.record():
                for _ in range(nrepeat):
                    b = b * a
                b.backward()
            stored_grad[grad_req] = a.grad.asnumpy()
        print('ctx={}, nrepeat={}, write={}, add={}'.format(ctx, nrepeat, stored_grad['write'], stored_grad['add']))
```

Result:
```
ctx=cpu(0), nrepeat=1, write=2.0, add=2.0
ctx=cpu(0), nrepeat=2, write=4.0, add=4.0
ctx=cpu(0), nrepeat=3, write=6.0, add=6.0
ctx=cpu(0), nrepeat=4, write=8.0, add=8.0
ctx=cpu(0), nrepeat=5, write=10.0, add=10.0
ctx=cpu(0), nrepeat=6, write=12.0, add=12.0
ctx=cpu(0), nrepeat=7, write=14.0, add=14.0
ctx=cpu(0), nrepeat=8, write=16.0, add=16.0
ctx=cpu(0), nrepeat=9, write=18.0, add=18.0
ctx=gpu(0), nrepeat=1, write=2.0, add=2.0
ctx=gpu(0), nrepeat=2, write=4.0, add=4.0
ctx=gpu(0), nrepeat=3, write=6.0, add=6.0
ctx=gpu(0), nrepeat=4, write=8.0, add=8.0
ctx=gpu(0), nrepeat=5, write=10.0, add=62.0
ctx=gpu(0), nrepeat=6, write=12.0, add=126.0
ctx=gpu(0), nrepeat=7, write=14.0, add=254.0
ctx=gpu(0), nrepeat=8, write=16.0, add=16.0
ctx=gpu(0), nrepeat=9, write=18.0, add=18.0
```",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610219553,sxjscience,2020-04-07 07:17:41,17989,[17995],Algorithm design bug,1,"Just verified that there is no problem when [code] . Also, I've found a simpler script to reproduce the problem: ``[code]`[code]`[code]``"
@eric-haibin-lin @szha @szhengac @zhreshold This is the worst problem I've found and it impacts all models with `grad_req=add`.,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610220167,sxjscience,2020-04-07 07:19:11,17989,[17995],Algorithm design bug,1,@eric-haibin-lin @szha @szhengac @zhreshold This is the worst problem I've found and it impacts all models with [code].
This bug does affect many models with parameters sharing.,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610225332,zheyuye,2020-04-07 07:31:19,17989,[17995],Algorithm design bug,1,This bug does affect many models with parameters sharing.
@ptrendx both this issue and the #16708 seem to be GPU-specific. Would you mind taking a look?,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610466779,szha,2020-04-07 15:49:52,17989,[17995],Algorithm design bug,1,@ptrendx both this issue and the #16708 seem to be GPU-specific. Would you mind taking a look?
Sure,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610480017,ptrendx,2020-04-07 16:12:37,17989,[17995],Algorithm design bug,1,Sure
"Hmm, I just tried the latest script from @sxjscience and I got exactly opposite results - gpu working as expected and cpu not working right:
```
ctx=cpu(0), nrepeat=1, write=2.0, add=2.0
ctx=cpu(0), nrepeat=2, write=4.0, add=4.0
ctx=cpu(0), nrepeat=3, write=6.0, add=6.0
ctx=cpu(0), nrepeat=4, write=8.0, add=8.0
ctx=cpu(0), nrepeat=5, write=10.0, add=62.0
ctx=cpu(0), nrepeat=6, write=12.0, add=126.0
ctx=cpu(0), nrepeat=7, write=14.0, add=254.0
ctx=cpu(0), nrepeat=8, write=16.0, add=16.0
ctx=cpu(0), nrepeat=9, write=18.0, add=18.0
ctx=gpu(0), nrepeat=1, write=2.0, add=2.0
ctx=gpu(0), nrepeat=2, write=4.0, add=4.0
ctx=gpu(0), nrepeat=3, write=6.0, add=6.0
ctx=gpu(0), nrepeat=4, write=8.0, add=8.0
ctx=gpu(0), nrepeat=5, write=10.0, add=10.0
ctx=gpu(0), nrepeat=6, write=12.0, add=12.0
ctx=gpu(0), nrepeat=7, write=14.0, add=14.0
ctx=gpu(0), nrepeat=8, write=16.0, add=16.0
ctx=gpu(0), nrepeat=9, write=18.0, add=18.0
```

I ran it ~15 times and always got the same result. What is the version of MXNet you tried it on, @sxjscience?",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610591415,ptrendx,2020-04-07 20:00:41,17989,[17995],Algorithm design bug,1,"Hmm, I just tried the latest script from @sxjscience and I got exactly opposite results - gpu working as expected and cpu not working right: ``[code]`` I ran it ~15 times and always got the same result. What is the version of MXNet you tried it on, @sxjscience?"
"@ptrendx @szha @zhreshold I find that the bug also exists in 1.5.0, 1.4.0, 1.3.1, 1.2.1. In fact, results on both CPU and GPU are wrong in these versions. Reproducible script is given as follows (I used the legacy mx.nd).

```python
import mxnet as mx
import numpy as np


for ctx in [mx.cpu(), mx.gpu()]:
    for nrepeat in range(1, 10):
        stored_grad = dict()
        for grad_req in ['write', 'add']:
            a = mx.nd.array([1], ctx=ctx)
            b = mx.nd.array([2], ctx=ctx)
            if grad_req == 'write':
                a.attach_grad(grad_req='write')
            elif grad_req == 'add':
                a.attach_grad(grad_req='add')
            a.grad[:] = 0
            with mx.autograd.record():
                for _ in range(nrepeat):
                    b = b * a
                b.backward()
            stored_grad[grad_req] = a.grad.asscalar()
        print('ctx={}, nrepeat={}, write={}, add={}'.format(ctx, nrepeat, stored_grad['write'], stored_grad['add']))
```

For MXNet 1.5.0, I used `pip install mxnet-cu101==1.5.0`
For MXNet 1.4.0, I used `pip install mxnet-cu92==1.4.0`
For MXNet 1.3.1, I used `pip install mxnet-cu92==1.3.1`
For MXNet 1.2.1, I used `pip install mxnet-cu92==1.2.1`

Output
```
ctx=cpu(0), nrepeat=1, write=2.0, add=2.0
ctx=cpu(0), nrepeat=2, write=4.0, add=4.0
ctx=cpu(0), nrepeat=3, write=6.0, add=6.0
ctx=cpu(0), nrepeat=4, write=8.0, add=8.0
ctx=cpu(0), nrepeat=5, write=10.0, add=62.0
ctx=cpu(0), nrepeat=6, write=12.0, add=126.0
ctx=cpu(0), nrepeat=7, write=14.0, add=254.0
ctx=cpu(0), nrepeat=8, write=16.0, add=16.0
ctx=cpu(0), nrepeat=9, write=18.0, add=18.0
ctx=gpu(0), nrepeat=1, write=2.0, add=2.0
ctx=gpu(0), nrepeat=2, write=4.0, add=4.0
ctx=gpu(0), nrepeat=3, write=6.0, add=6.0
ctx=gpu(0), nrepeat=4, write=8.0, add=8.0
ctx=gpu(0), nrepeat=5, write=10.0, add=62.0
ctx=gpu(0), nrepeat=6, write=12.0, add=126.0
ctx=gpu(0), nrepeat=7, write=14.0, add=254.0
ctx=gpu(0), nrepeat=8, write=16.0, add=16.0
ctx=gpu(0), nrepeat=9, write=18.0, add=18.0
```",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610591835,sxjscience,2020-04-07 20:01:38,17989,[17995],Algorithm design bug,1,"@ptrendx @szha @zhreshold I find that the bug also exists in 1.5.0, 1.4.0, 1.3.1, 1.2.1. In fact, results on both CPU and GPU are wrong in these versions. Reproducible script is given as follows (I used the legacy mx.nd). ``[code]`[code]pip install mxnet-cu101==1.5.0[code]pip install mxnet-cu92==1.4.0[code]pip install mxnet-cu92==1.3.1[code]pip install mxnet-cu92==1.2.1[code]`[code]``"
"@ptrendx @zhreshold @szha I tried to run with MXNet==1.0.0 but it give me another error. The earliest version I can confirm that has this issue is 1.2.0. This is really critical and impacts the very basic functionality of a DL framework, i.e., autograd.",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610595164,sxjscience,2020-04-07 20:09:05,17989,[17995],Algorithm design bug,1,"@ptrendx @zhreshold @szha I tried to run with MXNet==1.0.0 but it give me another error. The earliest version I can confirm that has this issue is 1.2.0. This is really critical and impacts the very basic functionality of a DL framework, i.e., autograd."
"@ptrendx I'm using a compiled version of master. Are you able to reproduce it using the script I attached at the beginning of the issue?
```
wget https://gist.githubusercontent.com/sxjscience/0bd336c921396b3c66331354e1866886/raw/d618ba69cbecf04d3013db77af86c29d62fe0336/grad_req_addto_bug.py -O grad_req_addto_bug.py
python grad_req_addto_bug.py --addto
python grad_req_addto_bug.py
```",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610596656,sxjscience,2020-04-07 20:12:38,17989,[17995],Algorithm design bug,1,@ptrendx I'm using a compiled version of master. Are you able to reproduce it using the script I attached at the beginning of the issue? ``[code]``
"I first tried on our container (which is based on 1.6.0), since that is the easiest thing for me to try first. When I tried both your numpy and ndarray small examples I see CPU exhibiting the error and GPU not, so looking into what could be different between those implementations.",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610598084,ptrendx,2020-04-07 20:15:50,17989,[17995],Algorithm design bug,1,"I first tried on our container (which is based on 1.6.0), since that is the easiest thing for me to try first. When I tried both your numpy and ndarray small examples I see CPU exhibiting the error and GPU not, so looking into what could be different between those implementations."
May be it's not related to specific implementation in the GPU side.,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610599468,sxjscience,2020-04-07 20:18:39,17989,[17995],Algorithm design bug,1,May be it's not related to specific implementation in the GPU side.
"It is ElementwiseSum, although I'm not sure why after 7 repeats you get correct result again.
The code of ElementwiseSum for more than 4 repetitions:
```c
    default: {
      DType* in_0_dptr = in_data[0].dptr<DType>();
      Kernel<Sum, xpu>::Launch(s, out_size, out_dptr, req[0], in_0_dptr);
      for (size_t i = 1; i < size; ++i) {
        DType* in_dptr = in_data[i].dptr<DType>();
        Kernel<Sum, xpu>::Launch(s, out_size, out_dptr, req[0], out_dptr, in_dptr);
      }
      break;
    }
```
which is wrong - if `req[0]` is kAddTo (which it will be in this case), after the first kernel you get 
```
out = out + in_0
```
but then the subsequent ones instead of doing
```
out = out + in_i
```
do
```
out += out + in_i
```",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610604273,ptrendx,2020-04-07 20:29:24,17989,[17995],Algorithm design bug,1,"It is ElementwiseSum, although I'm not sure why after 7 repeats you get correct result again. The code of ElementwiseSum for more than 4 repetitions: ``[code]`[code]req[0][code]`[code]`[code]`[code]`[code]`[code]``"
"I don't see this issue in our (not yet released) container because I changed ElementwiseSum implementation to be vectorized, and there I do have the proper logic:
```c
      for (size_t i = 0; i < inputs.size(); i += num_inputs_per_kernel) {
        if (i == 0) {
          using Kernel = VectorizedElementwiseSumFwd<DType, Req>;
          typename Kernel::ParamType params;
          params.num_inputs = std::min(num_inputs_per_kernel, inputs.size() - i);
          for (int j = 0; j < params.num_inputs; ++j) {
            params.inputs[j] = inputs[i + j].dptr<DType>();
          }
          params.outputs[0] = outputs[0].dptr<DType>();
          VectorizedKernelLauncher<DType, LType, Kernel>(size, s, params);
        } else {
          /* During subsequent launches we need to
             accumulate into the previous outputs
          */
          using Kernel = VectorizedElementwiseSumFwd<DType, kAddTo>;
          typename Kernel::ParamType params;
          params.num_inputs = std::min(num_inputs_per_kernel, inputs.size() - i);
          for (int j = 0; j < params.num_inputs; ++j) {
            params.inputs[j] = inputs[i + j].dptr<DType>();
          }
          params.outputs[0] = outputs[0].dptr<DType>();
          VectorizedKernelLauncher<DType, LType, Kernel>(size, s, params);
        }
      }
```
where I change the sum to do `kAddTo` and not including the output as one of the parameters.",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610605392,ptrendx,2020-04-07 20:31:49,17989,[17995],Algorithm design bug,1,"I don't see this issue in our (not yet released) container because I changed ElementwiseSum implementation to be vectorized, and there I do have the proper logic: ``[code]`[code]kAddTo` and not including the output as one of the parameters."
@ptrendx Thanks! I think that explains the cause.,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610605521,sxjscience,2020-04-07 20:32:06,17989,[17995],Algorithm design bug,1,@ptrendx Thanks! I think that explains the cause.
It does not explain why it does the right thing for nrepeat=8 forward. There has to be something else going on there that limits elementwisesum to 7 inputs somehow.,IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610605942,ptrendx,2020-04-07 20:33:03,17989,[17995],Algorithm design bug,1,It does not explain why it does the right thing for nrepeat=8 forward. There has to be something else going on there that limits elementwisesum to 7 inputs somehow.
"I don't think it's related to particular op implementation, it's something may not be working at all when autograd is introduced. 
See the experiments I did: https://github.com/apache/incubator-mxnet/issues/16708#issuecomment-558876214

What I did is replicate the same node N times, if N is in (1, 2, 3, 4, 8, 9, 10...) times, the loss and gradients are always GOOD, however, with (5, 6, 7), the gradients will diverge at the first iteration",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610623359,zhreshold,2020-04-07 21:12:34,17989,[17995],Algorithm design bug,1,"I don't think it's related to particular op implementation, it's something may not be working at all when autograd is introduced. See the experiments I did: [url]#issuecomment-558876214 What I did is replicate the same node N times, if N is in (1, 2, 3, 4, 8, 9, 10...) times, the loss and gradients are always GOOD, however, with (5, 6, 7), the gradients will diverge at the first iteration"
"@zhreshold And for any op that you check you introduce the elemwisesum node in the backward pass that aggregates the gradients, which you do not see in the model. As I said, I still do not understand why 8+ is fine, I would expect everything above 4 to fail (as elemwisesum implementation has special cases for 2,3 and 4, and then buggy one for 5+). I will look into it further.",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610639500,ptrendx,2020-04-07 21:54:04,17989,[17995],Algorithm design bug,1,"@zhreshold And for any op that you check you introduce the elemwisesum node in the backward pass that aggregates the gradients, which you do not see in the model. As I said, I still do not understand why 8+ is fine, I would expect everything above 4 to fail (as elemwisesum implementation has special cases for 2,3 and 4, and then buggy one for 5+). I will look into it further."
"@ptrendx After checking the source code, I think it's due to the `MXNET_EXEC_INPLACE_GRAD_SUM_CAP` setting to 8 by default. The `ElementwiseSum` is used in the GraphExecutor to accumulate the gradient:
https://github.com/apache/incubator-mxnet/blob/79c576b8157539d365cc9e0e1e355d4ca12f7374/src/executor/graph_executor.cc#L255-L263

And the inplace_sum_cap is by default 8:
https://github.com/apache/incubator-mxnet/blob/79c576b8157539d365cc9e0e1e355d4ca12f7374/src/executor/graph_executor.cc#L228",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610643223,sxjscience,2020-04-07 22:04:11,17989,[17995],Algorithm design bug,1,"@ptrendx After checking the source code, I think it's due to the [code] setting to 8 by default. The [code] is used in the GraphExecutor to accumulate the gradient: [url]#L255-L263 And the inplace_sum_cap is by default 8: [url]#L228"
"Yup, setting that env variable (undocumented BTW ;-) ) to higher value makes the test fail for the higher cases too.",IssueComment,https://github.com/apache/mxnet/issues/17989#issuecomment-610646412,ptrendx,2020-04-07 22:13:45,17989,[17995],Algorithm design bug,1,"Yup, setting that env variable (undocumented BTW ;-) ) to higher value makes the test fail for the higher cases too."
"Discovered in all 5 builds of PR https://github.com/apache/incubator-mxnet/pull/17993
",IssueComment,https://github.com/apache/mxnet/issues/18005#issuecomment-611692990,ChaiBapchya,2020-04-09 18:47:18,18005,[18018],Build bug,0,Discovered in all 5 builds of PR [url]
"Ubuntu 14.04 is no longer supported. Nvidia may now require a newer ssl toolchain than provided by ubuntu 14.04.

The issue doesn't occur on master because it's hidden by the Docker cache",IssueComment,https://github.com/apache/mxnet/issues/18005#issuecomment-611702142,leezu,2020-04-09 19:07:27,18005,[18018],Build bug,0,Ubuntu 14.04 is no longer supported. Nvidia may now require a newer ssl toolchain than provided by ubuntu 14.04. The issue doesn't occur on master because it's hidden by the Docker cache
@leezu FYI.,IssueComment,https://github.com/apache/mxnet/issues/18091#issuecomment-615284958,TaoLv,2020-04-17 14:44:24,18091,[18114],Build bug,0,@leezu FYI.
"Thanks @anko-intel. This used to work, when we relied on standard cmake `initial-cache` feature to handle our config: `cmake -C ../config.cmake`. 

```
-C <initial-cache>
              Pre-load a script to populate the cache.

              When cmake is first run in an empty build tree, it creates a CMakeCache.txt file and populates  it  with
              customizable  settings  for  the  project.  This option may be used to specify a file from which to load
              cache entries before the first pass through the project’s cmake listfiles.  The loaded entries take pri‐
              ority  over  the  project’s default values.  The given file should be a CMake script containing SET com‐
              mands that use the CACHE option, not a cache-format file.
```

But people were unhappy with the extra typing and we now load the config file automatically if it is present. At the time of loading, the build release type is already set and can't be configured anymore, so we need to remove the `Uncomment the following line to compile with debug information` parts from the config files. You need to invoke cmake with `cmake -DCMAKE_BUILD_TYPE=Debug` instead.

Would you like to open a PR?",IssueComment,https://github.com/apache/mxnet/issues/18091#issuecomment-615357495,leezu,2020-04-17 16:59:54,18091,[18114],Build bug,0,"Thanks @anko-intel. This used to work, when we relied on standard cmake [code] feature to handle our config: [code]. ``[code]`[code]Uncomment the following line to compile with debug information[code]cmake -DCMAKE_BUILD_TYPE=Debug` instead. Would you like to open a PR?"
It's the same issue as https://github.com/apache/incubator-mxnet/pull/17742,IssueComment,https://github.com/apache/mxnet/issues/18091#issuecomment-615358586,leezu,2020-04-17 17:02:12,18091,[18114],Build bug,0,It's the same issue as [url]
"ok, I will",IssueComment,https://github.com/apache/mxnet/issues/18091#issuecomment-615388550,anko-intel,2020-04-17 18:07:50,18091,[18114],Build bug,0,"ok, I will"
Thank you,IssueComment,https://github.com/apache/mxnet/issues/18091#issuecomment-615402044,leezu,2020-04-17 18:38:37,18091,[18114],Build bug,0,Thank you
"Hey, this is the MXNet Label Bot. 
 Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. 
 Here are my recommended labels: Doc",IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-478703572,mxnet-label-bot,2019-04-01 19:00:40,14583,[18182],Visualization bug,0,"Hey, this is the MXNet Label Bot. Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it. Here are my recommended labels: Doc"
"@mxnet-label-bot add [Website, Bug]",IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-478704005,zachgk,2019-04-01 19:01:51,14583,[18182],Visualization bug,0,"@mxnet-label-bot add [Website, Bug]"
@zachgk Is this still an issue on the new site?,IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-536652955,aaronmarkham,2019-09-30 16:57:53,14583,[18182],Visualization bug,0,@zachgk Is this still an issue on the new site?
"@aaronmarkham This issue is still there. To reproduce, 

1. go to the installation docs on the get_started page. 
2. Randomly click on any of the buttons. Note that it changes the url path to index.html which is also wrong.
3. If you press the browser back button, it doesn't do anything. If you press it repeatedly, it only takes effect if you press it more than the number of times you changed the installation options.


You can see the relevant code [here](https://github.com/apache/incubator-mxnet/blob/master/docs/static_site/src/assets/js/options.js). There are two ways to resolve this problem:
1. We simply don't use the [history api](https://developer.mozilla.org/en-US/docs/Web/API/History_API) and delete all occurrences of history.pushState. Then, the back button will simply leave the get_started page.
2. We handle the history `popstate` event so that presses to the back (and I believe forward) button are handled and will switch to the previously selected options. There may be other things to fix to get this working as well (especially due to the new site).",IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-536697442,zachgk,2019-09-30 18:45:02,14583,[18182],Visualization bug,0,"@aaronmarkham This issue is still there. To reproduce, 1. go to the installation docs on the get_started page. 2. Randomly click on any of the buttons. Note that it changes the url path to index.html which is also wrong. 3. If you press the browser back button, it doesn't do anything. If you press it repeatedly, it only takes effect if you press it more than the number of times you changed the installation options. You can see the relevant code [here]([url] There are two ways to resolve this problem: 1. We simply don't use the [history api]([url] and delete all occurrences of history.pushState. Then, the back button will simply leave the get_started page. 2. We handle the history [code] event so that presses to the back (and I believe forward) button are handled and will switch to the previously selected options. There may be other things to fix to get this working as well (especially due to the new site)."
Sheng fixed this on the new site.,IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-544173332,aaronmarkham,2019-10-19 17:05:01,14583,[18182],Visualization bug,0,Sheng fixed this on the new site.
I think Sheng fixed a different problem. I just checked and this problem still occurs,IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-546046564,zachgk,2019-10-24 18:32:56,14583,[18182],Visualization bug,0,I think Sheng fixed a different problem. I just checked and this problem still occurs
"@aaronmarkham This bug is still reproducible on new website, can I work on this?",IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-620291638,ys2843,2020-04-27 23:40:06,14583,[18182],Visualization bug,0,"@aaronmarkham This bug is still reproducible on new website, can I work on this?"
@aaronmarkham This issue was resolved. We can close it.,IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-635636738,ys2843,2020-05-28 22:15:10,14583,[18182],Visualization bug,0,@aaronmarkham This issue was resolved. We can close it.
Thanks @ys2843. Fixed in #18182,IssueComment,https://github.com/apache/mxnet/issues/14583#issuecomment-635660767,zachgk,2020-05-28 23:05:50,14583,[18182],Visualization bug,0,Thanks @ys2843. Fixed in #18182
I will work on this issue,IssueComment,https://github.com/apache/mxnet/issues/18235#issuecomment-624144561,ys2843,2020-05-05 16:02:58,18235,[18243],Visualization bug,0,I will work on this issue
Thanks!,IssueComment,https://github.com/apache/mxnet/issues/18235#issuecomment-624245447,apeforest,2020-05-05 19:01:22,18235,[18243],Visualization bug,0,Thanks!
Note that the original error was wrong due to confusing log on teardown of module which attributes errors to the last test (i.e. `test_fp16_casting`). This happens even though it was disabled which helped me catch this problem. This bug is tracked in https://github.com/pytest-dev/pytest/issues/7101,IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-616289525,szha,2020-04-20 03:33:09,18099,[18292],Test bug,0,Note that the original error was wrong due to confusing log on teardown of module which attributes errors to the last test (i.e. [code]). This happens even though it was disabled which helped me catch this problem. This bug is tracked in [url]
cc @yzhliu as we discussed about a (supposedly) related problem with `less_scalar_gpufloat32_2bool_2` before.,IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-616291162,leezu,2020-04-20 03:40:36,18099,[18292],Test bug,0,cc @yzhliu as we discussed about a (supposedly) related problem with [code] before.
same problem with `test_amp_conversion` http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-gpu/detail/PR-18025/35/pipeline/417,IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-616369610,szha,2020-04-20 07:41:22,18099,[18292],Test bug,0,same problem with [code] [url]
Seems to have the same problem for the rest of the tests so this indicates a problem to the amp module itself. I've disabled all amp tests at the moment,IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-616667052,szha,2020-04-20 16:31:18,18099,[18292],Test bug,0,Seems to have the same problem for the rest of the tests so this indicates a problem to the amp module itself. I've disabled all amp tests at the moment
"The error comes from TVM actually, not pointwise fusion.",IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-627557905,ptrendx,2020-05-12 19:53:02,18099,[18292],Test bug,0,"The error comes from TVM actually, not pointwise fusion."
Let's reenable the test as TVMOP is now disabled on GPU builds,IssueComment,https://github.com/apache/mxnet/issues/18099#issuecomment-627621350,leezu,2020-05-12 22:00:36,18099,[18292],Test bug,0,Let's reenable the test as TVMOP is now disabled on GPU builds
This is very likely caused by parallel test. See https://github.com/apache/incubator-mxnet/pull/18323,IssueComment,https://github.com/apache/mxnet/issues/18322#issuecomment-628816780,zhreshold,2020-05-14 18:38:21,18322,[18323],Test bug,0,This is very likely caused by parallel test. See [url]
@mxnet-label-bot add [Website],IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631044152,ys2843,2020-05-19 19:48:56,18367,[18369],Visualization bug,0,@mxnet-label-bot add [Website]
"jquery has its own CDN which isn't affected by the blocking of google CDN. so the simplest approach to try is to use that https://code.jquery.com/

could you summarize how the current website hosting works? to have more control on this we could use cloudfront for static hosting, which can also improve stability.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631056893,szha,2020-05-19 20:13:39,18367,[18369],Visualization bug,0,"jquery has its own CDN which isn't affected by the blocking of google CDN. so the simplest approach to try is to use that [url] could you summarize how the current website hosting works? to have more control on this we could use cloudfront for static hosting, which can also improve stability."
"@ys2843 - Thanks for looking into this, is the jquery loading time causing huge delay in loading MXNet website in China?

I remember a ticket by @aaronmarkham for Apache Infra for CDN in China was closed. @szha - Is there a way to statically host in China and how to overcome with Apache processes?
",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631063008,sandeep-krishnamurthy,2020-05-19 20:25:58,18367,[18369],Visualization bug,0,"@ys2843 - Thanks for looking into this, is the jquery loading time causing huge delay in loading MXNet website in China? I remember a ticket by @aaronmarkham for Apache Infra for CDN in China was closed. @szha - Is there a way to statically host in China and how to overcome with Apache processes?"
"ECharts project worked on CDN in China according to this board report: https://cwiki.apache.org/confluence/display/INCUBATOR/May2019

We may reach out to them for their experiences.

Update: in [a later report](https://cwiki.apache.org/confluence/display/INCUBATOR/August2019) they wrote: `Redirecting echarts.baidu.com to echarts.apache.org. Because we found some speed problem of Apache CDN in China, we haven't done the redirecting job yet. We will do more test and ask for help if necessary.`

Seems that using custom CDN is possible.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631094530,szha,2020-05-19 21:32:48,18367,[18369],Visualization bug,0,ECharts project worked on CDN in China according to this board report: [url] We may reach out to them for their experiences. Update: in [a later report]([url] they wrote: [code] Seems that using custom CDN is possible.
"> jquery has its own CDN which isn't affected by the blocking of google CDN. so the simplest approach to try is to use that https://code.jquery.com/
> 
> could you summarize how the current website hosting works? to have more control on this we could use cloudfront for static hosting, which can also improve stability.

Good idea, we could simply replace google CDN with `https://code.jquery.com/`, tested with Chinese servers and it is reachable, although it is slightly slower than local CDN, but it saves the fallback time. 
Currently MXNet website hosting is handled by Apache Infra team, according to [Apache infra team:](https://issues.apache.org/jira/browse/INFRA-20203)
> MXNet is likely served from Japan for Chinese visitors.
We cannot at present get access to serve from China itself, the process for doing so is extremely convoluted due to Chinese government regulations. IF we are presented with an option that works in conjunction with MX records (aka not a CDN that relies on CNAME addresses), we will revisit our options and notify projects, but for the time being, we do not have any available options to speed up traffic for Chinese visitors.

Most of the resources are hosted locally with the website and the only thing not reachable is JQuery.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631096175,ys2843,2020-05-19 21:35:31,18367,[18369],Visualization bug,0,"> jquery has its own CDN which isn't affected by the blocking of google CDN. so the simplest approach to try is to use that [url] > > could you summarize how the current website hosting works? to have more control on this we could use cloudfront for static hosting, which can also improve stability. Good idea, we could simply replace google CDN with [code], tested with Chinese servers and it is reachable, although it is slightly slower than local CDN, but it saves the fallback time. Currently MXNet website hosting is handled by Apache Infra team, according to [Apache infra team:]([url] > MXNet is likely served from Japan for Chinese visitors. We cannot at present get access to serve from China itself, the process for doing so is extremely convoluted due to Chinese government regulations. IF we are presented with an option that works in conjunction with MX records (aka not a CDN that relies on CNAME addresses), we will revisit our options and notify projects, but for the time being, we do not have any available options to speed up traffic for Chinese visitors. Most of the resources are hosted locally with the website and the only thing not reachable is JQuery."
"@ys2843 thanks for sharing. Is the speed adequate for the items hosted locally with the website? If so, you may also consider simply downloading the uglified javascript file and include it with the website codebase.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631097651,szha,2020-05-19 21:39:04,18367,[18369],Visualization bug,0,"@ys2843 thanks for sharing. Is the speed adequate for the items hosted locally with the website? If so, you may also consider simply downloading the uglified javascript file and include it with the website codebase."
"@szha It is hard to tell about the speed of hosting it locally with website. But currently there are complains about the loading speed (Avg 19s) of MXNet website from China. Although include it with website codebase is definitely the most reliable way. 
I would say for now we replace google cdn with Jquery cdn first to fix this blocker bug, and I will do more research on CDN possible choices, and reach out to EChart to see their experience.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631106737,ys2843,2020-05-19 22:00:54,18367,[18369],Visualization bug,0,"@szha It is hard to tell about the speed of hosting it locally with website. But currently there are complains about the loading speed (Avg 19s) of MXNet website from China. Although include it with website codebase is definitely the most reliable way. I would say for now we replace google cdn with Jquery cdn first to fix this blocker bug, and I will do more research on CDN possible choices, and reach out to EChart to see their experience."
"Agree, I think replacing with jQuery CDN for now is the fastest and easiest way.",IssueComment,https://github.com/apache/mxnet/issues/18367#issuecomment-631108556,marcoabreu,2020-05-19 22:05:08,18367,[18369],Visualization bug,0,"Agree, I think replacing with jQuery CDN for now is the fastest and easiest way."
"I find that issue does not only happen in numpy but also exists in ndarray:
```python
import mxnet as mx
import json
import pprint
#mx.npx.set_np()
net = mx.gluon.nn.BatchNorm(epsilon=2E-5, axis=2)
net.hybridize()
net.initialize()
a = net(mx.nd.ones((10, 3, 5, 5)))
net.export('bnorm', 0)
with open('bnorm-symbol.json') as f:
   dat = json.load(f)
   pprint.pprint(dat)
```

Output:
```
           {'attrs': {'__profiler_scope__': 'batchnorm0:',
                      'axis': '1',
                      'eps': '1e-05',
                      'fix_gamma': 'False',
                      'momentum': '0.9',
                      'use_global_stats': 'False'},
            'inputs': [[0, 0, 0], [1, 0, 0], [2, 0, 0], [3, 0, 1], [4, 0, 1]],
            'name': 'batchnorm0_fwd',
            'op': 'BatchNorm'}]}
```",IssueComment,https://github.com/apache/mxnet/issues/18373#issuecomment-631629163,sxjscience,2020-05-20 17:52:25,18373,[18377],Algorithm design bug,0,I find that issue does not only happen in numpy but also exists in ndarray: ``[code]`[code]`[code]``
"Hi @sxjscience , is it available to delete the pre-built pip packages impacted by this issue? 

BatchNorm is universally used, and this bug will not raise any exception. Users may install the previous version of MXNet with this bug, and find that the accuracy drops.",IssueComment,https://github.com/apache/mxnet/issues/18373#issuecomment-639180906,wkcn,2020-06-05 00:01:45,18373,[18377],Algorithm design bug,0,"Hi @sxjscience , is it available to delete the pre-built pip packages impacted by this issue? BatchNorm is universally used, and this bug will not raise any exception. Users may install the previous version of MXNet with this bug, and find that the accuracy drops."
"@wkcn Yes, this is a disaster for the users. However, deleting the pre-built pip packages is also not a good option because there are users that are not using BatchNorm. We will need to ensure that the official 1.7 release does not contain this bug.",IssueComment,https://github.com/apache/mxnet/issues/18373#issuecomment-639182149,sxjscience,2020-06-05 00:05:17,18373,[18377],Algorithm design bug,0,"@wkcn Yes, this is a disaster for the users. However, deleting the pre-built pip packages is also not a good option because there are users that are not using BatchNorm. We will need to ensure that the official 1.7 release does not contain this bug."
cc @ciyongch ,IssueComment,https://github.com/apache/mxnet/issues/18373#issuecomment-639194101,szha,2020-06-05 00:48:52,18373,[18377],Algorithm design bug,0,cc @ciyongch
"Hi @szha, v1.7.x doesn't include the PR https://github.com/apache/incubator-mxnet/pull/17679 (it's a new feature after code freeze), so there's no such issue on this branch. While for v1.x branch, the fix were already cherry-picked. 
I just check the latest commit of both v1.7.x and v1.x branches with the above reproducer, it works well. So no action is needed for this case.
```
           {'attrs': {'axis': '2',
                      'eps': '2e-05',
                      'fix_gamma': 'False',
                      'momentum': '0.9',
                      'use_global_stats': 'False'},
            'inputs': [[0, 0, 0], [1, 0, 0], [2, 0, 0], [3, 0, 1], [4, 0, 1]],
            'name': 'batchnorm0_fwd',
            'op': 'BatchNorm'}]}
```",IssueComment,https://github.com/apache/mxnet/issues/18373#issuecomment-639216004,ciyongch,2020-06-05 02:11:43,18373,[18377],Algorithm design bug,0,"Hi @szha, v1.7.x doesn't include the PR [url] (it's a new feature after code freeze), so there's no such issue on this branch. While for v1.x branch, the fix were already cherry-picked. I just check the latest commit of both v1.7.x and v1.x branches with the above reproducer, it works well. So no action is needed for this case. ``[code]``"
"I think we should throw the error. Currently, `float64` + `float32` cannot be mixed for division and we haven't raised the error.",IssueComment,https://github.com/apache/mxnet/issues/18353#issuecomment-631768898,sxjscience,2020-05-20 22:46:57,18353,[18393],Data bug,0,"I think we should throw the error. Currently, [code] + [code] cannot be mixed for division and we haven't raised the error."
"Actually, a warning would be thrown out

```python
>>> np.ones((2,2),dtype='float64') / np.zeros((2,2))
[04:07:01] ../src/operator/numpy/./np_true_divide-inl.h:124: not implemented yet...
array([[0.00000000e+000, 6.95161511e-310],
       [0.00000000e+000, 0.00000000e+000]], dtype=float64)
```
The reason behind the division operation's zero output is that the computation kernel is not launched.",IssueComment,https://github.com/apache/mxnet/issues/18353#issuecomment-631867278,xidulu,2020-05-21 04:08:52,18353,[18393],Data bug,0,"Actually, a warning would be thrown out ``[code]`` The reason behind the division operation's zero output is that the computation kernel is not launched."
"Actually, for this issue, we might expect a proper error that aborts the compuation.",IssueComment,https://github.com/apache/mxnet/issues/18353#issuecomment-632994463,zheyuye,2020-05-23 06:27:12,18353,[18393],Data bug,0,"Actually, for this issue, we might expect a proper error that aborts the compuation."
@BenjaminCHEN2016 will be helping after #18277 is merged.,IssueComment,https://github.com/apache/mxnet/issues/18299#issuecomment-631895809,yzhliu,2020-05-21 05:51:51,18299,[18427],Data bug,1,@BenjaminCHEN2016 will be helping after #18277 is merged.
"@sxjscience 

Thanks for your issue, I was able to reproduce this issue on my device, we discovered that the cause of this problem is https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/numpy/multiarray.py#L988, where the origin tensor would be multiplied by `-1.0` when calling negation.

I believe this problem could be resolved by changing `-1.0` to `-1`, as the backend is now able to perform type promotion for scalar-tensor binary ops, which should return a value of type `int32`.

I will create a fix for this issue. ",IssueComment,https://github.com/apache/mxnet/issues/18414#issuecomment-634489767,ryanwentaoxu,2020-05-27 07:46:27,18414,[18467],Data bug,0,"@sxjscience Thanks for your issue, I was able to reproduce this issue on my device, we discovered that the cause of this problem is [url]#L988, where the origin tensor would be multiplied by [code] when calling negation. I believe this problem could be resolved by changing [code] to [code], as the backend is now able to perform type promotion for scalar-tensor binary ops, which should return a value of type [code]. I will create a fix for this issue."
"@CassiniXu thanks for finding the root cause. In terms of the solution, I'm not sure if multiply is the right way to go. After all, we don't want int8 number to be promoted to int32 when being negated. How about implementing a negation kernel?",IssueComment,https://github.com/apache/mxnet/issues/18414#issuecomment-634891629,szha,2020-05-27 19:28:29,18414,[18467],Data bug,0,"@CassiniXu thanks for finding the root cause. In terms of the solution, I'm not sure if multiply is the right way to go. After all, we don't want int8 number to be promoted to int32 when being negated. How about implementing a negation kernel?"
"Changing
```
  shape = matchall(r""\d+"", str)
```
to
```
  shape = collect(m.match for m in eachmatch(r""\d+"", str))
```

will fix this error.",IssueComment,https://github.com/apache/mxnet/issues/18491#issuecomment-640907463,andevellicus,2020-06-08 21:53:36,18491,[18515],Version compatibility bug,0,Changing ``[code]`[code]`[code]`` will fix this error.
Please open a PR,IssueComment,https://github.com/apache/mxnet/issues/18491#issuecomment-640914343,leezu,2020-06-08 22:11:13,18491,[18515],Version compatibility bug,0,Please open a PR
Thanks for reporting the issue @xidulu! I opened a PR to fix it. Please review.,IssueComment,https://github.com/apache/mxnet/issues/18544#issuecomment-643505938,leezu,2020-06-12 22:22:32,18544,[18554],Data bug,0,Thanks for reporting the issue @xidulu! I opened a PR to fix it. Please review.
"@mxnet-label-bot add [website, bug]",IssueComment,https://github.com/apache/mxnet/issues/18567#issuecomment-644427403,ys2843,2020-06-15 22:44:56,18567,[18571],Visualization bug,0,"@mxnet-label-bot add [website, bug]"
How did you arrive at the `master` version URL? It's not linked from the https://mxnet.apache.org/get_started page,IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-572008576,leezu,2020-01-08 11:28:23,17244,[18587],Documentation bug,0,How did you arrive at the [code] version URL? It's not linked from the [url] page
"There is a drop-down list for ""MXNet version"", and i selected ""Master""",IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-572187977,eric-haibin-lin,2020-01-08 18:06:09,17244,[18587],Documentation bug,0,"There is a drop-down list for ""MXNet version"", and i selected ""Master"""
Thanks. I didn't realize that this dropdown list exists. There seems to be no UI element indicating the existence of the dropdown prior to hovering over the version number.,IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-572188934,leezu,2020-01-08 18:08:37,17244,[18587],Documentation bug,0,Thanks. I didn't realize that this dropdown list exists. There seems to be no UI element indicating the existence of the dropdown prior to hovering over the version number.
"The content looks fine in here: https://github.com/apache/incubator-mxnet/blame/master/docs/static_site/src/_includes/get_started/linux/python/cpu/pip.md

This is probably some javascript bug in here: https://github.com/apache/incubator-mxnet/blob/master/docs/static_site/src/assets/js/options.js
Never been a fan of these conditional selectors... seems like they're always breaking. 
",IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-574389324,aaronmarkham,2020-01-14 21:46:45,17244,[18587],Documentation bug,0,The content looks fine in here: [url] This is probably some javascript bug in here: [url] Never been a fan of these conditional selectors... seems like they're always breaking.
@aaronmarkham what's the alternative solution you're proposing? ,IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-574520361,eric-haibin-lin,2020-01-15 06:45:05,17244,[18587],Documentation bug,0,@aaronmarkham what's the alternative solution you're proposing?
"> @aaronmarkham what's the alternative solution you're proposing?

Maybe mxnet-oncall can spend time looking at the js function that runs that feature.
I looked it over and nothing pops out as why it isn't working. Happy to work with someone on it that maybe knows more js and how to use the debugger to experiment with the div states.",IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-574759781,aaronmarkham,2020-01-15 17:11:21,17244,[18587],Documentation bug,0,> @aaronmarkham what's the alternative solution you're proposing? Maybe mxnet-oncall can spend time looking at the js function that runs that feature. I looked it over and nothing pops out as why it isn't working. Happy to work with someone on it that maybe knows more js and how to use the debugger to experiment with the div states.
"@aaronmarkham @eric-haibin-lin Above PR fixed this issue, preview link attached, please feel free to verify.",IssueComment,https://github.com/apache/mxnet/issues/17244#issuecomment-646344319,ys2843,2020-06-18 22:52:58,17244,[18587],Documentation bug,0,"@aaronmarkham @eric-haibin-lin Above PR fixed this issue, preview link attached, please feel free to verify."
Also happened in https://github.com/apache/incubator-mxnet/pull/18579/,IssueComment,https://github.com/apache/mxnet/issues/18596#issuecomment-647025894,eric-haibin-lin,2020-06-20 17:44:39,18596,[18601],Test bug,0,Also happened in [url]
Our team will take a look if this is related to MKL integration.  Thanks report the issue @stu1130 ,IssueComment,https://github.com/apache/mxnet/issues/18569#issuecomment-645106526,pengzhao-intel,2020-06-17 02:18:59,18569,[18602],Algorithm design bug,1,Our team will take a look if this is related to MKL integration. Thanks report the issue @stu1130
"Agree the expected result, which is consistent with `mx.nd`:
```python
import mxnet as mx
import copy

v = mx.nd.array([1,2,3]).as_np_ndarray()
b = copy.copy(v)
b[0] = 10
print(b) # [10. 2. 3.]
print(v) # [1. 2. 3.]
```",IssueComment,https://github.com/apache/mxnet/issues/18685#issuecomment-656511012,wkcn,2020-07-10 06:38:50,18685,[18686],Data bug,0,"Agree the expected result, which is consistent with [code]: ``[code]``"
"I notice that 1.6 site displays both locations

https://mxnet.apache.org/versions/1.6/api/python/docs/api/gluon/block.html#mxnet.gluon.Block
https://mxnet.apache.org/versions/1.6/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Block

Maybe Sphinx get's interrupted during generating the gluon/block.html page because it contains some files with broken links: 
For example, the following file refers to API that has been removed https://github.com/apache/incubator-mxnet/blob/243ade93bcb8b7962d1faeb89c98409e3ae0d7a4/docs/python_docs/python/api/gluon/parameter_dict.rst 

Deleting this file may be sufficient for fixing the site

@ys2843 can you please help make sure the broken link checker runs on every PR to avoid such issues going forward? Maybe activating the `warning-is-error` in https://github.com/apache/incubator-mxnet/blob/243ade93bcb8b7962d1faeb89c98409e3ae0d7a4/docs/python_docs/python/scripts/conf.py or changing a different option would be sufficient?",IssueComment,https://github.com/apache/mxnet/issues/18680#issuecomment-656373159,leezu,2020-07-09 22:04:07,18680,[18689],Documentation bug,0,"I notice that 1.6 site displays both locations [url]#mxnet.gluon.Block [url]#mxnet.gluon.nn.Block Maybe Sphinx get's interrupted during generating the gluon/block.html page because it contains some files with broken links: For example, the following file refers to API that has been removed [url] Deleting this file may be sufficient for fixing the site @ys2843 can you please help make sure the broken link checker runs on every PR to avoid such issues going forward? Maybe activating the [code] in [url] or changing a different option would be sufficient?"
"@leezu - Should we revert the PR ( https://github.com/apache/incubator-mxnet/pull/18413 ) if we are unable to fix the broken website issue quickly. Ideally this should have been caught before the PR was merged. We shall work on it as you suggested above to have broken link checker as part of the PR build. But, given currently Python APIs are broken, I prefer we revert and merge back with the fix.
What do you think?",IssueComment,https://github.com/apache/mxnet/issues/18680#issuecomment-656416744,sandeep-krishnamurthy,2020-07-10 00:31:06,18680,[18689],Documentation bug,0,"@leezu - Should we revert the PR ( [url] ) if we are unable to fix the broken website issue quickly. Ideally this should have been caught before the PR was merged. We shall work on it as you suggested above to have broken link checker as part of the PR build. But, given currently Python APIs are broken, I prefer we revert and merge back with the fix. What do you think?"
"I suggest we focus on ensuring our users see the stable API docs, given that most users don't use MXNet 2 at this point anyways. This PR has exposed some bugs in the Sphinx setup which need to be fixed before the release; but this is an issue with the website generation script and shouldn't delay the feature development. What do you think @sandeep-krishnamurthy ",IssueComment,https://github.com/apache/mxnet/issues/18680#issuecomment-656446448,leezu,2020-07-10 02:32:25,18680,[18689],Documentation bug,0,"I suggest we focus on ensuring our users see the stable API docs, given that most users don't use MXNet 2 at this point anyways. This PR has exposed some bugs in the Sphinx setup which need to be fixed before the release; but this is an issue with the website generation script and shouldn't delay the feature development. What do you think @sandeep-krishnamurthy"
" WARNING: autodoc: failed to import module 'executor' from module 'mxnet'; the following exception was raised:
No module named 'scipy'
from the log


https://github.com/apache/incubator-mxnet/commit/b4b8b805fe94a6df905c6eae7f6c1f83cfea9b73 introduces the scipy import",IssueComment,https://github.com/apache/mxnet/issues/18680#issuecomment-656806613,leezu,2020-07-10 17:57:55,18680,[18689],Documentation bug,0,WARNING: autodoc: failed to import module 'executor' from module 'mxnet'; the following exception was raised: No module named 'scipy' from the log [url] introduces the scipy import
"Thanks for reporting, Jean. I confirm that this affects numpy operators too.",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-657254886,szha,2020-07-12 17:53:35,18695,[18707],Data bug,1,"Thanks for reporting, Jean. I confirm that this affects numpy operators too."
"Two changes will help immediately:
- Collapse consecutive axes of the input array into a single axis here https://github.com/apache/incubator-mxnet/blob/master/src/operator/numpy/np_matrix_op-inl.h#L142-L149
- Extend the implementation to support more https://github.com/apache/incubator-mxnet/blob/master/src/operator/tensor/matrix_op-inl.h#L362-L419

The root cause is really the limitation of template based programming in mshadow. Because of that choice, the axis is in template variable and thus needs to be expanded at compile time. We should move away from this approach and have a transpose implementation without mshadow instead.

Let's focus on only the immediate changes, and I will open a separate issue for the larger change.",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-657255680,szha,2020-07-12 18:02:13,18695,[18707],Data bug,1,"Two changes will help immediately: - Collapse consecutive axes of the input array into a single axis here [url]#L142-L149 - Extend the implementation to support more [url]#L362-L419 The root cause is really the limitation of template based programming in mshadow. Because of that choice, the axis is in template variable and thus needs to be expanded at compile time. We should move away from this approach and have a transpose implementation without mshadow instead. Let's focus on only the immediate changes, and I will open a separate issue for the larger change."
"Hi @szha , I am trying to fix it and select 'Collapse consecutive axes of the input array into a single axis here when axes.ndim() > 2'.

https://github.com/wkcn/incubator-mxnet/tree/support_transpose_dim_gt_6",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-657825425,wkcn,2020-07-13 22:29:59,18695,[18707],Data bug,1,"Hi @szha , I am trying to fix it and select 'Collapse consecutive axes of the input array into a single axis here when axes.ndim() > 2'. [url]"
"Hi @szha , I create a [`TransposeKernel`](https://github.com/wkcn/incubator-mxnet/blob/support_transpose_dim_gt_6/src/operator/tensor/matrix_op-inl.h#L324) and need to copy the argument (namely `axes`) into the target context. Is there any good way to copy it?

I try to call [`get_space_typed`](https://github.com/wkcn/incubator-mxnet/blob/support_transpose_dim_gt_6/src/operator/tensor/matrix_op-inl.h#L429) to alloc a workspace, but it need to declare a temporary space in each operator which calls `TransposeImpl`.

## Solution
I will create a new function `TransposeExImpl` to support up to 6 dimensions, which needs to alloc extra workspace. The original `TransposeImpl` will not be modified.",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-657934929,wkcn,2020-07-14 02:50:13,18695,[18707],Data bug,1,"Hi @szha , I create a [[code]]([url]#L324) and need to copy the argument (namely [code]) into the target context. Is there any good way to copy it? I try to call [[code]]([url]#L429) to alloc a workspace, but it need to declare a temporary space in each operator which calls [code]. ## Solution I will create a new function [code] to support up to 6 dimensions, which needs to alloc extra workspace. The original [code] will not be modified."
"I still get the issue with `mxnet-1.8.0.post`: 

```python
import mxnet.numpy as mnp

a = mnp.arange(2**7).reshape((2,)*7)
a = mnp.transpose(a)
```

raises the following issue:
```python
MXNetError: Traceback (most recent call last):
  File ""../src/operator/numpy/np_matrix_op.cc"", line 53
MXNetError: Check failed: shp.ndim() <= 6 (7 vs. 6) : Transpose support at most 6 dimensions
```",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-904141001,JeanKossaifi,2021-08-23 21:19:31,18695,[18707],Data bug,1,I still get the issue with [code]: ``[code]`[code]`[code]``
"@JeanKossaifi I think the change was not included in the 1.x but only in 2.0. The np support in 1.x is experimental, and if you plan on integrating with the np interface, it's probably a good idea to do so with 2.0 which includes all the latest fixes.",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-904226490,szha,2021-08-24 00:26:50,18695,[18707],Data bug,1,"@JeanKossaifi I think the change was not included in the 1.x but only in 2.0. The np support in 1.x is experimental, and if you plan on integrating with the np interface, it's probably a good idea to do so with 2.0 which includes all the latest fixes."
"Thanks @szha -- what's the recommended way to install mxnet 2.0? 
I'm looking mostly for a pypi version for the CI. I tried `pip install mxnet==2.0.0a0` but I get an error: 
```
ERROR: Could not find a version that satisfies the requirement mxnet==2.0.0a0 (from versions: 1.6.0, 1.7.0.post1, 1.7.0.post2, 1.8.0.post0)
ERROR: No matching distribution found for mxnet==2.0.0a0
```
",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-904580889,JeanKossaifi,2021-08-24 12:08:45,18695,[18707],Data bug,1,Thanks @szha -- what's the recommended way to install mxnet 2.0? I'm looking mostly for a pypi version for the CI. I tried [code] but I get an error: ``[code]``
@JeanKossaifi you can use the nightly version for development and testing purpose from https://dist.mxnet.io/python,IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-905603367,szha,2021-08-25 15:20:52,18695,[18707],Data bug,1,@JeanKossaifi you can use the nightly version for development and testing purpose from [url]
Thanks! Which version should we use for CI (CPU only)? Is there a way to directly pip install (the latest) version from there? ,IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-907488399,JeanKossaifi,2021-08-27 21:33:22,18695,[18707],Data bug,1,Thanks! Which version should we use for CI (CPU only)? Is there a way to directly pip install (the latest) version from there?
"It would be
```
pip install mxnet<3 --pre -f https://dist.mxnet.io/python
```",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-907721986,szha,2021-08-29 03:15:32,18695,[18707],Data bug,1,It would be ``[code]``
"@szha when I run 
```pip install mxnet<3 --pre -f https://dist.mxnet.io/python;```, 
I get the error: `/home/runner/work/_temp/41bba640-2fbf-4c14-beef-a62339f5a353.sh: line 16: 3: No such file or directory` 

I did install `gfortran`. Am I missing something? 
Thanks! :) 
",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-1160849427,JeanKossaifi,2022-06-20 21:10:01,18695,[18707],Data bug,1,@szha when I run ``[code]`[code]/home/runner/work/_temp/41bba640-2fbf-4c14-beef-a62339f5a353.sh: line 16: 3: No such file or directory[code]gfortran`. Am I missing something? Thanks! :)
"@JeanKossaifi Try this:
```bash
pip install mxnet --pre -f https://dist.mxnet.io/python
```
This will install the latest nightly build. As of today version `2.0.0b20220718`.",IssueComment,https://github.com/apache/mxnet/issues/18695#issuecomment-1188223249,merajhashemi,2022-07-18 19:40:02,18695,[18707],Data bug,1,@JeanKossaifi Try this: ``[code]`[code]2.0.0b20220718`.
@leezu how would you like this module documented?,IssueComment,https://github.com/apache/mxnet/issues/18725#issuecomment-659126473,szha,2020-07-16 02:54:42,18725,[18733],Documentation bug,0,@leezu how would you like this module documented?
The module should be documented in the same way as any other module on the website. The issue here is that the website build is not tested in PRs and thus it's no surprise to find various parts of the build break. Can we prioritize fixing the root-cause? https://github.com/apache/incubator-mxnet/issues/18732,IssueComment,https://github.com/apache/mxnet/issues/18725#issuecomment-659573454,leezu,2020-07-16 17:54:58,18725,[18733],Documentation bug,0,The module should be documented in the same way as any other module on the website. The issue here is that the website build is not tested in PRs and thus it's no surprise to find various parts of the build break. Can we prioritize fixing the root-cause? [url]
"I'm sure it's cased by
https://github.com/apache/incubator-mxnet/pull/17767",IssueComment,https://github.com/apache/mxnet/issues/18170#issuecomment-619503074,chinakook,2020-04-26 07:43:43,18170,[18761],Build bug,0,I'm sure it's cased by [url]
Are you sure you have a clean build environment? When developing that PR I did see that there are some problems with dependency tracking for some of the files touched by it (not caused by the PR) and I had to do a clean build to see all the changes made. Maybe it is a manifestation of the same problem?,IssueComment,https://github.com/apache/mxnet/issues/18170#issuecomment-620783473,ptrendx,2020-04-28 18:35:47,18170,[18761],Build bug,0,Are you sure you have a clean build environment? When developing that PR I did see that there are some problems with dependency tracking for some of the files touched by it (not caused by the PR) and I had to do a clean build to see all the changes made. Maybe it is a manifestation of the same problem?
I'll try again.,IssueComment,https://github.com/apache/mxnet/issues/18170#issuecomment-620971607,chinakook,2020-04-29 03:32:34,18170,[18761],Build bug,0,I'll try again.
@ptrendx Thank for your reply. It's solved by using right version of gfortran in Ubuntu 20.04.,IssueComment,https://github.com/apache/mxnet/issues/18170#issuecomment-628999182,chinakook,2020-05-15 03:02:52,18170,[18761],Build bug,0,@ptrendx Thank for your reply. It's solved by using right version of gfortran in Ubuntu 20.04.
Solved. Change the last row to ```print([x.asnumpy().shape for x in b])```.,IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-661750018,chinakook,2020-07-21 09:37:22,18765,[18768],Processor bug,0,Solved. Change the last row to ``[code]``.
@chinakook thanks for providing a workaround. I think it's still a bug,IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662027411,leezu,2020-07-21 18:20:04,18765,[18768],Processor bug,0,@chinakook thanks for providing a workaround. I think it's still a bug
"Simpler reproducible example for latest master:

```
import mxnet as mx
from mxnet import gluon
from mxnet.gluon import nn

a = mx.nd.random.uniform(shape=(1,3,224,224))

backbone = gluon.model_zoo.vision.resnet18_v1()
backbone.initialize()
backbone.hybridize()

backbone(a)

# Alternative:
# backbone.reset_ctx(mx.gpu(0))
# b = backbone(a.as_in_context(mx.gpu(0)))
# print([x.shape for x in b])

sym_file, params_file = backbone.export('/tmp/model')

f = gluon.SymbolBlock.imports(sym_file, 'data', params_file)
f.reset_ctx(mx.gpu(0))
b = f(a.as_in_context(mx.gpu(0)))
print([x.shape for x in b])
```

It fails with

```
[18:59:34] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for CPU
/home/ubuntu/src/mxnet-master/python/mxnet/gluon/block.py:1723: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:
        data: None
  input_sym_arg_type = in_param.infer_type()[0]
[18:59:37] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for GPU
[(1000,)]
[18:59:38] ../src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)

Segmentation fault: 11

zsh: abort (core dumped)  python3 symbolblockbug.py
```


However, the following will work:
```

import mxnet as mx
from mxnet import gluon
from mxnet.gluon import nn

a = mx.nd.random.uniform(shape=(1,3,224,224))

backbone = gluon.model_zoo.vision.resnet18_v1()
backbone.initialize()
backbone.hybridize()

# backbone(a)

# Alternative:
backbone.reset_ctx(mx.gpu(0))
b = backbone(a.as_in_context(mx.gpu(0)))
print([x.shape for x in b])

sym_file, params_file = backbone.export('/tmp/model')

f = gluon.SymbolBlock.imports(sym_file, 'data', params_file)
f.reset_ctx(mx.gpu(0))
b = f(a.as_in_context(mx.gpu(0)))
print([x.shape for x in b])
```",IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662047865,leezu,2020-07-21 19:00:16,18765,[18768],Processor bug,0,Simpler reproducible example for latest master: ``[code]`[code]`[code]`[code]`[code]``
"Backtrace

```
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff70608b1 in __GI_abort () at abort.c:79
#2  0x00007ffff70a9907 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7ffff71d6dfa ""%s\n"") at ../sysdeps/posix/libc_fatal.c:181
#3  0x00007ffff70b097a in malloc_printerr (str=str@entry=0x7ffff71d4fe8 ""free(): invalid pointer"") at malloc.c:5350
#4  0x00007ffff70b7e8c in _int_free (have_lock=0, p=0x7ffcc0a783d8, av=0x7ffff740bc40 <main_arena>) at malloc.c:4157
#5  __GI___libc_free (mem=0x7ffcc0a783e8) at malloc.c:3124
#6  0x00007fff461d3d46 in __gnu_cxx::new_allocator<char>::deallocate (this=0x7ffd81ffde10, __p=0x7ffcc0a783e8 """") at /usr/include/c++/7/ext/new_allocator.h:125
#7  0x00007fff461d2dc3 in std::allocator_traits<std::allocator<char> >::deallocate (__a=..., __p=0x7ffcc0a783e8 """", __n=305) at /usr/include/c++/7/bits/alloc_traits.h:462
#8  0x00007fff461d2178 in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_destroy (this=0x7ffd81ffde10, __size=304)
    at /usr/include/c++/7/bits/basic_string.h:226
#9  0x00007fff461d187a in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_dispose (this=0x7ffd81ffde10)
    at /usr/include/c++/7/bits/basic_string.h:221
#10 0x00007fff461d0318 in std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string (this=0x7ffd81ffde10, __in_chrg=<optimized out>)
    at /usr/include/c++/7/bits/basic_string.h:647
#11 0x00007fff5204c784 in mxnet::FusedOp::CompileCode (this=0x555559e922d0,
    code=""using DType_output0 = float;\nstatic const int ndim_output0 = 4;\nstatic const int ndim_input_1 = 4;\nusing DType_input_1 = float;\nstatic const int ndim_input_0 = 4;\nusing DType_input_0 = float;\nstatic c""..., kernel_name=""elemwise_add_Activation"", dev_id=0) at ../src/operator/fusion/fused_op.cu:651
#12 0x00007fff5204df75 in mxnet::FusedOp::Forward<mshadow::gpu> (this=0x555559e922d0, attrs=..., ctx=..., inputs=std::vector of length 2, capacity 2 = {...},
    req=std::vector of length 1, capacity 1 = {...}, outputs=std::vector of length 1, capacity 1 = {...}) at ../src/operator/fusion/fused_op.cu:766
#13 0x00007fff5204eac1 in mxnet::FusedOpForwardGPU (attrs=..., ctx=..., inputs=std::vector of length 2, capacity 2 = {...}, req=std::vector of length 1, capacity 1 = {...},
    outputs=std::vector of length 1, capacity 1 = {...}) at ../src/operator/fusion/fused_op.cu:836
#14 0x00007fff466b3bfb in std::_Function_handler<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&), void (*)(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::_M_invoke(std::_Any_data const&, nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) (__functor=..., __args#0=..., __args#1=...,
    __args#2=std::vector of length 2, capacity 2 = {...}, __args#3=std::vector of length 1, capacity 1 = {...}, __args#4=std::vector of length 1, capacity 1 = {...})
    at /usr/include/c++/7/bits/std_function.h:316
#15 0x00007fff464f09b4 in std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::operator()(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) const (this=0x555559e6eb20, __args#0=..., __args#1=..., __args#2=std::vector of length 2, capacity 2 = {...}, __args#3=std::vector of length 1, capacity 1 = {...},
    __args#4=std::vector of length 1, capacity 1 = {...}) at /usr/include/c++/7/bits/std_function.h:706
#16 0x00007fff4656a8d6 in mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const (__closure=0x555559e6ea90, rctx=...) at ../src/imperative/./imperative_utils.h:494
#17 0x00007fff46572f70 in std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::Op---Type <return> to continue, or q <return>
to quit---
:engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&) (__functor=...,
    __args#0=...) at /usr/include/c++/7/bits/std_function.h:316
#18 0x00007fff464bd868 in std::function<void (mxnet::RunContext)>::operator()(mxnet::RunContext) const (this=0x555555edb1e0, __args#0=...) at /usr/include/c++/7/bits/std_function.h:706
#19 0x00007fff464c96f6 in mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (__closure=0x555559e374b0, ctx=..., on_complete=...)
    at ../src/engine/./threaded_engine.h:537
#20 0x00007fff464ce27b in std::_Function_handler<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&, mxnet::engine::CallbackOnComplete&&) (__functor=..., __args#0=..., __args#1=...) at /usr/include/c++/7/bits/std_function.h:316
#21 0x00007fff464be704 in std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (this=0x555557eedc60, __args#0=..., __args#1=...)
    at /usr/include/c++/7/bits/std_function.h:706
#22 0x00007fff464d70e4 in mxnet::engine::ThreadedEngine::ExecuteOprBlock (this=0x555556fb6980, run_ctx=..., opr_block=0x5555580bd9b0) at ../src/engine/./threaded_engine.h:381
#23 0x00007fff464dcc57 in mxnet::engine::ThreadedEnginePerDevice::GPUWorker<(dmlc::ConcurrentQueueType)0> (this=0x555556fb6980, ctx=..., is_copy_worker=false, block=0x5555570b5b40,
    ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...}) at ../src/engine/threaded_engine_perdevice.cc:272
#24 0x00007fff464d8baa in mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#4}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}::operator()(dmlc::ManualEvent) const (__closure=0x5555580e8e20,
    ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...}) at ../src/engine/threaded_engine_perdevice.cc:186
#25 0x00007fff464dfc9d in std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#4}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<dmlc::ManualEvent>&&) (__functor=..., __args#0=...) at /usr/include/c++/7/bits/std_function.h:316
#26 0x00007fff464e088f in std::function<void (std::shared_ptr<dmlc::ManualEvent>)>::operator()(std::shared_ptr<dmlc::ManualEvent>) const (this=0x555558040268, __args#0=std::shared_ptr<dmlc::ManualEvent> (empty) = {...})
    at /usr/include/c++/7/bits/std_function.h:706
#27 0x00007fff464ddfbf in std::__invoke_impl<void, std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::__invoke_other, std::function<void (std::shared_ptr<dmlc::ManualEvent>)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__f=...) at /usr/include/c++/7/bits/invoke.h:60
#28 0x00007fff464d9b7d in std::__invoke<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::function<void (std::shared_ptr<dmlc::ManualEvent>)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__fn=...)
    at /usr/include/c++/7/bits/invoke.h:95
#29 0x00007fff464e746b in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=0x555558040258) at /usr/include/c++/7/thread:234
#30 0x00007fff464e73d3 in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::operator()() (this=0x555558040258) at /usr/include/c++/7/thread:243
#31 0x00007fff464e7372 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > > >::_M_run() (this=0x555558040250) at /usr/include/c++/7/thread:186
#32 0x00007ffff070a6df in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#33 0x00007ffff7bbd6db in start_thread (arg=0x7ffd81fff700) at pthread_create.c:463
#34 0x00007ffff7141a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
```

@ptrendx do you think this is a bug in fused op or in the engine?",IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662126632,leezu,2020-07-21 21:55:15,18765,[18768],Processor bug,0,Backtrace ``[code]`` @ptrendx do you think this is a bug in fused op or in the engine?
does adding `mx.nd.waitall()` help? ,IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662163971,eric-haibin-lin,2020-07-21 23:52:51,18765,[18768],Processor bug,0,does adding [code] help?
"@eric-haibin-lin Yes, it does.",IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662199323,chinakook,2020-07-22 02:06:47,18765,[18768],Processor bug,0,"@eric-haibin-lin Yes, it does."
"As I've tested, https://github.com/apache/incubator-mxnet/pull/18768 can solve this problem.",IssueComment,https://github.com/apache/mxnet/issues/18765#issuecomment-662201096,chinakook,2020-07-22 02:13:47,18765,[18768],Processor bug,0,"As I've tested, [url] can solve this problem."
i think we can remove one of them,IssueComment,https://github.com/apache/mxnet/issues/18266#issuecomment-626486720,eric-haibin-lin,2020-05-11 05:56:07,18266,[18794],Code bug,0,i think we can remove one of them
@eric-haibin-lin do we need this?,IssueComment,https://github.com/apache/mxnet/issues/18730#issuecomment-659125045,szha,2020-07-16 02:49:12,18730,[18802],Documentation bug,0,@eric-haibin-lin do we need this?
https://github.com/apache/incubator-mxnet/issues/18730 ,IssueComment,https://github.com/apache/mxnet/issues/18730#issuecomment-664752383,eric-haibin-lin,2020-07-28 03:19:08,18730,[18802],Documentation bug,0,[url]
Looks like protobuf was missing. I think in the past we've been statically linking protobuf. @leezu is the change towards dynamic linking intentional?,IssueComment,https://github.com/apache/mxnet/issues/18840#issuecomment-667633789,szha,2020-08-02 06:09:14,18840,[18851],Build bug,0,Looks like protobuf was missing. I think in the past we've been statically linking protobuf. @leezu is the change towards dynamic linking intentional?
"Actually it's changed to static linkage @szha:

https://github.com/apache/incubator-mxnet/blob/51340d8f40721e03938e0021c6cbb557a44dd90e/CMakeLists.txt#L151-L157

The problem is though that there is a `AND NOT APPLE` in the `if` condition. Need to move the static linkage option to a separate if clause.

Previously we have not linked protobuf in the cmake build as ps-lite was disabled. ",IssueComment,https://github.com/apache/mxnet/issues/18840#issuecomment-668119673,leezu,2020-08-03 16:33:15,18840,[18851],Build bug,0,Actually it's changed to static linkage @szha: [url]#L151-L157 The problem is though that there is a [code] in the [code] condition. Need to move the static linkage option to a separate if clause. Previously we have not linked protobuf in the cmake build as ps-lite was disabled.
"This works:
```python
import mxnet as mx

net_fp32 = mx.gluon.model_zoo.vision.resnet34_v2(pretrained=True)
net_fp32.cast('float64')
net_fp32.hybridize()
data = mx.nd.zeros((1,3,224,224), dtype='float64')
net_fp32(data)
sym_file, params_file = net_fp32.export('test', 0)

sm = mx.sym.load(sym_file)
inputs = mx.sym.var('data', dtype='float64')
net_fp64 = mx.gluon.SymbolBlock(sm, inputs)
net_fp64.load_parameters(params_file)
```",IssueComment,https://github.com/apache/mxnet/issues/18843#issuecomment-667744248,szha,2020-08-03 00:14:37,18843,[18853],Deployment bug,0,This works: ``[code]``
"The issue is due to the symbol (and thus it's  `json` representation) not being updated after the cast:

```
% diff test-symbol.json test2-symbol.json                                                                                                                                                 ~ ip-172-31-95-96
13c13
<         ""__dtype__"": ""0"",
---
>         ""__dtype__"": ""1"",
16,17c16,17
<         ""__profiler_scope__"": ""<unk>:"",
<         ""__shape__"": ""(0,)"",
---
>         ""__profiler_scope__"": ""hybridsequential0:"",
>         ""__shape__"": ""(3,)"",
27c27
<         ""__dtype__"": ""0"",
---
>         ""__dtype__"": ""1"",
30,31c30,31
<         ""__profiler_scope__"": ""<unk>:"",
<         ""__shape__"": ""(0,)"",
---
>         ""__profiler_scope__"": ""hybridsequential0:"",
>         ""__shape__"": ""(3,)"",
41c41
<         ""__dtype__"": ""0"",
---
>         ""__dtype__"": ""1"",
44,45c44,45
<         ""__profiler_scope__"": ""<unk>:"",
<         ""__shape__"": ""(0,)"",
---
>         ""__profiler_scope__"": ""hybridsequential0:"",
>         ""__shape__"": ""(3,)"",
55c55
<         ""__dtype__"": ""0"",
---
>         ""__dtype__"": ""1"",
[...]
```",IssueComment,https://github.com/apache/mxnet/issues/18843#issuecomment-668161817,leezu,2020-08-03 18:00:34,18843,[18853],Deployment bug,0,The issue is due to the symbol (and thus it's [code] representation) not being updated after the cast: ``[code]``
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/18911#issuecomment-672572398,github-actions[bot],2020-08-12 04:55:12,18911,[18917],Data bug,1,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
Thanks for reporting this. @zhreshold mind taking a look?,IssueComment,https://github.com/apache/mxnet/issues/18911#issuecomment-672661747,szha,2020-08-12 07:06:24,18911,[18917],Data bug,1,Thanks for reporting this. @zhreshold mind taking a look?
"confirmed, will update once I rooted the cause",IssueComment,https://github.com/apache/mxnet/issues/18911#issuecomment-673155336,zhreshold,2020-08-12 23:16:15,18911,[18917],Data bug,1,"confirmed, will update once I rooted the cause"
looking into this in #18839 ,IssueComment,https://github.com/apache/mxnet/issues/18909#issuecomment-672292828,szha,2020-08-11 21:38:56,18909,[18924],Documentation bug,1,looking into this in #18839
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/18927#issuecomment-674298925,github-actions[bot],2020-08-14 22:36:02,18927,[18972],Data bug,1,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
See the above PR for the fix.,IssueComment,https://github.com/apache/mxnet/issues/18927#issuecomment-677838726,szha,2020-08-20 18:53:43,18927,[18972],Data bug,1,See the above PR for the fix.
"I can reproduce the problem. The error also occurs in 1.x with the numpy interface
```python
from mxnet import np, npx

npx.set_np()

input = np.random.rand(0,1,1)
npx.leaky_relu(input)
```",IssueComment,https://github.com/apache/mxnet/issues/18934#issuecomment-674438736,szha,2020-08-15 19:30:23,18934,[18996],Algorithm design bug,1,I can reproduce the problem. The error also occurs in 1.x with the numpy interface ``[code]``
We will take a look. cc @pengzhao-intel ,IssueComment,https://github.com/apache/mxnet/issues/18934#issuecomment-678062133,TaoLv,2020-08-21 06:15:30,18934,[18996],Algorithm design bug,1,We will take a look. cc @pengzhao-intel
"update: 
== != < > <= >= are all affected, but after the fix they seem to be working

The scalar side of those comparison operators used to be converted to int32. Now I changed them all to int64_t.

Note that this is not an indexing issue but rather a value issue. Also note that to have very large int tensors we need to set dtype to int64",IssueComment,https://github.com/apache/mxnet/issues/19006#issuecomment-679430098,Zha0q1,2020-08-25 00:17:36,19006,[19008],Data bug,1,"update: == != < > <= >= are all affected, but after the fix they seem to be working The scalar side of those comparison operators used to be converted to int32. Now I changed them all to int64_t. Note that this is not an indexing issue but rather a value issue. Also note that to have very large int tensors we need to set dtype to int64"
"Ok, I believe `inf` is generated by the ffi for `np.clip` here: https://github.com/apache/incubator-mxnet/blob/master/src/api/operator/tensor/matrix_op.cc#L52

I will make PR with support for fusion of `clip` without `a_min` or `a_max` parameters tomorrow.",IssueComment,https://github.com/apache/mxnet/issues/19026#issuecomment-682250717,ptrendx,2020-08-28 00:03:19,19026,[19035],Build bug,1,"Ok, I believe [code] is generated by the ffi for [code] here: [url]#L52 I will make PR with support for fusion of [code] without [code] or [code] parameters tomorrow."
The bug causes the training issue of MobileBERT in GluonNLP: https://github.com/dmlc/gluon-nlp/issues/1322,IssueComment,https://github.com/apache/mxnet/issues/19043#issuecomment-683371624,sxjscience,2020-08-30 03:34:57,19043,[19044],Algorithm design bug,1,The bug causes the training issue of MobileBERT in GluonNLP: [url]
"@ys2843 would it make sense to use a url schema like `https://mxnet.apache.org/versions/master/` for master docs too, so that it won't break the links?",IssueComment,https://github.com/apache/mxnet/issues/18913#issuecomment-683469846,szha,2020-08-30 20:58:53,18913,[19190],Visualization bug,1,"@ys2843 would it make sense to use a url schema like [code] for master docs too, so that it won't break the links?"
"I think the following line should be added to convert the model to FP16.
```bash
net = net.cast(""float16"")
```
",IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-691098222,kohillyang,2020-09-11 13:33:57,19118,[19270],Algorithm design bug,0,I think the following line should be added to convert the model to FP16. ``[code]``
"
Forgot to add the `cast` in the example. The error I met is as follows:

```python
import mxnet as mx
mx.npx.set_np()
net = mx.gluon.nn.Dense(16, in_units=16)
net.cast(""float16"")
net.initialize(ctx=mx.gpu())
net.hybridize()
net(mx.np.random.normal(0, 1, (16, 16), dtype=mx.np.float16, ctx=mx.gpu()))
```

Error:
```
MXNetError: Traceback (most recent call last):
  File ""../src/imperative/./imperative_utils.h"", line 306
MXNetError: Check failed: outputs[i]->dtype() == out_types[i] (2 vs. 0) : 0-th output has invalid dtype. Expecting 0 got 2 in operator _npi_uniform
```",IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-691213582,sxjscience,2020-09-11 17:11:53,19118,[19270],Algorithm design bug,0,Forgot to add the [code] in the example. The error I met is as follows: ``[code]`[code]`[code]``
"@kohillyang Sorry that I forgot the past the `cast` call when creating the issue, updated the code.",IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-691215805,sxjscience,2020-09-11 17:16:37,19118,[19270],Algorithm design bug,0,"@kohillyang Sorry that I forgot the past the [code] call when creating the issue, updated the code."
Hi @sxjscience I want to fix this issue. Please assign me. Thanks,IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-702469111,AnshuTrivedi,2020-10-02 00:46:08,19118,[19270],Algorithm design bug,0,Hi @sxjscience I want to fix this issue. Please assign me. Thanks
Thanks for the contribution @AnshuTrivedi . You may try to fix the initializers and add a test case in https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_numpy_gluon.py,IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-702469478,sxjscience,2020-10-02 00:47:43,19118,[19270],Algorithm design bug,0,Thanks for the contribution @AnshuTrivedi . You may try to fix the initializers and add a test case in [url]
"@sxjscience i'm facing difficulty in [test_numpy_gluon](https://github.com/apache/incubator-mxnet/blob/2d3ce934f8ef95940744a84c24ba389bf307bb5a/tests/python/unittest/test_numpy_gluon.py#L66).
I think test is writen ,what changes have to make there?
Please help me.",IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-702538179,AnshuTrivedi,2020-10-02 05:48:36,19118,[19270],Algorithm design bug,0,"@sxjscience i'm facing difficulty in [test_numpy_gluon]([url]#L66). I think test is writen ,what changes have to make there? Please help me."
Thanks @AnshuTrivedi and @szha . This is now closed.,IssueComment,https://github.com/apache/mxnet/issues/19118#issuecomment-705110341,sxjscience,2020-10-07 18:17:33,19118,[19270],Algorithm design bug,0,Thanks @AnshuTrivedi and @szha . This is now closed.
"This bug is related to legacy (non-numpy) reshape operator. Reshape with `0` implies ""copy this dimension from input"".

We can see that all shapes are correct prior to infer_shape pass:

```
frame #6: 0x00007fff3bd1cb8b libmxnet.so`mxnet::imperative::SetShapeType(ctx=0x00007fffffff8f18, attrs=0x00007fffffff97a8, inputs=size=1, outputs=size=1, dispatch_mode=0x00007fffffff8f24) at imperative_utils.h:208:26
   205        common::ConvertToNumpyShape(&in_shapes);
   206        common::ConvertToNumpyShape(&out_shapes);
   207      }
-> 208      const bool success = infershape[attrs.op](attrs, &in_shapes, &out_shapes);
   209      if (!success) {
   210        std::stringstream os;
   211        os << ""Operator "" << attrs.op->name << "" inferring shapes failed.\n"";
(lldb) parray 5 in_shapes[0].data_heap_
(long *) $15 = 0x0000555555f45700 {
  (long) [0] = 1
  (long) [1] = 2
  (long) [2] = 4
  (long) [3] = 0
  (long) [4] = 128
}
(lldb) p out_shapes                                                                                                                                                                    (mxnet::ShapeVector) $16 = size=1 {
  [0] = {
    mxnet::Tuple<long> = {
      ndim_ = 4
      num_heap_allocated_ = 0
      data_stack_ = ([0] = 2, [1] = 4, [2] = 0, [3] = 128)
      data_heap_ = 0x0000000000000000
    }
  }
}
```

But after infer_shape, `0` is replaced by `4`.

```
frame #1: 0x00007fff4968c1be libmxnet.so`mxnet::op::ReshapeShape(attrs=0x00007fffffff97a8, in_attrs=0x00005555569cbb68, out_attrs=0x00005555569cbb80) at matrix_op-inl.h:235:3
   232      << ""Target: "" << oshape
   233      << ""\nSource: "" << dshape;
   234  #endif
-> 235    SHAPE_ASSIGN_CHECK(*out_attrs, 0, oshape);
   236    return ReverseReshapeInferShape(&(*in_attrs)[0], (*out_attrs)[0]);
   237  }
   238
(lldb) p oshape
(mxnet::TShape) $17 = {
  mxnet::Tuple<long> = {
    ndim_ = 4
    num_heap_allocated_ = 0
    data_stack_ = ([0] = 2, [1] = 4, [2] = 4, [3] = 128)
    data_heap_ = 0x0000000000000000
  }
}
```

https://github.com/apache/incubator-mxnet/blob/f732530c8e1f8dcc11134e935430e3793ddbf4c8/src/operator/tensor/matrix_op-inl.h#L194-L237

The root cause is that `MXNDArrayAt`, `MXNDArrayReshape` and `MXNDArraySlice` do not sufficiently distinguish between the numpy and non-numpy mode and always record legacy operators.

We need to update the recording step to record numpy / legacy operator based on if numpy / legacy mode is enabled:

 https://github.com/apache/incubator-mxnet/blob/72eff9b66ecc683c3e7f9ad2c0ba69efa8dd423b/src/ndarray/ndarray.cc#L302-L308 ",IssueComment,https://github.com/apache/mxnet/issues/19286#issuecomment-703816274,leezu,2020-10-05 18:41:23,19286,[19293],Version compatibility bug,1,"This bug is related to legacy (non-numpy) reshape operator. Reshape with [code] implies ""copy this dimension from input"". We can see that all shapes are correct prior to infer_shape pass: ``[code]mxnet::imperative::SetShapeType(ctx=0x00007fffffff8f18, attrs=0x00007fffffff97a8, inputs=size=1, outputs=size=1, dispatch_mode=0x00007fffffff8f24) at imperative_utils.h:208:26 205 common::ConvertToNumpyShape(&in_shapes); 206 common::ConvertToNumpyShape(&out_shapes); 207 } -> 208 const bool success = infershape[attrs.op](attrs, &in_shapes, &out_shapes); 209 if (!success) { 210 std::stringstream os; 211 os << ""Operator "" << attrs.op->name << "" inferring shapes failed.\n""; (lldb) parray 5 in_shapes[0].data_heap_ (long *) $15 = 0x0000555555f45700 { (long) [0] = 1 (long) [1] = 2 (long) [2] = 4 (long) [3] = 0 (long) [4] = 128 } (lldb) p out_shapes (mxnet::ShapeVector) $16 = size=1 { [0] = { mxnet::Tuple<long> = { ndim_ = 4 num_heap_allocated_ = 0 data_stack_ = ([0] = 2, [1] = 4, [2] = 0, [3] = 128) data_heap_ = 0x0000000000000000 } } } ``[code]0[code]4[code]`[code]mxnet::op::ReshapeShape(attrs=0x00007fffffff97a8, in_attrs=0x00005555569cbb68, out_attrs=0x00005555569cbb80) at matrix_op-inl.h:235:3 232 << ""Target: "" << oshape 233 << ""\nSource: "" << dshape; 234 #endif -> 235 SHAPE_ASSIGN_CHECK(*out_attrs, 0, oshape); 236 return ReverseReshapeInferShape(&(*in_attrs)[0], (*out_attrs)[0]); 237 } 238 (lldb) p oshape (mxnet::TShape) $17 = { mxnet::Tuple<long> = { ndim_ = 4 num_heap_allocated_ = 0 data_stack_ = ([0] = 2, [1] = 4, [2] = 4, [3] = 128) data_heap_ = 0x0000000000000000 } } ``[code]MXNDArrayAt[code]MXNDArrayReshape[code]MXNDArraySlice` do not sufficiently distinguish between the numpy and non-numpy mode and always record legacy operators. We need to update the recording step to record numpy / legacy operator based on if numpy / legacy mode is enabled: [url]#L302-L308"
You can adapt https://github.com/apache/incubator-mxnet/blob/af2b4bcecfbf52df3a833974651eb75301d8f860/python/mxnet/gluon/nn/conv_layers.py#L156-L175 or overwrite the `__repr__` method of Conv2dTranspose class in the same file,IssueComment,https://github.com/apache/mxnet/issues/19338#issuecomment-707378679,leezu,2020-10-12 22:34:30,19338,[19344],Code bug,0,You can adapt [url]#L156-L175 or overwrite the [code] method of Conv2dTranspose class in the same file
We proposed a PR to fix the __repr__ function for ConvTranspose classes and a small test.,IssueComment,https://github.com/apache/mxnet/issues/19338#issuecomment-707661400,gilbertfrancois,2020-10-13 10:55:40,19338,[19344],Code bug,0,We proposed a PR to fix the __repr__ function for ConvTranspose classes and a small test.
"We recently saw this issue too and I am looking for a fix now. I do not believe it is CUDA 11 specific, rather code layout/timing/environment specific - e.g. in our setup we did not see this issue on Ubuntu 18.04 but encounter it on 20.04. The problem is that MXNet does not actually wait for the side thread to finish before the program teardown. During the main thread teardown CUDA deinitializes itself. If the side thread is still running at this point and tries to destroy its mshadow stream, this calls `cudnnDestroy` on the cuDNN handle, which internally calls `cudaStreamDestroy` on cuDNN internal CUDA streams (CUDA is statically linked in cuDNN, which is why you see your segfault coming from `libcudnn_ops_infer.so.8`). When this call is done after the CUDA deinitialization, crash happens.

I started looking at this yesterday - brief look at the destructors seems to imply that `join` should actually be called on the side threads, so not yet sure why this does not actually do the right thing. If anyone has more experience with the internals of the `ThreadedEnginePerDevice` I would be happy to leave that issue to them, but poking in the meantime.",IssueComment,https://github.com/apache/mxnet/issues/19360#issuecomment-710129655,ptrendx,2020-10-16 15:52:31,19360,[19378],Test bug,0,"We recently saw this issue too and I am looking for a fix now. I do not believe it is CUDA 11 specific, rather code layout/timing/environment specific - e.g. in our setup we did not see this issue on Ubuntu 18.04 but encounter it on 20.04. The problem is that MXNet does not actually wait for the side thread to finish before the program teardown. During the main thread teardown CUDA deinitializes itself. If the side thread is still running at this point and tries to destroy its mshadow stream, this calls [code] on the cuDNN handle, which internally calls [code] on cuDNN internal CUDA streams (CUDA is statically linked in cuDNN, which is why you see your segfault coming from [code]). When this call is done after the CUDA deinitialization, crash happens. I started looking at this yesterday - brief look at the destructors seems to imply that [code] should actually be called on the side threads, so not yet sure why this does not actually do the right thing. If anyone has more experience with the internals of the [code] I would be happy to leave that issue to them, but poking in the meantime."
"Ok, so I think I understand this issue more - the problem is that `shared_ptr` to the engine is a static variable here: https://github.com/apache/incubator-mxnet/blob/master/src/engine/engine.cc#L62 and so the destruction timing of the engine itself is not specified (depends on the order of binaries in the linked executable). This makes it possible for CUDA deinitialization to happen before or after the destruction of the engine. If it happens after then everything is OK, because as part of its destruction engine actually joins on the side threads. However, if the CUDA deinit happens before, then side thread doing the cleanup actually triggers the segfault.

The easiest workaround would be to just skip cleanup on a side thread - @szha @mseth10 @leezu do you think that would be acceptable? Any other ideas?",IssueComment,https://github.com/apache/mxnet/issues/19360#issuecomment-710476208,ptrendx,2020-10-16 19:27:19,19360,[19378],Test bug,0,"Ok, so I think I understand this issue more - the problem is that [code] to the engine is a static variable here: [url]#L62 and so the destruction timing of the engine itself is not specified (depends on the order of binaries in the linked executable). This makes it possible for CUDA deinitialization to happen before or after the destruction of the engine. If it happens after then everything is OK, because as part of its destruction engine actually joins on the side threads. However, if the CUDA deinit happens before, then side thread doing the cleanup actually triggers the segfault. The easiest workaround would be to just skip cleanup on a side thread - @szha @mseth10 @leezu do you think that would be acceptable? Any other ideas?"
"As a short-term solution it's ok. At some point, we may benefit from being able to destruct engine properly at runtime other than at exit. For example, this could enable switching the engine at runtime. Thus, it would still be better if we have an actual solution for destruction order.",IssueComment,https://github.com/apache/mxnet/issues/19360#issuecomment-712361751,szha,2020-10-19 18:28:47,19360,[19378],Test bug,0,"As a short-term solution it's ok. At some point, we may benefit from being able to destruct engine properly at runtime other than at exit. For example, this could enable switching the engine at runtime. Thus, it would still be better if we have an actual solution for destruction order."
"Ok, I will open then a PR with the workaround and let's open an issue for the better handling of the destruction order of the engine.",IssueComment,https://github.com/apache/mxnet/issues/19360#issuecomment-712395017,ptrendx,2020-10-19 19:32:39,19360,[19378],Test bug,0,"Ok, I will open then a PR with the workaround and let's open an issue for the better handling of the destruction order of the engine."
"Thanks for the code to reproduce @bgawrych, I can reproduce on teh v1.8.x branch. will take a look",IssueComment,https://github.com/apache/mxnet/issues/19446#issuecomment-719660322,samskalicky,2020-10-30 16:36:10,19446,[19455],Processor bug,0,"Thanks for the code to reproduce @bgawrych, I can reproduce on teh v1.8.x branch. will take a look"
"After checking with @leezu it looks like we need to do something like:
```
# Partition the graph.                                                                                                               
out = out.optimize_for(self._backend, arg_dict, aux_dict, ctx, **self._backend_opts)

# convert to numpy symbol if needed                                                                                                  
if _mx_npx.is_np_array():
    out = out.as_np_ndarray()

#update cached graph with partitioned graph                                                                                          
self._cached_graph = data, out
```
here where we call `optimize_for` in the Gluon block:
https://github.com/apache/incubator-mxnet/blob/0faecf01eddc781793927cb7cb9415310e9b21a2/python/mxnet/gluon/block.py#L1039
With this change it seems to be working. I'll create a PR with this on v1.x and master branches. I guess there hasnt been any testing of Numpy functionality with optimize_for yet....",IssueComment,https://github.com/apache/mxnet/issues/19446#issuecomment-719679279,samskalicky,2020-10-30 17:08:45,19446,[19455],Processor bug,0,After checking with @leezu it looks like we need to do something like: ``[code]`[code]optimize_for` in the Gluon block: [url]#L1039 With this change it seems to be working. I'll create a PR with this on v1.x and master branches. I guess there hasnt been any testing of Numpy functionality with optimize_for yet....
"It's ""by design"" as you can't call the new API from the old API: Calling a HybridBock.forward block from HybridBlock.hybrid_forward is not supported. Note that all the MXNet provided HybridBlocks currently implement `hybrid_forward` for compatibility.

We can add temporary hack to HybridSequential to support this usecase if needed, as HybridSequential only passes through the arguments. In general HybridSequential will be switched to the new API together with all the other HybridBlocks in the near future.",IssueComment,https://github.com/apache/mxnet/issues/18593#issuecomment-646433798,leezu,2020-06-19 04:54:01,18593,[19470],Version compatibility bug,0,"It's ""by design"" as you can't call the new API from the old API: Calling a HybridBock.forward block from HybridBlock.hybrid_forward is not supported. Note that all the MXNet provided HybridBlocks currently implement [code] for compatibility. We can add temporary hack to HybridSequential to support this usecase if needed, as HybridSequential only passes through the arguments. In general HybridSequential will be switched to the new API together with all the other HybridBlocks in the near future."
"@leezu 
Thanks for your response, I am not in urgent need of this usecase.
But I believe it's crucial to let HybridSequential support the new API.",IssueComment,https://github.com/apache/mxnet/issues/18593#issuecomment-646442812,xidulu,2020-06-19 05:28:30,18593,[19470],Version compatibility bug,0,"@leezu Thanks for your response, I am not in urgent need of this usecase. But I believe it's crucial to let HybridSequential support the new API."
Removed the `needs triage` because @mk-61 is able to reproduce the issue per offline discussion.,IssueComment,https://github.com/apache/mxnet/issues/19463#issuecomment-721434426,sxjscience,2020-11-03 23:49:13,19463,[19488],Data bug,1,Removed the [code] because @mk-61 is able to reproduce the issue per offline discussion.
"> I think this is causing MXNet to use the slow CentOS OpenMP instead of the bundled support.

Another hypothesis is that the slowdown results from an interaction of the llvm openmp linked by mxnet + the gcc 7 openmp (?) provided by devtoolset 7 on centos and used by intgemm. Let's try if the issue persists when only linking one openmp implementation.",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724128401,leezu,2020-11-09 16:37:48,19502,[19507],Performance bug,1,> I think this is causing MXNet to use the slow CentOS OpenMP instead of the bundled support. Another hypothesis is that the slowdown results from an interaction of the llvm openmp linked by mxnet + the gcc 7 openmp (?) provided by devtoolset 7 on centos and used by intgemm. Let's try if the issue persists when only linking one openmp implementation.
"The Centos 7 OpenMP performance is terrible. 

To test @leezu 's hypothesis, I created a branch https://github.com/kpuatamazon/incubator-mxnet/tree/removeopenmp without the 3rdparty OpenMP.  This means there should only be the OS-/compiler-provided OpenMP.  

The same test above took 542.95s with intgemm's CMakeLists.txt as is, and 539.99s with my OpenMP commented out like below.  No real difference here.  
```
#option(USE_OPENMP ""Use OpenMP"" OFF)
#if (USE_OPENMP)
#  message(STATUS ""Compiling with OpenMP"")
#  find_package(OpenMP)
#  if (NOT ${OpenMP_CXX_FOUND})
#    message(SEND_ERROR ""OpenMP requested but C++ support not found"")
#  endif()
#  add_compile_options(${OpenMP_CXX_FLAGS})
#  target_link_libraries(intgemm PUBLIC OpenMP::OpenMP_CXX)
#endif()
```",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724176697,kpuatamazon,2020-11-09 18:00:02,19502,[19507],Performance bug,1,"The Centos 7 OpenMP performance is terrible. To test @leezu 's hypothesis, I created a branch [url] without the 3rdparty OpenMP. This means there should only be the OS-/compiler-provided OpenMP. The same test above took 542.95s with intgemm's CMakeLists.txt as is, and 539.99s with my OpenMP commented out like below. No real difference here. ``[code]``"
"Thank you for testing this @kpuatamazon! If the slowdown is solely due to the CentOS 7 OpenMP, it implies that we should also enable 3rdparty/openmp in the static builds distributed by others on pypi etc. It was previously disabled for consistency with the old Makefile staticbuild.

https://github.com/apache/incubator-mxnet/blob/564c6d307e3439c1e5bb9bbd7e82d6744bea6a83/CMakeLists.txt#L412

Further, if the slowdown is due to the CentOS 7 OpenMP and unrelated to intgemm, I would expect the ""slow test"" issue to be present in the Jenkins CD pipeline both before and after intgemm was merged.",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724179407,leezu,2020-11-09 18:04:58,19502,[19507],Performance bug,1,"Thank you for testing this @kpuatamazon! If the slowdown is solely due to the CentOS 7 OpenMP, it implies that we should also enable 3rdparty/openmp in the static builds distributed by others on pypi etc. It was previously disabled for consistency with the old Makefile staticbuild. [url]#L412 Further, if the slowdown is due to the CentOS 7 OpenMP and unrelated to intgemm, I would expect the ""slow test"" issue to be present in the Jenkins CD pipeline both before and after intgemm was merged."
"On the CD test pipeline (which uses the system provided openmp), I observe on Aug 30 2020:

```
[2020-08-30T20:36:17.326Z] 20.44s call     tests/python/unittest/test_gluon.py::test_slice_pooling2d_slice_pooling2d
```
https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1633/nodes/250/steps/288/log/?start=0


and on Sep 1 (after intgemm was merged)

```
[2020-09-01T20:22:31.173Z] 19.14s call     tests/python/unittest/test_gluon.py::test_slice_pooling2d_slice_pooling2d
```
https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1643/nodes/253/steps/288/log/?start=0


I checked the [respective build logs](https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1633/nodes/66/steps/173/log/?start=0), and they do not contain the string `3rdparty/openmp`. They only list

```
[2020-08-30T19:58:43.183Z] -- Found OpenMP_C: -fopenmp (found version ""4.5"") 
[2020-08-30T19:58:43.183Z] -- Found OpenMP_CXX: -fopenmp (found version ""4.5"") 
[2020-08-30T19:58:43.183Z] -- Found OpenMP: TRUE (found version ""4.5"")  
```

meaning that the CentOS 7 system openmp is used.",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724189109,leezu,2020-11-09 18:22:22,19502,[19507],Performance bug,1,"On the CD test pipeline (which uses the system provided openmp), I observe on Aug 30 2020: ``[code]`[code]`[code]`[code]3rdparty/openmp[code]`[code]`` meaning that the CentOS 7 system openmp is used."
"I can reproduce the difference in perforamce between CD CentOS setup and CI CentOS setup locally.
The test runs in ~20 seconds in the CD setup and takes much longer on the CI setup.
Both use the system provided openmp.

For CD setup, I use 
```
rm -rf lib build; python ci/build.py --cache-intermediate --platform centos7_cpu /work/runtime_functions.sh build_static_libmxnet cpu
```

and for CI setup I use

```
rm -rf lib build; ci/build.py --docker-registry mxnetci --platform centos7_cpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh build_centos7_cpu
```

In both cases I have modified the cmake file to remove intgemm:

``` diff
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 07075d752..8bfd03f53 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -295,12 +295,6 @@ if(USE_MKLDNN)
   set_target_properties(dnnl PROPERTIES CXX_CLANG_TIDY """")  # don't lint 3rdparty dependency
 endif()

-if(USE_INTGEMM)
-  message(STATUS ""Using intgemm"")
-  add_subdirectory(3rdparty/intgemm EXCLUDE_FROM_ALL)
-  add_definitions(-DMXNET_USE_INTGEMM=1)
-endif()
-
 # Allow Cuda compiles outside of src tree to find things in 'src' and 'include'
 include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)
 include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src)
@@ -509,10 +503,8 @@ endif()
 FILE(GLOB_RECURSE SOURCE ""src/*.cc"" ""src/*.h"" ""include/*.h"")
 FILE(GLOB_RECURSE CUDA ""src/*.cu"" ""src/*.cuh"")

-if(NOT USE_INTGEMM)
   FILE(GLOB_RECURSE INTGEMM_OPERATOR_SOURCE ""src/operator/contrib/intgemm/*.cc"" ""src/operator/contrib/intgemm/*.h"")
   list(REMOVE_ITEM SOURCE ${INTGEMM_OPERATOR_SOURCE})
-endif()

 # add nnvm to source
 FILE(GLOB_RECURSE NNVMSOURCE
@@ -840,9 +832,6 @@ if(USE_MKLDNN)
       ${CMAKE_BINARY_DIR}/3rdparty/mkldnn/include/dnnl_version.h  ${CMAKE_SOURCE_DIR}/include/mkldnn/)
 endif()

-if(USE_INTGEMM)
-  target_link_libraries(mxnet PRIVATE intgemm)
-endif()

 function(BuildTVMOP)
   # scope the variables in BuildTVM.cmake to avoid conflict
```

and deleted the `3rdparty/openmp` folder.
",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724212516,leezu,2020-11-09 19:02:16,19502,[19507],Performance bug,1,"I can reproduce the difference in perforamce between CD CentOS setup and CI CentOS setup locally. The test runs in ~20 seconds in the CD setup and takes much longer on the CI setup. Both use the system provided openmp. For CD setup, I use ``[code]`[code]`[code]`[code]`[code]`[code]3rdparty/openmp` folder."
"@kpuatamazon removing the test coverage instrumentations from the centos build brings the test back to `24.18s`.

I suspect that the test coverage instrumentation would never be part of llvm OpenMP (3rdparty/openmp) due to
https://github.com/apache/incubator-mxnet/blob/551a8d3e94cfa033a933a5aaafafcc0b23ecedcf/CMakeLists.txt#L401

However, as intgemm pulls in the system openmp the coverage instrumentations starts slowing down the tests.
I'm not yet sure why the same doesn't apply to the ubuntu cpu tests, which also include intgemm and enable test coverage instrumentations.",IssueComment,https://github.com/apache/mxnet/issues/19502#issuecomment-724316393,leezu,2020-11-09 22:28:17,19502,[19507],Performance bug,1,"@kpuatamazon removing the test coverage instrumentations from the centos build brings the test back to [code]. I suspect that the test coverage instrumentation would never be part of llvm OpenMP (3rdparty/openmp) due to [url]#L401 However, as intgemm pulls in the system openmp the coverage instrumentations starts slowing down the tests. I'm not yet sure why the same doesn't apply to the ubuntu cpu tests, which also include intgemm and enable test coverage instrumentations."
"Thanks for reporting the issue @sxjscience . This issue happens during training when the model is hybridized. It is a known bug #18823 with MXNet that we are currently working on. Meanwhile, what we can do is disable the graph partitioning by default. Will create a PR shortly.",IssueComment,https://github.com/apache/mxnet/issues/19524#issuecomment-726306783,mseth10,2020-11-12 19:56:50,19524,[19525],Algorithm design bug,0,"Thanks for reporting the issue @sxjscience . This issue happens during training when the model is hybridized. It is a known bug #18823 with MXNet that we are currently working on. Meanwhile, what we can do is disable the graph partitioning by default. Will create a PR shortly."
keeping this open for now to make sure the case is tested when actual fix is added.,IssueComment,https://github.com/apache/mxnet/issues/19524#issuecomment-726416232,szha,2020-11-13 00:00:23,19524,[19525],Algorithm design bug,0,keeping this open for now to make sure the case is tested when actual fix is added.
@szha Would it be a 2.0-release issue?,IssueComment,https://github.com/apache/mxnet/issues/19524#issuecomment-726416431,sxjscience,2020-11-13 00:01:03,19524,[19525],Algorithm design bug,0,@szha Would it be a 2.0-release issue?
Fixed by #19614 ,IssueComment,https://github.com/apache/mxnet/issues/19524#issuecomment-742276784,mseth10,2020-12-10 06:39:00,19524,[19525],Algorithm design bug,0,Fixed by #19614
"Yet another issue : unrelated PR #18206 
```
curl: (7) Failed to connect to releases.llvm.org port 443: Connection timed out
```
unix-cpu
http://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/mxnet-validation/pipelines/unix-cpu/branches/PR-18206/runs/7/nodes/47/log/?start=0

unix-gpu
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-gpu/detail/PR-18206/6/pipeline",IssueComment,https://github.com/apache/mxnet/issues/18229#issuecomment-624378158,ChaiBapchya,2020-05-06 00:27:51,18229,[19589],Build bug,0,Yet another issue : unrelated PR #18206 ``[code]`` unix-cpu [url] unix-gpu [url]
"Another one #18261 

http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-gpu/detail/PR-18261/1/pipeline

http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Funix-cpu/detail/PR-18261/1/pipeline

@leezu 
clang failed but not sure if its related to this issue
Is this 500 internal server error in google protobuf related to tvm?
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fclang/detail/PR-18261/1/pipeline",IssueComment,https://github.com/apache/mxnet/issues/18229#issuecomment-625674190,ChaiBapchya,2020-05-08 07:13:47,18229,[19589],Build bug,0,Another one #18261 [url] [url] @leezu clang failed but not sure if its related to this issue Is this 500 internal server error in google protobuf related to tvm? [url]
"No, that's due to using Makefile build",IssueComment,https://github.com/apache/mxnet/issues/18229#issuecomment-625932687,leezu,2020-05-08 17:38:57,18229,[19589],Build bug,0,"No, that's due to using Makefile build"
"I think this issue requires developers to navigate through the set-up of CI pipeline and eventually come to the responsible script, so it's not a good first issue without pointers on the code base.",IssueComment,https://github.com/apache/mxnet/issues/18229#issuecomment-657258660,szha,2020-07-12 18:30:40,18229,[19589],Build bug,0,"I think this issue requires developers to navigate through the set-up of CI pipeline and eventually come to the responsible script, so it's not a good first issue without pointers on the code base."
"This is being invoked here:
https://github.com/apache/incubator-mxnet/blob/3840786a25b16d0cfe6411e26f25aba8d3b574ff/cmake/BuildTVM.cmake#L19

that tries to download the llvm archive here:
https://github.com/apache/incubator-mxnet/blob/3840786a25b16d0cfe6411e26f25aba8d3b574ff/contrib/tvmop/prepare_tvm.sh#L48",IssueComment,https://github.com/apache/mxnet/issues/18229#issuecomment-690517857,samskalicky,2020-09-10 17:02:41,18229,[19589],Build bug,0,This is being invoked here: [url]#L19 that tries to download the llvm archive here: [url]#L48
@leezu @szha This is a very serious bug and we need to fix it. Would you try to verify the bug?,IssueComment,https://github.com/apache/mxnet/issues/19595#issuecomment-735353971,sxjscience,2020-11-29 07:11:24,19595,[19596],Algorithm design bug,1,@leezu @szha This is a very serious bug and we need to fix it. Would you try to verify the bug?
"I can reproduce. Added a couple of `waitall()` calls didn't resolve it so I think it's not related to synchronization. Thus, I think it's related to serialization/deserialization itself.",IssueComment,https://github.com/apache/mxnet/issues/19595#issuecomment-735411295,szha,2020-11-29 15:27:42,19595,[19596],Algorithm design bug,1,"I can reproduce. Added a couple of [code] calls didn't resolve it so I think it's not related to synchronization. Thus, I think it's related to serialization/deserialization itself."
This is quite critical...... I trained several NMT models but the weights are not saved appropriately....,IssueComment,https://github.com/apache/mxnet/issues/19595#issuecomment-735436263,sxjscience,2020-11-29 18:35:45,19595,[19596],Algorithm design bug,1,This is quite critical...... I trained several NMT models but the weights are not saved appropriately....
I tried the 20201117 wheel and it does not have the issue. So I think it's introduced by the recent npx.savez PR.,IssueComment,https://github.com/apache/mxnet/issues/19595#issuecomment-735447166,sxjscience,2020-11-29 20:12:30,19595,[19596],Algorithm design bug,1,I tried the 20201117 wheel and it does not have the issue. So I think it's introduced by the recent npx.savez PR.
cc @rongzha1 @pengzhao-intel ,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-663941201,szha,2020-07-26 06:18:14,18788,[19608],Data bug,0,cc @rongzha1 @pengzhao-intel
"@bgawrych @luc,pior",IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-664738282,rongzha1,2020-07-28 02:26:05,18788,[19608],Data bug,0,"@bgawrych @luc,pior"
@leezu could you give more description of what's the problem is and where it is used?,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-665495419,pengzhao-intel,2020-07-29 07:45:56,18788,[19608],Data bug,0,@leezu could you give more description of what's the problem is and where it is used?
"MXNet bfloat16 type is registered as having numpy equivalent `np.dtype([('bfloat16', np.uint16)])`:

 https://github.com/apache/incubator-mxnet/blob/c1098aa33d6795f84a19601d0319d5bb8e19f317/python/mxnet/ndarray/ndarray.py#L86

As printing a NDArray works via converting to numpy, this causes `bfloat16` arrays to be printed as `uint16`.

The fundamental issue here is that numpy does not support `bfloat16`. We can consider if there is a better solution",IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-665793579,leezu,2020-07-29 17:19:32,18788,[19608],Data bug,0,"MXNet bfloat16 type is registered as having numpy equivalent [code]: [url]#L86 As printing a NDArray works via converting to numpy, this causes [code] arrays to be printed as [code]. The fundamental issue here is that numpy does not support [code]. We can consider if there is a better solution"
@ElaineBao could you explain the background?,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-670336291,pengzhao-intel,2020-08-07 05:40:09,18788,[19608],Data bug,0,@ElaineBao could you explain the background?
"As @leezu said, numpy doesn't support the data type of `bfloat16`, when printing a NDArray as type `bfloat16`, it will be printed as pre-defined `uint16`. 

I think it's not a bug actually, if printing a NDArray has to work via converting to numpy, and numpy has no support of `bfloat16`, the only way to print is converting to another numpy supported data type.",IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-670341638,ElaineBao,2020-08-07 05:58:46,18788,[19608],Data bug,0,"As @leezu said, numpy doesn't support the data type of [code], when printing a NDArray as type [code], it will be printed as pre-defined [code]. I think it's not a bug actually, if printing a NDArray has to work via converting to numpy, and numpy has no support of [code], the only way to print is converting to another numpy supported data type."
@ElaineBao I agree it's expected for people who understand how the internal mxnet printing logic works. But I'm not sure if this is the optimal user experience.,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-670577338,leezu,2020-08-07 15:36:39,18788,[19608],Data bug,0,@ElaineBao I agree it's expected for people who understand how the internal mxnet printing logic works. But I'm not sure if this is the optimal user experience.
"Yes, I think so, it's not that friendly for users to print out `bfloat16` values. We can consider if there is a better solution for this.",IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-670796064,ElaineBao,2020-08-08 00:33:27,18788,[19608],Data bug,0,"Yes, I think so, it's not that friendly for users to print out [code] values. We can consider if there is a better solution for this."
"> MXNet bfloat16 type is registered as having numpy equivalent `np.dtype([('bfloat16', np.uint16)])`:
> 
> https://github.com/apache/incubator-mxnet/blob/c1098aa33d6795f84a19601d0319d5bb8e19f317/python/mxnet/ndarray/ndarray.py#L86
> 
> As printing a NDArray works via converting to numpy, this causes `bfloat16` arrays to be printed as `uint16`.
> 
> The fundamental issue here is that numpy does not support `bfloat16`. We can consider if there is a better solution

Should we just convert to float32 in Numpy?",IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-670824689,samskalicky,2020-08-08 04:59:51,18788,[19608],Data bug,0,"> MXNet bfloat16 type is registered as having numpy equivalent [code]: > > [url]#L86 > > As printing a NDArray works via converting to numpy, this causes [code] arrays to be printed as [code]. > > The fundamental issue here is that numpy does not support [code]. We can consider if there is a better solution Should we just convert to float32 in Numpy?"
Is it possible to customized a dtype in numpy just for printing?,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-673703215,sxjscience,2020-08-13 20:49:12,18788,[19608],Data bug,0,Is it possible to customized a dtype in numpy just for printing?
@anko-intel ,IssueComment,https://github.com/apache/mxnet/issues/18788#issuecomment-683538227,pengzhao-intel,2020-08-31 04:09:04,18788,[19608],Data bug,0,@anko-intel
Related code on MXNet side is at https://github.com/apache/incubator-mxnet/blob/91503f71e17fca9151779503fc9f5edefe26f2ef/tools/pip/setup.py#L148-L150,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-747548497,leezu,2020-12-17 16:25:30,19690,[19706],Build bug,1,Related code on MXNet side is at [url]#L148-L150
I've met this error too. @bartekkuncer  Is it possible to add the header to pip wheel? Also ping @szha ,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-747882704,sxjscience,2020-12-18 05:48:21,19690,[19706],Build bug,1,I've met this error too. @bartekkuncer Is it possible to add the header to pip wheel? Also ping @szha
"In terms of the wheel, I think the last wheel that works is 20201214.",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-747886114,sxjscience,2020-12-18 05:56:46,19690,[19706],Build bug,1,"In terms of the wheel, I think the last wheel that works is 20201214."
"@leezu I believe that changing L149 to: `shutil.copytree(os.path.join(CURRENT_DIR, 'mxnet-build/3rdparty/mkldnn/include/oneapi/dnnl'),` fixes the issue but as I am not familiar with horovod I do not know how to check if the fix works. Can you provide me with a way to reproduce the issue?",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-748077395,bartekkuncer,2020-12-18 13:13:38,19690,[19706],Build bug,1,@leezu I believe that changing L149 to: [code] fixes the issue but as I am not familiar with horovod I do not know how to check if the fix works. Can you provide me with a way to reproduce the issue?
"@bartekkuncer For me, I met this error when trying to install horovod (you may change the cuda version):

```bash

python3 -m pip install -U --pre ""mxnet-cu102==2.0.0b20201217"" -f https://dist.mxnet.io/python
HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_WITHOUT_GLOO=1 HOROVOD_WITH_MPI=1 HOROVOD_WITH_MXNET=1 HOROVOD_WITHOUT_PYTORCH=1 HOROVOD_WITHOUT_TENSORFLOW=1 python3 -m pip install --no-cache-dir horovod
```",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-748174182,sxjscience,2020-12-18 16:00:39,19690,[19706],Build bug,1,"@bartekkuncer For me, I met this error when trying to install horovod (you may change the cuda version): ``[code]``"
"@bartekkuncer I recommend the following for verification:
- Download a wheel with the correct headers (e.g. the 20201214 one that @sxjscience mentioend) and one without (the ones after 1214)
- Unzip them and examine the header content and see where the headers should be.
- Correct the toolts/pip/setup.py and use the wheel build scripts to build correct wheel.
- Verify if the headers are in the right location in the resulting wheel.",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-748176237,szha,2020-12-18 16:04:21,19690,[19706],Build bug,1,@bartekkuncer I recommend the following for verification: - Download a wheel with the correct headers (e.g. the 20201214 one that @sxjscience mentioend) and one without (the ones after 1214) - Unzip them and examine the header content and see where the headers should be. - Correct the toolts/pip/setup.py and use the wheel build scripts to build correct wheel. - Verify if the headers are in the right location in the resulting wheel.
Thanks @bartekkuncer! I opened https://github.com/apache/incubator-mxnet/pull/19694 as weekend has started in your timezone,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-748261762,leezu,2020-12-18 18:56:55,19690,[19706],Build bug,1,Thanks @bartekkuncer! I opened [url] as weekend has started in your timezone
"Horovod now fails with 

```
[0m[91m    In file included from /tmp/pip-req-build-bhade3mm/horovod/mxnet/mpi_ops.h:24:0,
                     from /tmp/pip-req-build-bhade3mm/horovod/mxnet/mpi_ops.cc:21:
    /usr/local/lib/python3.6/dist-packages/mxnet/include/mxnet/ndarray.h:41:10: fatal error: mkldnn.hpp: No such file or directory
[0m[91m     #include <mkldnn.hpp>
[0m[91m              ^~~~~~~~~~~~
[0m[91m    compilation terminated.
[0m[91m    horovod/mxnet/CMakeFiles/mxnet.dir/build.make:758: recipe for target 'horovod/mxnet/CMakeFiles/mxnet.dir/mpi_ops.cc.o' failed
```",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-749005881,leezu,2020-12-21 14:38:09,19690,[19706],Build bug,1,Horovod now fails with ``[code]``
"> Horovod now fails with
> 
> ```
> �[0m�[91m    In file included from /tmp/pip-req-build-bhade3mm/horovod/mxnet/mpi_ops.h:24:0,
>                      from /tmp/pip-req-build-bhade3mm/horovod/mxnet/mpi_ops.cc:21:
>     /usr/local/lib/python3.6/dist-packages/mxnet/include/mxnet/ndarray.h:41:10: fatal error: mkldnn.hpp: No such file or directory
> �[0m�[91m     #include <mkldnn.hpp>
> �[0m�[91m              ^~~~~~~~~~~~
> �[0m�[91m    compilation terminated.
> �[0m�[91m    horovod/mxnet/CMakeFiles/mxnet.dir/build.make:758: recipe for target 'horovod/mxnet/CMakeFiles/mxnet.dir/mpi_ops.cc.o' failed
> ```

Yes, I saw that, working on the fix.",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-749007656,bartekkuncer,2020-12-21 14:41:50,19690,[19706],Build bug,1,"> Horovod now fails with > > ``[code]`` Yes, I saw that, working on the fix."
Thanks @bartekkuncer!,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-749008036,leezu,2020-12-21 14:42:31,19690,[19706],Build bug,1,Thanks @bartekkuncer!
@leezu I think this PR https://github.com/apache/incubator-mxnet/pull/19706 should fix the problem.,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-749610413,bartekkuncer,2020-12-22 15:47:30,19690,[19706],Build bug,1,@leezu I think this PR [url] should fix the problem.
"@bartekkuncer looks like the `dnnl_config.h` is included in the wrong directory. It should be in `oneapi/dnnl/dnnl_config.h` at least horovod build fails with `usr/local/lib/python3.6/dist-packages/mxnet/include/mkldnn/oneapi/dnnl/dnnl.hpp:23:10: fatal error: oneapi/dnnl/dnnl_config.h: No such file or directory  #include ""oneapi/dnnl/dnnl_config.h""`)",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-752636115,leezu,2020-12-30 14:13:42,19690,[19706],Build bug,1,@bartekkuncer looks like the [code] is included in the wrong directory. It should be in [code] at least horovod build fails with [code])
"The files included in are

```
mxnet/include/mkldnn
mxnet/include/mkldnn/mkldnn_version.h
mxnet/include/mkldnn/dnnl_debug.h
mxnet/include/mkldnn/mkldnn_debug.h
mxnet/include/mkldnn/dnnl_ocl.h
mxnet/include/mkldnn/dnnl_sycl.h
mxnet/include/mkldnn/dnnl_ocl.hpp
mxnet/include/mkldnn/mkldnn_types.h
mxnet/include/mkldnn/dnnl_version.h
mxnet/include/mkldnn/oneapi
mxnet/include/mkldnn/oneapi/dnnl
mxnet/include/mkldnn/oneapi/dnnl/dnnl_debug.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl_ocl.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl_ocl.hpp
mxnet/include/mkldnn/oneapi/dnnl/dnnl_types.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl.hpp
mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl_types.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool_iface.hpp
mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool.hpp
mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl.hpp
mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool.h
mxnet/include/mkldnn/oneapi/dnnl/dnnl.h
mxnet/include/mkldnn/mkldnn_config.h
mxnet/include/mkldnn/dnnl_types.h
mxnet/include/mkldnn/dnnl.hpp
mxnet/include/mkldnn/dnnl_config.h
mxnet/include/mkldnn/mkldnn.hpp
mxnet/include/mkldnn/mkldnn_dnnl_mangling.h
mxnet/include/mkldnn/dnnl_sycl_types.h
mxnet/include/mkldnn/dnnl_threadpool_iface.hpp
mxnet/include/mkldnn/dnnl_threadpool.hpp
mxnet/include/mkldnn/dnnl_sycl.hpp
mxnet/include/mkldnn/dnnl_threadpool.h
mxnet/include/mkldnn/mkldnn.h
mxnet/include/mkldnn/dnnl.h
```
I think we may need to update https://github.com/apache/incubator-mxnet/blob/3c5beb3596b6bc01f77bc7ddd14ed90221c31950/cd/mxnet_lib/static/Jenkins_pipeline.groovy#L36 to ensure that the config files are stashed correctly on the CD",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-752637355,leezu,2020-12-30 14:18:02,19690,[19706],Build bug,1,The files included in are ``[code]`` I think we may need to update [url]#L36 to ensure that the config files are stashed correctly on the CD
we might also consider making it robust in setup.py by asserting the existence of these header files instead of only include when available.,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-752806688,szha,2020-12-31 01:04:09,19690,[19706],Build bug,1,we might also consider making it robust in setup.py by asserting the existence of these header files instead of only include when available.
"> The files included in are
> 
> ```
> mxnet/include/mkldnn
> mxnet/include/mkldnn/mkldnn_version.h
> mxnet/include/mkldnn/dnnl_debug.h
> mxnet/include/mkldnn/mkldnn_debug.h
> mxnet/include/mkldnn/dnnl_ocl.h
> mxnet/include/mkldnn/dnnl_sycl.h
> mxnet/include/mkldnn/dnnl_ocl.hpp
> mxnet/include/mkldnn/mkldnn_types.h
> mxnet/include/mkldnn/dnnl_version.h
> mxnet/include/mkldnn/oneapi
> mxnet/include/mkldnn/oneapi/dnnl
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_debug.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_ocl.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_ocl.hpp
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_types.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl.hpp
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl_types.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool_iface.hpp
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool.hpp
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_sycl.hpp
> mxnet/include/mkldnn/oneapi/dnnl/dnnl_threadpool.h
> mxnet/include/mkldnn/oneapi/dnnl/dnnl.h
> mxnet/include/mkldnn/mkldnn_config.h
> mxnet/include/mkldnn/dnnl_types.h
> mxnet/include/mkldnn/dnnl.hpp
> mxnet/include/mkldnn/dnnl_config.h
> mxnet/include/mkldnn/mkldnn.hpp
> mxnet/include/mkldnn/mkldnn_dnnl_mangling.h
> mxnet/include/mkldnn/dnnl_sycl_types.h
> mxnet/include/mkldnn/dnnl_threadpool_iface.hpp
> mxnet/include/mkldnn/dnnl_threadpool.hpp
> mxnet/include/mkldnn/dnnl_sycl.hpp
> mxnet/include/mkldnn/dnnl_threadpool.h
> mxnet/include/mkldnn/mkldnn.h
> mxnet/include/mkldnn/dnnl.h
> ```
> 
> I think we may need to update
> 
> https://github.com/apache/incubator-mxnet/blob/3c5beb3596b6bc01f77bc7ddd14ed90221c31950/cd/mxnet_lib/static/Jenkins_pipeline.groovy#L36
> 
> to ensure that the config files are stashed correctly on the CD

Thanks @leezu . I changed it in CI but must have overlooked it in CD.",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-753866603,bartekkuncer,2021-01-04 09:34:01,19690,[19706],Build bug,1,> The files included in are > > ``[code]`` > > I think we may need to update > > [url]#L36 > > to ensure that the config files are stashed correctly on the CD Thanks @leezu . I changed it in CI but must have overlooked it in CD.
"It looks like there are still more issues with the CD. Horovod still fails with

```
[0m[91m    /usr/local/lib/python3.6/dist-packages/mxnet/include/mkldnn/oneapi/dnnl/dnnl.hpp:23:10: fatal error: oneapi/dnnl/dnnl_config.h: No such file or directory
[0m[91m     #include ""oneapi/dnnl/dnnl_config.h""
[0m[91m              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
[0m[91m    compilation terminated.
[0m[91m    horovod/mxnet/CMakeFiles/mxnet.dir/build.make:758: recipe for target 'horovod/mxnet/CMakeFiles/mxnet.dir/mpi_ops.cc.o' failed
```",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-754019516,leezu,2021-01-04 14:51:07,19690,[19706],Build bug,1,It looks like there are still more issues with the CD. Horovod still fails with ``[code]``
"> It looks like there are still more issues with the CD. Horovod still fails with
> 
> ```
> �[0m�[91m    /usr/local/lib/python3.6/dist-packages/mxnet/include/mkldnn/oneapi/dnnl/dnnl.hpp:23:10: fatal error: oneapi/dnnl/dnnl_config.h: No such file or directory
> �[0m�[91m     #include ""oneapi/dnnl/dnnl_config.h""
> �[0m�[91m              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
> �[0m�[91m    compilation terminated.
> �[0m�[91m    horovod/mxnet/CMakeFiles/mxnet.dir/build.make:758: recipe for target 'horovod/mxnet/CMakeFiles/mxnet.dir/mpi_ops.cc.o' failed
> ```

Yes, I saw that. https://github.com/apache/incubator-mxnet/pull/19726 should fix the issue.",IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-754046194,bartekkuncer,2021-01-04 15:37:00,19690,[19706],Build bug,1,"> It looks like there are still more issues with the CD. Horovod still fails with > > ``[code]`` Yes, I saw that. [url] should fix the issue."
Thank you @bartekkuncer!,IssueComment,https://github.com/apache/mxnet/issues/19690#issuecomment-755414239,leezu,2021-01-06 16:38:41,19690,[19706],Build bug,1,Thank you @bartekkuncer!
"Removing 
```
ENV CUDA_VERSION=10.2.89
ENV CUDNN_VERSION=8.0.4.30
COPY install/ubuntu_cudnn.sh /work/
RUN /work/ubuntu_cudnn.sh
```
in the test env did not work..
@mseth10 would you provide some context on why you updated cudnn from 7 to 8?
I think cu102 mxnet is still built with cudnn7
https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job-1.x/runs/1522/nodes/74/steps/203/log/?start=0",IssueComment,https://github.com/apache/mxnet/issues/19929#issuecomment-782409556,Zha0q1,2021-02-19 22:26:50,19929,[19940],Processor bug,0,Removing ``[code]`` in the test env did not work.. @mseth10 would you provide some context on why you updated cudnn from 7 to 8? I think cu102 mxnet is still built with cudnn7 [url]
I built mxnet in the test env and it passed the tests,IssueComment,https://github.com/apache/mxnet/issues/19929#issuecomment-782441817,Zha0q1,2021-02-19 23:14:26,19929,[19940],Processor bug,0,I built mxnet in the test env and it passed the tests
fixed,IssueComment,https://github.com/apache/mxnet/issues/19929#issuecomment-783638485,Zha0q1,2021-02-22 20:03:45,19929,[19940],Processor bug,0,fixed
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/20069#issuecomment-803923528,github-actions[bot],2021-03-22 09:47:07,20069,[20070],Code bug,1,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
"Simillar error on linux:
MXNetError: The size of NDArray doesn't match the requested MKLDNN memory desc. MKLDNN memory requests for 50331648 bytes, but got 34603008 bytes from NDArray",IssueComment,https://github.com/apache/mxnet/issues/19586#issuecomment-733815695,bartekkuncer,2020-11-25 16:33:44,19586,[20129],Processor bug,1,"Simillar error on linux: MXNetError: The size of NDArray doesn't match the requested MKLDNN memory desc. MKLDNN memory requests for 50331648 bytes, but got 34603008 bytes from NDArray"
@mxnet-label-bot MKLDNN,IssueComment,https://github.com/apache/mxnet/issues/19586#issuecomment-734181964,anko-intel,2020-11-26 09:27:32,19586,[20129],Processor bug,1,@mxnet-label-bot MKLDNN
@mxnet-label-bot add [MKLDNN],IssueComment,https://github.com/apache/mxnet/issues/19586#issuecomment-735678272,anko-intel,2020-11-30 09:51:16,19586,[20129],Processor bug,1,@mxnet-label-bot add [MKLDNN]
I am working on this.,IssueComment,https://github.com/apache/mxnet/issues/19586#issuecomment-749532262,PawelGlomski-Intel,2020-12-22 13:09:32,19586,[20129],Processor bug,1,I am working on this.
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/20368#issuecomment-864825114,github-actions[bot],2021-06-21 08:11:02,20368,[20369],Build bug,0,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
This is caused by outdated copy of https://gitlab.kitware.com/cmake/cmake/blob/master/Modules/FindCUDA/select_compute_arch.cmake Should be fixed by https://github.com/apache/incubator-mxnet/pull/20369,IssueComment,https://github.com/apache/mxnet/issues/20368#issuecomment-865090492,leezu,2021-06-21 14:44:20,20368,[20369],Build bug,0,This is caused by outdated copy of [url] Should be fixed by [url]
"Thanks @leezu . I think this issus is fixed by the https://github.com/apache/incubator-mxnet/pull/20369. Closed.
![image](https://user-images.githubusercontent.com/41060790/122848644-8fdc2580-d33c-11eb-8860-2f0492f70078.png)
",IssueComment,https://github.com/apache/mxnet/issues/20368#issuecomment-865453730,Adnios,2021-06-22 01:31:54,20368,[20369],Build bug,0,Thanks @leezu . I think this issus is fixed by the [url] Closed. ![image]([url]
Good catch! will be fixed in #20421,IssueComment,https://github.com/apache/mxnet/issues/20420#issuecomment-875201256,barry-jin,2021-07-07 01:37:24,20420,[20421],Data bug,1,Good catch! will be fixed in #20421
This seems a problem specific with the `elemwise_add` operator. Replacing it with `elemwise_mul` in the json string produces correct gradients for all the request combinations.,IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-850523100,matteosal,2021-05-28 16:09:39,20293,[20462],Algorithm design bug,1,This seems a problem specific with the [code] operator. Replacing it with [code] in the json string produces correct gradients for all the request combinations.
Any help on this?,IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-856097741,matteosal,2021-06-07 16:47:41,20293,[20462],Algorithm design bug,1,Any help on this?
"**Analysis:**

The issue is solved mainly by comparing ` elemwise_add` with  `elemwise_mul` at operation registration.  Thanks to @barry-jin  pointing out an important discrepancy between these two operators today. The discrepancy is in “registering the function for creating the node of the operator in a backward pass”:
 https://github.com/apache/incubator-mxnet/blob/da4ff3a4dc0bd6a54af3d75c492021d18ba1867b/src/operator/tensor/elemwise_binary_op_basic.cc#L111 https://github.com/apache/incubator-mxnet/blob/da4ff3a4dc0bd6a54af3d75c492021d18ba1867b/src/operator/tensor/elemwise_binary_op_basic.cc#L238 

`elemwise_mul` uses `ElemwiseGradUseIn`, since calculation of `_back_mul` depends on input value. However,  `elemwise_add` uses `CloneGradient`, the reason of which is not clear yet. But ` ElemwiseGradUseNone` is another valid option for `elemwise_add` . So I tested it by replacing ` CloneGradient`  with` ElemwiseGradUseNone` .

**Result:**

**The issue is solved!** `./demo2 0 1` specifies the gradient request as following, and 2021 is the hardcoding head gradient.
```
 uint32_t grad_req_1[1] = {0};
 uint32_t grad_req_2[1] = {1};
```
```
$~/mxnet/issue2/build2$ ./demo2 0 1
[19:45:23] ../src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[19:45:23] ../src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[19:45:23] ../src/engine/engine.cc:55: MXNet start using engine: NaiveEngine
[19:45:23] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
INPUT 1: 0.4
INPUT 2: 0.5
OUTPUT: 0.9
OUTGRAD: 2021
GRAD 1: 0
GRAD 2: 2021

$~/mxnet/issue2/build2$ ./demo2 1 0
[19:49:59] ../src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[19:49:59] ../src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[19:49:59] ../src/engine/engine.cc:55: MXNet start using engine: NaiveEngine
[19:49:59] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
INPUT 1: 0.4
INPUT 2: 0.5
OUTPUT: 0.9
OUTGRAD: 2021
GRAD 1: 2021
GRAD 2: 0

$~/mxnet/issue2/build2$ ./demo2 1 1
[19:50:11] ../src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[19:50:11] ../src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[19:50:11] ../src/engine/engine.cc:55: MXNet start using engine: NaiveEngine
[19:50:11] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
INPUT 1: 0.4
INPUT 2: 0.5
OUTPUT: 0.9
OUTGRAD: 2021
GRAD 1: 2021
GRAD 2: 2021
```
**Further investigation:**

This result shows CloneGradient   is the most possible reason that causes this issue. So the remaining check is 

1. In bug report , the sym is built by:
```
 {
        ""nodes"": [
            {
                \""op\"":\""null\"",
                \""name\"":\"".Inputs.Input1\"",
                \""inputs\"":[]
            },
            {
                \""op\"":\""null\"",
                \""name\"":\"".Inputs.Input2\"",
                \""inputs\"":[]
            },
            {
                \""op\"":\""elemwise_add\"",
                \""name\"":\"".$0\"",
                \""inputs\"":[[0,0,0],[1,0,0]]
            },
            {
                \""op\"":\""_copy\"",
                \""name\"":\"".Outputs.Output\"",
                \""inputs\"":[[2,0,0]]
            }
        ],
        \""arg_nodes\"":[0,1],
        \""heads\"":[[3,0,0]]
    }
```
It is possible that ""_copy"" operation intrigues  this issue in CloneGradient. This helps understand why it fails.

2. Other usages of CloneGradient in mxnet\src\operator\contrib\stes_op.cc
``` 
CloneGradient{""_backward_round_ste""}
CloneGradient{""_backward_sign_ste""}
```
It is possible they have the similar bug.
",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-866448263,KexinFeng,2021-06-23 01:17:43,20293,[20462],Algorithm design bug,1,"**Analysis:** The issue is solved mainly by comparing [code] with [code] at operation registration. Thanks to @barry-jin pointing out an important discrepancy between these two operators today. The discrepancy is in “registering the function for creating the node of the operator in a backward pass”: [url]#L111 [url]#L238 [code] uses [code], since calculation of [code] depends on input value. However, [code] uses [code], the reason of which is not clear yet. But [code] is another valid option for [code] . So I tested it by replacing [code] with[code] . **Result:** **The issue is solved!** [code] specifies the gradient request as following, and 2021 is the hardcoding head gradient. ``[code]`[code]`[code]`[code]`[code]`[code]`[code]`` It is possible they have the similar bug."
"@KexinFeng thanks for looking into this!
> It is possible that ""_copy"" operation intrigues this issue in CloneGradient. This helps understand why it fails.

There must be other factors besides this because the equivalent python example in my original report works fine and it also has the `_copy` operator. This also mean that there is a particular combination of C API calls (the one used by python) which is able to work around this bug.",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-866951015,matteosal,2021-06-23 15:45:38,20293,[20462],Algorithm design bug,1,"@KexinFeng thanks for looking into this! > It is possible that ""_copy"" operation intrigues this issue in CloneGradient. This helps understand why it fails. There must be other factors besides this because the equivalent python example in my original report works fine and it also has the [code] operator. This also mean that there is a particular combination of C API calls (the one used by python) which is able to work around this bug."
"I see. 
> because the equivalent python example in my original report works fine and it also has the _copy operator

Actually I notice a discrepancy between the python call and cpp call. In python code,  it calls `CachedOp::Forward()` and then `CachedOp::StaticForward()`. In cpp code, `CachedOp::Forward()` calls `CachedOp::DynamicForward()`.  I currently don't know if this discrepancy triggers the bug. But it can be tested quicly by setting something like `hybridize(static_alloc=True)` on python, or setting `static_alloc=False` on cpp.

*Update*
As a temporary solution, I find changing static allocation to false will avoid this bug. 

/* Create cached op */
const char *cachedop_keys[1] = {""static_alloc""};
const char *cachedop_vals[1] = {""false""};

I'm wondering if dynamic allocation is applicable in your usage?",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-867012740,KexinFeng,2021-06-23 17:08:33,20293,[20462],Algorithm design bug,1,"I see. > because the equivalent python example in my original report works fine and it also has the _copy operator Actually I notice a discrepancy between the python call and cpp call. In python code, it calls [code] and then [code]. In cpp code, [code] calls [code]. I currently don't know if this discrepancy triggers the bug. But it can be tested quicly by setting something like [code] on python, or setting [code] on cpp. *Update* As a temporary solution, I find changing static allocation to false will avoid this bug. /* Create cached op */ const char *cachedop_keys[1] = {""static_alloc""}; const char *cachedop_vals[1] = {""false""}; I'm wondering if dynamic allocation is applicable in your usage?"
"Unfortunately dynamic allocation is not an option for me, at least for now.

Anyway, I have verified that `static_alloc = false` indeed fixes the issue. It also explains why it works on python, because no flag is passed to the cached op from python and `static_alloc` is false by default. Hardcoding ` flags = {""static_alloc"": ""true""}` [here](https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/_ctypes/cached_op.py#L50) exposes the issue in the python script as well.

Thanks for submitting the PR.",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-867631898,matteosal,2021-06-24 13:21:10,20293,[20462],Algorithm design bug,1,"Unfortunately dynamic allocation is not an option for me, at least for now. Anyway, I have verified that [code] indeed fixes the issue. It also explains why it works on python, because no flag is passed to the cached op from python and [code] is false by default. Hardcoding [code] [here]([url]#L50) exposes the issue in the python script as well. Thanks for submitting the PR."
"@szha @leezu @sandeep-krishnamurthy Bringing this issue to your attention - this issue is most probably a bug in the handling of reqs in `CachedOp` when `static_alloc` is set to `True`, which seems pretty major. Could you assign somebody to take a look?",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-881567012,ptrendx,2021-07-16 16:21:33,20293,[20462],Algorithm design bug,1,"@szha @leezu @sandeep-krishnamurthy Bringing this issue to your attention - this issue is most probably a bug in the handling of reqs in [code] when [code] is set to [code], which seems pretty major. Could you assign somebody to take a look?"
"I don't think the root cause is in CachedOp. As I was debugging this issue, the elemwise_add is using [CloneGradient](https://github.com/apache/incubator-mxnet/blob/3480ba2c6df02bb907d3a975d354efa8697c4e71/src/operator/tensor/elemwise_binary_op_basic.cc#L111), which means copy ograds multiple times for the inputs. 

For cached_op, if the static_alloc is on, then it will construct backward graph with grad_graph outputs
https://github.com/apache/incubator-mxnet/blob/3480ba2c6df02bb907d3a975d354efa8697c4e71/src/imperative/cached_op.cc#L270-L281
In the case of elemwise_add(a, b), the grad_graph will be like this. 
![Screen Shot 2021-07-16 at 4 19 40 PM](https://user-images.githubusercontent.com/69359374/126018275-a3d1505f-69ee-43b3-9897-2dbb15428c7b.png)
The gradient of b will be the copy of the gradient of a. So there will be divergence between (case1: a.grad_req = null, b.grad_req = write) and (case2: a.grad_req = write, b.grad_req = null) when constructing the new graph based on the grad_graph. 

From my point of view, the solution of this bug is to change the elemwise_add gradient function to this
```
.set_attr<nnvm::FGradient>(""FGradient"",
  [](const nnvm::ObjectPtr& n, const std::vector<nnvm::NodeEntry>& ograds) {
    std::vector<nnvm::NodeEntry> ret;
    const size_t input_count = n->inputs.size();
    ret.reserve(input_count);
    for (size_t i = 0; i < input_count; ++i) {
      ret.emplace_back(MakeNode(""ones_like"", n->attrs.name + ""_grad_ones"", {n->inputs[i]}, nullptr, &n));
    }
    return ret;
});
```
@KexinFeng FYI",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-881776555,barry-jin,2021-07-16 23:56:41,20293,[20462],Algorithm design bug,1,"I don't think the root cause is in CachedOp. As I was debugging this issue, the elemwise_add is using [CloneGradient]([url]#L111), which means copy ograds multiple times for the inputs. For cached_op, if the static_alloc is on, then it will construct backward graph with grad_graph outputs [url]#L270-L281 In the case of elemwise_add(a, b), the grad_graph will be like this. ![Screen Shot 2021-07-16 at 4 19 40 PM]([url] The gradient of b will be the copy of the gradient of a. So there will be divergence between (case1: a.grad_req = null, b.grad_req = write) and (case2: a.grad_req = write, b.grad_req = null) when constructing the new graph based on the grad_graph. From my point of view, the solution of this bug is to change the elemwise_add gradient function to this ``[code]`` @KexinFeng FYI"
"The graph is exactly the same, whether you have static or dynamic alloc. The problem I believe lies here:
https://github.com/apache/incubator-mxnet/blob/3480ba2c6df02bb907d3a975d354efa8697c4e71/src/imperative/cached_op.cc#L986-L1006

especially lines 992 and 1002 - it sets the reqs for `NodeEntries` of the graph outputs based on the user preferences. This is only fine if those `NodeEntries` are not actually used in the other places in the computation, which for most cases is true. However, the  gradient of `elementwise_add` is just a split. The `CloneGradient` method does not do any copying actually, it just reuses the `NodeEntry` and passes it to both downstream operators - this is the right behavior. But then when 1 of those downstream places is the output of the graph marked with `req=null`, you get a problem - because of those lines in `StaticBackward` function that edge is marked as `kNullOp` even though it is used in the rest of the computation.

The proper fix therefore would be to change the logic in the `StaticBackward` to take into account all usages of those `NodeEntries` and only mark it as null if the graph output is the only usage. There could also be an additional improvement here (although the real world impact of this is probably pretty minimal) to make the `kNullOp` requirement ""traverse"" the graph - right now if you have a chain of (single input/output) operations A -> B -> C which ultimately produces unnecessary output, then only the C operation will know about this and A and B will be performed (and their outputs allocated) no matter what.",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-881829759,ptrendx,2021-07-17 05:04:52,20293,[20462],Algorithm design bug,1,"The graph is exactly the same, whether you have static or dynamic alloc. The problem I believe lies here: [url]#L986-L1006 especially lines 992 and 1002 - it sets the reqs for [code] of the graph outputs based on the user preferences. This is only fine if those [code] are not actually used in the other places in the computation, which for most cases is true. However, the gradient of [code] is just a split. The [code] method does not do any copying actually, it just reuses the [code] and passes it to both downstream operators - this is the right behavior. But then when 1 of those downstream places is the output of the graph marked with [code], you get a problem - because of those lines in [code] function that edge is marked as [code] even though it is used in the rest of the computation. The proper fix therefore would be to change the logic in the [code] to take into account all usages of those [code] and only mark it as null if the graph output is the only usage. There could also be an additional improvement here (although the real world impact of this is probably pretty minimal) to make the [code] requirement ""traverse"" the graph - right now if you have a chain of (single input/output) operations A -> B -> C which ultimately produces unnecessary output, then only the C operation will know about this and A and B will be performed (and their outputs allocated) no matter what."
"Also, you definitely do not want to make gradient of add be `ones_like` ;-).",IssueComment,https://github.com/apache/mxnet/issues/20293#issuecomment-881830220,ptrendx,2021-07-17 05:08:28,20293,[20462],Algorithm design bug,1,"Also, you definitely do not want to make gradient of add be [code] ;-)."
"The line 258 means it can not get GPU device.
```
CUDA_DRIVER_CALL((*device_get_ptr)(&cu_device, dev_id));
```

@mseth10 
What is the name of libcuda in the CentOS system? Is it also libcuda.so?
",IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-894510632,TristonC,2021-08-06 20:48:07,20494,[20512],Test bug,1,The line 258 means it can not get GPU device. ``[code]`` @mseth10 What is the name of libcuda in the CentOS system? Is it also libcuda.so?
"@TristonC looks like it is the same. I ran the same docker image as in CI, and got this
```
# find /usr/local/cuda/ -name '*cuda*.so*'
/usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.10.2
/usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.10.2.89
/usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so
/usr/local/cuda/targets/x86_64-linux/lib/libcudart.so
/usr/local/cuda/compat/libcuda.so.440.118.02
/usr/local/cuda/compat/libcuda.so.1
/usr/local/cuda/compat/libcuda.so
```",IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-895086488,mseth10,2021-08-09 09:40:18,20494,[20512],Test bug,1,"@TristonC looks like it is the same. I ran the same docker image as in CI, and got this ``[code]``"
I don't have a CentOS system. @barry-jin @mseth10  Is this issue reproducible? Or is it just flaky in CI? ,IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-895368929,TristonC,2021-08-09 16:34:25,20494,[20512],Test bug,1,I don't have a CentOS system. @barry-jin @mseth10 Is this issue reproducible? Or is it just flaky in CI?
I find that I cannot reproduce this issue in CentOS7 docker. ,IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-897039029,barry-jin,2021-08-11 18:07:21,20494,[20512],Test bug,1,I find that I cannot reproduce this issue in CentOS7 docker.
"@barry-jin new mxnetlinux-gpu AMI was created recently, so this issue should be resolved now. Can you retry CI on your PR? ",IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-897060535,mseth10,2021-08-11 18:39:07,20494,[20512],Test bug,1,"@barry-jin new mxnetlinux-gpu AMI was created recently, so this issue should be resolved now. Can you retry CI on your PR?"
"Looks like CentOS tests still failed. https://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-gpu/detail/PR-20512/4/pipeline

I think the root cause may be that libcuda.so is linked to `/usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so`, which should only be used in build time, but now mxnet is trying to dlopen it in runtime. 

I cannot reproduce it because libcuda.so is linked to the libcuda.so used for runtime. 
```
[root@1dc28a587f4f build]# ldconfig -p | grep cuda
...
	libcuda.so.1 (libc6,x86-64) => /lib64/libcuda.so.1
	libcuda.so (libc6,x86-64) => /lib64/libcuda.so
...
```",IssueComment,https://github.com/apache/mxnet/issues/20494#issuecomment-897179376,barry-jin,2021-08-11 21:53:28,20494,[20512],Test bug,1,"Looks like CentOS tests still failed. [url] I think the root cause may be that libcuda.so is linked to [code], which should only be used in build time, but now mxnet is trying to dlopen it in runtime. I cannot reproduce it because libcuda.so is linked to the libcuda.so used for runtime. ``[code]``"
Could you describe how you run cmake ?,IssueComment,https://github.com/apache/mxnet/issues/20836#issuecomment-1019921032,anko-intel,2022-01-24 10:02:34,20836,[20893],Build bug,0,Could you describe how you run cmake ?
"@anko-intel here is my invocation with all the flags: 
```
cmake \
 `# GENERAL FLAGS` \
 -DCMAKE_INSTALL_PREFIX=$output_dir \
 -DCMAKE_BUILD_TYPE=Release \
 -DCMAKE_SKIP_BUILD_RPATH=On \
 -DUSE_OPENCV=OFF \
 -DUSE_JEMALLOC=OFF \
 -DUSE_F16C=Off `# float16 support`\
 -DUSE_INT64_TENSOR_SIZE=ON `# this + MKL_MULTI_THREADED=ON + MKL_USE_SINGLE_DYNAMIC_LIBRARY=OFF select MKL ILP64`\
 -DCMAKE_C_FLAGS_RELEASE=""-DNDEBUG"" \
 -DCMAKE_CXX_FLAGS_RELEASE=""-DNDEBUG"" \
 -DCMAKE_SHARED_LINKER_FLAGS=""-Wl,--as-needed"" `# prevent libgomp from linking` \
 `# MATH BACKENDS` \
 -DBLAS=MKL \
 -DUSE_LAPACK=ON \
 -DUSE_LAPACKE_INTERFACE=ON \
 -DUSE_ONEDNN=ON \
 -DBLA_STATIC=OFF \
 -DMKL_MULTI_THREADED=ON \
 -DMKL_USE_SINGLE_DYNAMIC_LIBRARY=OFF \
 -DMKL_INCLUDE_DIR=$mkl_dir \
 -DBLAS_LIBRARIES=""$mkl_dir/libmkl_def.so;$mkl_dir/libmkl_intel_ilp64.so;$mkl_dir/libmkl_core.so;$mkl_dir/libmkl_intel_thread.so;$mkl_dir/libiomp5.so"" \
 `# OPENMP` \
 -DUSE_OPENMP=ON \
 -DOpenMP_C_FLAGS=""-I$mkl_dir -fopenmp"" \
 -DOpenMP_C_LIB_NAMES=""libiomp5"" \
 -DOpenMP_CXX_FLAGS=""-I$mkl_dir -fopenmp"" \
 -DOpenMP_CXX_LIB_NAMES=""libiomp5"" \
 -DOpenMP_libiomp5_LIBRARY=""$mkl_dir/libiomp5.so"" \
 `# CUDA` \
 -DUSE_CUDA=OFF \
 $mxnet_dir
```",IssueComment,https://github.com/apache/mxnet/issues/20836#issuecomment-1019967635,matteosal,2022-01-24 10:54:17,20836,[20893],Build bug,0,@anko-intel here is my invocation with all the flags: ``[code]# GENERAL FLAGS[code]# float16 support[code]# this + MKL_MULTI_THREADED=ON + MKL_USE_SINGLE_DYNAMIC_LIBRARY=OFF select MKL ILP64[code]# prevent libgomp from linking[code]# MATH BACKENDS[code]# OPENMP[code]# CUDA[code]``
@DickJC123 Good catch! [This line](https://github.com/apache/incubator-mxnet/blob/ded04008fd3cd95793e0eaaa72f5904deb77f838/.github/workflows/os_x_staticbuild.yml#L60) should be replaced with `MXNET_ENABLE_CYTHON: 0` to disable cython. ,IssueComment,https://github.com/apache/mxnet/issues/20963#issuecomment-1068722386,barry-jin,2022-03-16 04:33:59,20963,[20965],Test bug,1,@DickJC123 Good catch! [This line]([url]#L60) should be replaced with [code] to disable cython.
"After further investigation found out that the real problem was reading from neither updated nor cleared register during convolution with weights stored in blocked format. This problem does not occur when the amount of input channels is 4 and higher as full register is being overwritten. Fix for this issue is coming in v2.6 of oneDNN.

Original shapes have been restored in the test. There was a temporary fix in place: https://github.com/apache/incubator-mxnet/pull/20847 forcing blocking the weights by 8 instead of 16, but it turned out only to make the problem occur less often. Therefore until v2.6 of oneDNN arrives there will be no blocking the weights for convolutions with the amount of input channels lower than 4: https://github.com/apache/incubator-mxnet/pull/20970.",IssueComment,https://github.com/apache/mxnet/issues/20826#issuecomment-1073960486,bartekkuncer,2022-03-21 14:22:20,20826,[20982],Test bug,0,"After further investigation found out that the real problem was reading from neither updated nor cleared register during convolution with weights stored in blocked format. This problem does not occur when the amount of input channels is 4 and higher as full register is being overwritten. Fix for this issue is coming in v2.6 of oneDNN. Original shapes have been restored in the test. There was a temporary fix in place: [url] forcing blocking the weights by 8 instead of 16, but it turned out only to make the problem occur less often. Therefore until v2.6 of oneDNN arrives there will be no blocking the weights for convolutions with the amount of input channels lower than 4: [url]"
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/21053#issuecomment-1144618521,github-actions[bot],2022-06-02 08:55:29,21053,[21058],Data bug,0,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
"Hi @vitobellini , thanks for submitting your issue. I managed to get the reproduction and find the root cause. Fix is here: https://github.com/apache/incubator-mxnet/pull/21058.",IssueComment,https://github.com/apache/mxnet/issues/21053#issuecomment-1148460715,bartekkuncer,2022-06-07 10:02:16,21053,[21058],Data bug,0,"Hi @vitobellini , thanks for submitting your issue. I managed to get the reproduction and find the root cause. Fix is here: [url]"
"Here is also an easy way to set random seed for MXNet:
```import mxnet
import random
from mxnet import nd
mxnet.random.seed(random.randint(0, 2**31-1))
for _ in range(3):
    print(nd.random.uniform(0, 1).asscalar())",IssueComment,https://github.com/apache/mxnet/issues/21053#issuecomment-1148580155,bartekkuncer,2022-06-07 12:09:30,21053,[21058],Data bug,0,"Here is also an easy way to set random seed for MXNet: ```import mxnet import random from mxnet import nd mxnet.random.seed(random.randint(0, 2**31-1)) for _ in range(3): print(nd.random.uniform(0, 1).asscalar())"
thank you very much!,IssueComment,https://github.com/apache/mxnet/issues/21053#issuecomment-1150811306,vitobellini,2022-06-09 08:11:44,21053,[21058],Data bug,0,thank you very much!
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/20942#issuecomment-1061667930,github-actions[bot],2022-03-08 11:14:35,20942,[21087],Documentation bug,0,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
anybody?,IssueComment,https://github.com/apache/mxnet/issues/20942#issuecomment-1068700257,github-eliviate,2022-03-16 03:44:05,20942,[21087],Documentation bug,0,anybody?
"+1,
![mxnetpower](https://user-images.githubusercontent.com/82866148/176891600-03a4fbba-e2f3-4dab-b1a4-cb6eeb1d15ce.PNG)
maybe, broadcast_power(x, y)=x^y=[[ 1.,  1.,  1.],
     [ 1.,  1.,  1.]]",IssueComment,https://github.com/apache/mxnet/issues/20942#issuecomment-1172279370,Tongfengyu,2022-07-01 12:09:48,20942,[21087],Documentation bug,0,"+1, ![mxnetpower]([url] maybe, broadcast_power(x, y)=x^y=[[ 1., 1., 1.], [ 1., 1., 1.]]"
Fixed here: https://github.com/apache/incubator-mxnet/pull/21087.,IssueComment,https://github.com/apache/mxnet/issues/20942#issuecomment-1176119081,bartekkuncer,2022-07-06 11:42:03,20942,[21087],Documentation bug,0,Fixed here: [url]
"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue.
Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly.
If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet](https://mxnet.apache.org/community/contribute) and our [development guides wiki](https://cwiki.apache.org/confluence/display/MXNET/Developments).",IssueComment,https://github.com/apache/mxnet/issues/20731#issuecomment-962298680,github-actions[bot],2021-11-06 01:03:55,20731,[21090],Visualization bug,0,"Welcome to Apache MXNet (incubating)! We are on a mission to democratize AI, and we are glad that you are contributing to it by opening this issue. Please make sure to include all the relevant context, and one of the @apache/mxnet-committers will be here shortly. If you are interested in contributing to our project, let us know! Also, be sure to check out our guide on [contributing to MXNet]([url] and our [development guides wiki]([url]"
Will be fixed in #20716 ,IssueComment,https://github.com/apache/mxnet/issues/20731#issuecomment-962377608,barry-jin,2021-11-06 02:32:04,20731,[21090],Visualization bug,0,Will be fixed in #20716
Documentation is fixed: https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/gluon_migration_guide.html,IssueComment,https://github.com/apache/mxnet/issues/20731#issuecomment-962661232,barry-jin,2021-11-07 18:45:00,20731,[21090],Visualization bug,0,Documentation is fixed: [url]
"Thank you so much.

But some parts are still strange.",IssueComment,https://github.com/apache/mxnet/issues/20731#issuecomment-962700674,dai-ichiro,2021-11-07 23:20:58,20731,[21090],Visualization bug,0,Thank you so much. But some parts are still strange.
Fix here: https://github.com/apache/incubator-mxnet/pull/21090.,IssueComment,https://github.com/apache/mxnet/issues/20731#issuecomment-1177722574,bartekkuncer,2022-07-07 14:40:28,20731,[21090],Visualization bug,0,Fix here: [url]
"Thanks for reporting this, should be fixed after #665 get merged
",IssueComment,https://github.com/apache/mxnet/issues/659#issuecomment-158508196,tqchen,2015-11-20 19:53:08,659,[665],Data bug,1,"Thanks for reporting this, should be fixed after #665 get merged"
Why would int->int64 have any effect on this? 54686454 is less than max int,PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r146391780,piiswrong,2017-10-23 20:57:39,8303,8398,Data bug,1,Why would int->int64 have any effect on this? 54686454 is less than max int
"After discussing with @sxjscience , we found that https://github.com/apache/incubator-mxnet/issues/8303 is because the fix for range in mshadow https://github.com/dmlc/mshadow/commit/eced9571c601260e3a2deb0e8e9a41e8c0a9f0f1 is not included in mxnet master (range still use float).

So just replacing `range` with kernel launch can fix this issue. I change `int64` back to `int`. Or is `int64` better than `int` here? ",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r146546989,ZiyueHuang,2017-10-24 12:45:49,8303,8398,Data bug,1,"After discussing with @sxjscience , we found that [url] is because the fix for range in mshadow [url] is not included in mxnet master (range still use float). So just replacing [code] with kernel launch can fix this issue. I change [code] back to [code]. Or is [code] better than [code] here?"
OK lets change range to kernel launch since we are deprecating mshadow. int is fine for now. Leave a note and we'll support int64 when we do the int64 index support in general.,PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r146750455,piiswrong,2017-10-25 04:02:08,8303,8398,Data bug,1,OK lets change range to kernel launch since we are deprecating mshadow. int is fine for now. Leave a note and we'll support int64 when we do the int64 index support in general.
"repeat should be an integer.
also don't use dptr_. Use `dptr<DType>()`",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r146751859,piiswrong,2017-10-25 04:16:22,8303,8398,Data bug,1,repeat should be an integer. also don't use dptr_. Use [code]
"Thanks for your fix. But it is a `Tensor` not a `TBlob`, there is no such method `dptr<DType>()`.

I chenge `1.0f` to `1` since `repeat` is an integer.",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r146770669,ZiyueHuang,2017-10-25 07:11:42,8303,8398,Data bug,1,"Thanks for your fix. But it is a [code] not a [code], there is no such method [code]. I chenge [code] to [code] since [code] is an integer."
"Then you need to use a reinterprete_cast, otherwise there is no guarantee that the correct dtype will be used",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r147564138,piiswrong,2017-10-28 20:45:22,8303,8398,Data bug,1,"Then you need to use a reinterprete_cast, otherwise there is no guarantee that the correct dtype will be used"
"nvm, Tensor::dptr_ is actually typed",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r147564173,piiswrong,2017-10-28 20:46:15,8303,8398,Data bug,1,"nvm, Tensor::dptr_ is actually typed"
Is there any other way to test this rather than this big ndarray?,PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r180077093,larroy,2018-04-09 12:27:34,8303,8398,Data bug,1,Is there any other way to test this rather than this big ndarray?
Is not clear what was the problem and how the fix works. ,PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r180082288,larroy,2018-04-09 12:48:58,8303,8398,Data bug,1,Is not clear what was the problem and how the fix works.
"I think the problem is with the range being stored in a float, values bigger than the mantissa 2^24 will get truncated.",PrComment,https://github.com/apache/mxnet/pull/8398#discussion_r185568286,larroy,2018-05-02 17:02:54,8303,8398,Data bug,1,"I think the problem is with the range being stored in a float, values bigger than the mantissa 2^24 will get truncated."
use constexpr so that you don't need to define it separately,PrComment,https://github.com/apache/mxnet/pull/9256#discussion_r159145857,piiswrong,2017-12-31 19:37:35,9229,9256,Build bug,1,use constexpr so that you don't need to define it separately
"I think we can also revise the `check_label_shapes` above to be `check_label_shapes(labels, preds, 1)` to force the labels and preds have the same shape. Currently the default behavior is to just match the `ndim`.",PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171426781,sxjscience,2018-03-01 00:03:06,9865,9930,Algorithm design bug,0,I think we can also revise the [code] above to be [code] to force the labels and preds have the same shape. Currently the default behavior is to just match the [code].
Is there a reason to set `shape=0` by default?,PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171428505,hetong007,2018-03-01 00:13:00,9865,9930,Algorithm design bug,0,Is there a reason to set [code] by default?
Not sure. I think shape=1 is more suitable to be the default.,PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171429077,sxjscience,2018-03-01 00:16:33,9865,9930,Algorithm design bug,0,Not sure. I think shape=1 is more suitable to be the default.
@piiswrong can we have your input?,PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171436622,hetong007,2018-03-01 01:02:28,9865,9930,Algorithm design bug,0,@piiswrong can we have your input?
"I think the check_label_shapes function should be redesigned.
1. it should test if labels is a single NDArray instead of a list, and wrap it with a list `labels=[labels]`
2. 0/1 is not pythonic. Use boolean switches
",PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171662699,piiswrong,2018-03-01 19:11:28,9865,9930,Algorithm design bug,0,"I think the check_label_shapes function should be redesigned. 1. it should test if labels is a single NDArray instead of a list, and wrap it with a list [code] 2. 0/1 is not pythonic. Use boolean switches"
Why don't we check both length and shape? I assume they shoud always match.,PrComment,https://github.com/apache/mxnet/pull/9930#discussion_r171671073,hetong007,2018-03-01 19:40:08,9865,9930,Algorithm design bug,0,Why don't we check both length and shape? I assume they shoud always match.
"Use `assert same(a_np, a_nd.asnumpy())`. Same for all others.",PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172037661,reminisce,2018-03-04 05:22:10,9976,9981,Data bug,1,Use [code]. Same for all others.
What's the purpose of this check? This seems always true.,PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172037688,reminisce,2018-03-04 05:24:38,9976,9981,Data bug,1,What's the purpose of this check? This seems always true.
"1. Add issue link as code comment.
2. Change the function name to a special one since this is only for testing special cases. General cases have been covered by `test_ndarray_indexing`.",PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172037698,reminisce,2018-03-04 05:25:48,9976,9981,Data bug,1,1. Add issue link as code comment. 2. Change the function name to a special one since this is only for testing special cases. General cases have been covered by [code].
should we also check for size? @reminisce ,PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172040979,piiswrong,2018-03-04 08:31:43,9976,9981,Data bug,1,should we also check for size? @reminisce
"@piiswrong There is a shape equality check after this. For `CopyFromTo`, I think the added condition is sufficient. Agree we should also check sizes if it is for a function of checking if two ndarrays are the same one.",PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172065630,reminisce,2018-03-04 21:37:39,9976,9981,Data bug,1,"@piiswrong There is a shape equality check after this. For [code], I think the added condition is sufficient. Agree we should also check sizes if it is for a function of checking if two ndarrays are the same one."
Thank you! I will modify it.,PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172075502,wkcn,2018-03-05 01:31:15,9976,9981,Data bug,1,Thank you! I will modify it.
"My mistake. I want to check the memory address of NDArray didn't change.
And `assert np.allclose(a_np, a_nd.asnumpy())` means checking issue #9976.",PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172075822,wkcn,2018-03-05 01:33:26,9976,9981,Data bug,1,My mistake. I want to check the memory address of NDArray didn't change. And [code] means checking issue #9976.
Okay. Thanks!,PrComment,https://github.com/apache/mxnet/pull/9981#discussion_r172075839,wkcn,2018-03-05 01:33:39,9976,9981,Data bug,1,Okay. Thanks!
Suggest we should not remove the PTB experiments. I think it's originally included in the gluon example.,PrComment,https://github.com/apache/mxnet/pull/10225#discussion_r176860732,sxjscience,2018-03-23 20:54:53,10224,10225,Code bug,1,Suggest we should not remove the PTB experiments. I think it's originally included in the gluon example.
We can bring it back once the CorpusReader is available in the main branch.,PrComment,https://github.com/apache/mxnet/pull/10225#discussion_r176860890,szha,2018-03-23 20:55:24,10224,10225,Code bug,1,We can bring it back once the CorpusReader is available in the main branch.
"The current default setting is problematic. After running the experiment, the best testing ppl is 106.9",PrComment,https://github.com/apache/mxnet/pull/10225#discussion_r176873734,szhengac,2018-03-23 21:56:47,10224,10225,Code bug,1,"The current default setting is problematic. After running the experiment, the best testing ppl is 106.9"
Will update the number. Let's find a decent setting and put it back in later.,PrComment,https://github.com/apache/mxnet/pull/10225#discussion_r176874393,szha,2018-03-23 22:00:23,10224,10225,Code bug,1,Will update the number. Let's find a decent setting and put it back in later.
indentation?,PrComment,https://github.com/apache/mxnet/pull/10400#discussion_r179225126,eric-haibin-lin,2018-04-04 17:39:52,8524,10400,Data bug,1,indentation?
nit: `CheckAndAlloc(const std::vector<TShape> &aux_shapes)` will make it neater.. ,PrComment,https://github.com/apache/mxnet/pull/10400#discussion_r179225556,eric-haibin-lin,2018-04-04 17:41:25,8524,10400,Data bug,1,nit: [code] will make it neater..
log(fatal)? ,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191000176,eric-haibin-lin,2018-05-25 20:17:25,10453,11041,Memory bug ,1,log(fatal)?
are you sure + NDEV is not needed any more? what if NDEV=32 and min_chunk=33 and handle.size=30? Original code would allocate 62. New code would allocate 33,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191615781,piiswrong,2018-05-30 00:47:23,10453,11041,Memory bug ,1,are you sure + NDEV is not needed any more? what if NDEV=32 and min_chunk=33 and handle.size=30? Original code would allocate 62. New code would allocate 33
page size instead of min chunk?,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191615805,piiswrong,2018-05-30 00:47:37,10453,11041,Memory bug ,1,page size instead of min chunk?
new line,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191615822,piiswrong,2018-05-30 00:47:49,10453,11041,Memory bug ,1,new line
"does this need to be so complicated? You just need to take the highest bit and shift left by 1 if it's smaller than size.

This is called the finding the MSB. See https://www.google.com/search?ei=__UNW-DMG6iF0wLqyr4g&q=how+to+find+most+significant+bit+in+c&oq=take+highest+bit&gs_l=psy-ab.1.0.0i71k1l8.0.0.0.4417.0.0.0.0.0.0.0.0..0.0....0...1c..64.psy-ab..0.0.0....0.LUbIFjlZyeU",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191616515,piiswrong,2018-05-30 00:54:25,10453,11041,Memory bug ,1,does this need to be so complicated? You just need to take the highest bit and shift left by 1 if it's smaller than size. This is called the finding the MSB. See [url]
"cc'd @ptrendx. My understanding on this was that there needs to be enough bytes to make sure that for 32 devices at least each device has 1 byte, for nccl scattering. Could you confirm, @ptrendx?",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191633830,szha,2018-05-30 03:30:56,10453,11041,Memory bug ,1,"cc'd @ptrendx. My understanding on this was that there needs to be enough bytes to make sure that for 32 devices at least each device has 1 byte, for nccl scattering. Could you confirm, @ptrendx?"
"Yes, that is correct. ",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r191843458,ptrendx,2018-05-30 16:54:03,10453,11041,Memory bug ,1,"Yes, that is correct."
these builtins would utilize hardware instructions when available.,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r192253368,szha,2018-05-31 22:15:45,10453,11041,Memory bug ,1,these builtins would utilize hardware instructions when available.
Is it really faster? It looks too complicated.,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r192496824,piiswrong,2018-06-01 19:42:04,10453,11041,Memory bug ,1,Is it really faster? It looks too complicated.
also the default implementation with pow and log is really slow,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r192497796,piiswrong,2018-06-01 19:46:44,10453,11041,Memory bug ,1,also the default implementation with pow and log is really slow
I will change the default implementation to use bit shifting and then do a comparison,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r192523184,szha,2018-06-01 21:46:30,10453,11041,Memory bug ,1,I will change the default implementation to use bit shifting and then do a comparison
"I compared my current solution, the bit shifting, and `static_cast<int>(std::ceil(std::log2(s)))`, with `-O3` is turned on on my mac (clang), the speed looks like the following:
```
Running 10000000 iters.
Addr width 64
It took me 0.00981569 seconds. result: 223222785
It took me 0.128623 seconds. result: 223222785
It took me 0.0801588 seconds. result: 223222785
```",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r192616747,szha,2018-06-04 02:44:28,10453,11041,Memory bug ,1,"I compared my current solution, the bit shifting, and [code], with [code] is turned on on my mac (clang), the speed looks like the following: ``[code]``"
revert,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r193599496,piiswrong,2018-06-07 00:43:04,10453,11041,Memory bug ,1,revert
How long does this variable persist? It could have side effects on other tests,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194218296,marcoabreu,2018-06-09 07:48:55,10453,11041,Memory bug ,1,How long does this variable persist? It could have side effects on other tests
Duplicate import? I think it's already part of the storage namespace at mxnet/storage.h,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194218366,marcoabreu,2018-06-09 07:51:20,10453,11041,Memory bug ,1,Duplicate import? I think it's already part of the storage namespace at mxnet/storage.h
Is it really necessary to import this in every single test? Looks a bit ugly tbh,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194464308,marcoabreu,2018-06-11 16:24:15,10453,11041,Memory bug ,1,Is it really necessary to import this in every single test? Looks a bit ugly tbh
"applying this change would allow all tests within a module to finish before moving onto the next test, thus eliminating the case where side effect of tests in another module spills over to the next. In terms of testing practice, including a setup/teardown is common.",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194471060,szha,2018-06-11 16:46:19,10453,11041,Memory bug ,1,"applying this change would allow all tests within a module to finish before moving onto the next test, thus eliminating the case where side effect of tests in another module spills over to the next. In terms of testing practice, including a setup/teardown is common."
"Yeah, but we're not actually using it in most files, right?",PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194471886,marcoabreu,2018-06-11 16:49:08,10453,11041,Memory bug ,1,"Yeah, but we're not actually using it in most files, right?"
now we are,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194472380,szha,2018-06-11 16:50:45,10453,11041,Memory bug ,1,now we are
Ah in common.py :) But isn't it sufficient to import it there?,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194473467,marcoabreu,2018-06-11 16:54:34,10453,11041,Memory bug ,1,Ah in common.py :) But isn't it sufficient to import it there?
unfortunately no. it is the same case as setup_module,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194478459,szha,2018-06-11 17:12:02,10453,11041,Memory bug ,1,unfortunately no. it is the same case as setup_module
argh :/,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194490302,marcoabreu,2018-06-11 17:50:50,10453,11041,Memory bug ,1,argh :/
What will happen to the storage handles currently pointing to some of the memory?,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194660923,lebeg,2018-06-12 08:52:26,10453,11041,Memory bug ,1,What will happen to the storage handles currently pointing to some of the memory?
Even if it's no error (the rvalue reference will de deduced to normal lvalue reference) it's better to use it explicitly as auto&,PrComment,https://github.com/apache/mxnet/pull/11041#discussion_r194661450,lebeg,2018-06-12 08:53:54,10453,11041,Memory bug ,1,Even if it's no error (the rvalue reference will de deduced to normal lvalue reference) it's better to use it explicitly as auto&
You can use `self.params.get` to declare the parameter so that it doesn't require model.params.update later,PrComment,https://github.com/apache/mxnet/pull/11375#discussion_r197624021,szha,2018-06-23 22:33:31,11352,11375,Algorithm design bug,0,You can use [code] to declare the parameter so that it doesn't require model.params.update later
see above comment,PrComment,https://github.com/apache/mxnet/pull/11375#discussion_r197624027,szha,2018-06-23 22:33:54,11352,11375,Algorithm design bug,0,see above comment
"`cmp = lambda(x, y): (x > y) - (x < y)` could also work with a bit less code",PrComment,https://github.com/apache/mxnet/pull/12295#discussion_r212266865,lebeg,2018-08-23 10:59:38,8270,12295,Version compatibility bug,0,[code] could also work with a bit less code
"1. It is a Python 3 syntax error
2. With the syntax fixed it generates a PEP8 violation in flake8 https://stackoverflow.com/questions/25010167/e731-do-not-assign-a-lambda-expression-use-a-def",PrComment,https://github.com/apache/mxnet/pull/12295#discussion_r212270403,cclauss,2018-08-23 11:14:37,8270,12295,Version compatibility bug,0,1. It is a Python 3 syntax error 2. With the syntax fixed it generates a PEP8 violation in flake8 [url]
"Ok, I see. Thanks!",PrComment,https://github.com/apache/mxnet/pull/12295#discussion_r212271093,lebeg,2018-08-23 11:17:36,8270,12295,Version compatibility bug,0,"Ok, I see. Thanks!"
"spaces 
kernel_size % 2, 0",PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217575519,Vikas-kum,2018-09-14 00:52:48,9034,12558,Documentation bug,1,"spaces kernel_size % 2, 0"
"print? instead do you want to assert?
Also, I would recommend to add a test case with odd version number
",PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217576278,Vikas-kum,2018-09-14 00:58:51,9034,12558,Documentation bug,1,"print? instead do you want to assert? Also, I would recommend to add a test case with odd version number"
Cool incorporating that,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217577379,ChaiBapchya,2018-09-14 01:08:35,9034,12558,Documentation bug,1,Cool incorporating that
Yup thanks,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217577393,ChaiBapchya,2018-09-14 01:08:42,9034,12558,Documentation bug,1,Yup thanks
I think these are already imported in this module,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217773161,apeforest,2018-09-14 16:39:39,9034,12558,Documentation bug,1,I think these are already imported in this module
"Agree with @Vikas89. I think your PR is mainly to test the error message. Therefore, you do not need to test the regular case and check output. Instead, you may want to test the case where the exception will be raised. ",PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217773810,apeforest,2018-09-14 16:41:47,9034,12558,Documentation bug,1,"Agree with @Vikas89. I think your PR is mainly to test the error message. Therefore, you do not need to test the regular case and check output. Instead, you may want to test the case where the exception will be raised."
True. My bad. Removing it.,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217784185,ChaiBapchya,2018-09-14 17:20:28,9034,12558,Documentation bug,1,True. My bad. Removing it.
"I am not testing the regular case (kernel_size being odd).
Exception is raised when it is even. I have tested for even number (kernel_size=28).
But yes, here the aim is only to test if error is raised or not, hence removing the print statement since not needed. Only function to be tested is the `forward`
Hence keeping at that.",PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217785257,ChaiBapchya,2018-09-14 17:24:20,9034,12558,Documentation bug,1,"I am not testing the regular case (kernel_size being odd). Exception is raised when it is even. I have tested for even number (kernel_size=28). But yes, here the aim is only to test if error is raised or not, hence removing the print statement since not needed. Only function to be tested is the [code] Hence keeping at that."
"Sorry, I misunderstood. My earlier comment is to ask you to test for the case that invoked the exception. Please see the test utility mxnet.test_utils.assert_exception for example. @Vikas89 asked to also test the regular case (odd kernel_size) to make sure it works as before (if we don't have such test already, I think is also necessary).",PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217856085,apeforest,2018-09-14 22:22:00,9034,12558,Documentation bug,1,"Sorry, I misunderstood. My earlier comment is to ask you to test for the case that invoked the exception. Please see the test utility mxnet.test_utils.assert_exception for example. @Vikas89 asked to also test the regular case (odd kernel_size) to make sure it works as before (if we don't have such test already, I think is also necessary)."
Yup. Added both. Thanks for the clarification.,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r217869730,ChaiBapchya,2018-09-15 00:40:35,9034,12558,Documentation bug,1,Yup. Added both. Thanks for the clarification.
nit: add space round '%',PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r218137843,apeforest,2018-09-17 16:27:09,9034,12558,Documentation bug,1,nit: add space round '%'
nit: break the long line.,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r218138022,apeforest,2018-09-17 16:27:46,9034,12558,Documentation bug,1,nit: break the long line.
same here,PrComment,https://github.com/apache/mxnet/pull/12558#discussion_r218138082,apeforest,2018-09-17 16:27:55,9034,12558,Documentation bug,1,same here
could we assert the real output value instead of nan to make the test more robust?,PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r234858561,stu1130,2018-11-20 03:26:13,13141,13328,Version compatibility bug,0,could we assert the real output value instead of nan to make the test more robust?
"Could you build other corner cases to cover more, such as 1e-30 as input or other possible combinations?",PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r234864294,pengzhao-intel,2018-11-20 04:14:57,13141,13328,Version compatibility bug,0,"Could you build other corner cases to cover more, such as 1e-30 as input or other possible combinations?"
Asserting real value. Thanks for pointing that!,PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r235212538,mseth10,2018-11-20 23:57:17,13141,13328,Version compatibility bug,0,Asserting real value. Thanks for pointing that!
Added more corner cases: extremely large positive and negative inputs covered (also checked for max and min values of float32 inputs). Please suggest if you think there are more corner cases that should be added.,PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r235239845,mseth10,2018-11-21 02:58:37,13141,13328,Version compatibility bug,0,Added more corner cases: extremely large positive and negative inputs covered (also checked for max and min values of float32 inputs). Please suggest if you think there are more corner cases that should be added.
can we use assert_almost_equal in test_utils.,PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r235242621,anirudh2290,2018-11-21 03:20:33,13141,13328,Version compatibility bug,0,can we use assert_almost_equal in test_utils.
assert_almost_equal makes more sense as we are comparing floats here. Corrected.,PrComment,https://github.com/apache/mxnet/pull/13328#discussion_r235278053,mseth10,2018-11-21 07:37:24,13141,13328,Version compatibility bug,0,assert_almost_equal makes more sense as we are comparing floats here. Corrected.
this line should be outside the else,PrComment,https://github.com/apache/mxnet/pull/13413#discussion_r237206118,vandanavk,2018-11-28 18:26:26,13300,13413,Deployment bug,0,this line should be outside the else
done,PrComment,https://github.com/apache/mxnet/pull/13413#discussion_r239906264,Roshrini,2018-12-07 18:43:51,13300,13413,Deployment bug,0,done
"Sorry for my naive understanding. But, I do not understand why +1? And this mean, we will have one extra size in the workspace always?",PrComment,https://github.com/apache/mxnet/pull/13727#discussion_r244032106,sandeep-krishnamurthy,2018-12-26 18:28:41,13710,13727,Memory bug,0,"Sorry for my naive understanding. But, I do not understand why +1? And this mean, we will have one extra size in the workspace always?"
"Yes, we will have extra size sometimes when we need to hold int32_t data in DType workspace, and the calculation assures that only the smallest needed space is allocated. 
```
if workspace = (N * sizeof(int32_t) - 1) / sizeof(DType) + 1,
then workspace * sizeof(DType) >= N * sizeof(int32_t) - 1 - (sizeof(DType) - 1) + sizeof(DType)
                                = N * sizeof(int32_t)
```
Actually, [L426](https://github.com/apache/incubator-mxnet/blob/602376066c55d2783691a19cb07c9e7eb4df532c/src/operator/contrib/bounding_box-inl.h#L426) has already calculated correctly.",PrComment,https://github.com/apache/mxnet/pull/13727#discussion_r244089639,arcadiaphy,2018-12-27 06:22:37,13710,13727,Memory bug,0,"Yes, we will have extra size sometimes when we need to hold int32_t data in DType workspace, and the calculation assures that only the smallest needed space is allocated. ``[code]`` Actually, [L426]([url]#L426) has already calculated correctly."
why are we changing `set_lower_bound` to `set_default`. This is for proper error messaging when user provides bad inputs.,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257824829,anirudh2290,2019-02-18 20:55:08,13078,14035,Documentation bug,1,why are we changing [code] to [code]. This is for proper error messaging when user provides bad inputs.
num_args is populated for variable length inputs without users having to worry about it. If we remove this the additional num_args field can confuse users. ,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257827587,anirudh2290,2019-02-18 21:09:49,13078,14035,Documentation bug,1,num_args is populated for variable length inputs without users having to worry about it. If we remove this the additional num_args field can confuse users.
We can just move this information to the description of data field.,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257827673,anirudh2290,2019-02-18 21:10:10,13078,14035,Documentation bug,1,We can just move this information to the description of data field.
you could use `itertools.product` like here - https://github.com/anirudhacharya/incubator-mxnet/blob/master/tests/python/unittest/test_optimizer.py#L1225,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257837616,anirudhacharya,2019-02-18 22:04:44,13078,14035,Documentation bug,1,you could use [code] like here - [url]#L1225
missed changing this back. will do that,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257852858,vandanavk,2019-02-18 23:47:19,13078,14035,Documentation bug,1,missed changing this back. will do that
"ok, then i'll just add a note in the description about providing 2 inputs for bilinear sampling",PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257852921,vandanavk,2019-02-18 23:47:45,13078,14035,Documentation bug,1,"ok, then i'll just add a note in the description about providing 2 inputs for bilinear sampling"
ok,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257852934,vandanavk,2019-02-18 23:47:53,13078,14035,Documentation bug,1,ok
ok,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257852964,vandanavk,2019-02-18 23:48:06,13078,14035,Documentation bug,1,ok
Why did you change the unit test if your PR is to fix documentation?,PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r257860513,apeforest,2019-02-19 00:57:17,13078,14035,Documentation bug,1,Why did you change the unit test if your PR is to fix documentation?
"@apeforest the unit test was not being called previously. And based on inputs from the issues in Ref section of the PR description, many users are confused about how to use the Upsampling operator with the bilinear upsampling option. So i thought I could add/modify tests here as well - which would show how to use the operator. 
I have updated the PR title - should I open a separate PR instead?

Note: This PR is WIP as an assert has to be added at the end of the test. Based on an offline discussion with @anirudh2290 yesterday, the assert will be similar to what was done for test_deconvolution()",PrComment,https://github.com/apache/mxnet/pull/14035#discussion_r258125213,vandanavk,2019-02-19 16:38:44,13078,14035,Documentation bug,1,"@apeforest the unit test was not being called previously. And based on inputs from the issues in Ref section of the PR description, many users are confused about how to use the Upsampling operator with the bilinear upsampling option. So i thought I could add/modify tests here as well - which would show how to use the operator. I have updated the PR title - should I open a separate PR instead? Note: This PR is WIP as an assert has to be added at the end of the test. Based on an offline discussion with @anirudh2290 yesterday, the assert will be similar to what was done for test_deconvolution()"
Can we just use `index_t` for M so you don't need so many casting in this function? ,PrComment,https://github.com/apache/mxnet/pull/14082#discussion_r255763840,apeforest,2019-02-12 01:11:43,9314,14082,Data bug,1,Can we just use [code] for M so you don't need so many casting in this function?
+1 for using index_t for M.,PrComment,https://github.com/apache/mxnet/pull/14082#discussion_r255766069,yuxihu,2019-02-12 01:20:23,9314,14082,Data bug,1,+1 for using index_t for M.
"Sure. i looked at other places where large array support was mentioned. It seemed that
`index_t` was used where the variable is used as index
and `size_t` was used otherwise. Hence I stuck to that. But I'll change it to avoid casting if that's what's useful.",PrComment,https://github.com/apache/mxnet/pull/14082#discussion_r255799692,ChaiBapchya,2019-02-12 04:31:27,9314,14082,Data bug,1,Sure. i looked at other places where large array support was mentioned. It seemed that [code] was used where the variable is used as index and [code] was used otherwise. Hence I stuck to that. But I'll change it to avoid casting if that's what's useful.
This test is not sufficient. I ran it in current MXNet master and it passed as well.,PrComment,https://github.com/apache/mxnet/pull/14082#discussion_r256534634,apeforest,2019-02-13 18:44:04,9314,14082,Data bug,1,This test is not sufficient. I ran it in current MXNet master and it passed as well.
The new test would fail on master without the fix. Looks good now,PrComment,https://github.com/apache/mxnet/pull/14082#discussion_r257346109,eric-haibin-lin,2019-02-15 18:39:16,9314,14082,Data bug,1,The new test would fail on master without the fix. Looks good now
Can you add code comments here?,PrComment,https://github.com/apache/mxnet/pull/14121#discussion_r256086709,Roshrini,2019-02-12 18:27:54,13061,14121,Deployment bug,0,Can you add code comments here?
Done,PrComment,https://github.com/apache/mxnet/pull/14121#discussion_r256089109,vandanavk,2019-02-12 18:33:48,13061,14121,Deployment bug,0,Done
"The curly braces are not needed. You can just write `def outputShapes: IndexedSeq[(String, Shape)] = predictor.outputShapes`. Same for predictor.scala as well",PrComment,https://github.com/apache/mxnet/pull/14215#discussion_r258745903,zachgk,2019-02-21 01:18:42,14181,14215,Code bug,0,The curly braces are not needed. You can just write [code]. Same for predictor.scala as well
done,PrComment,https://github.com/apache/mxnet/pull/14215#discussion_r259563413,frankfliu,2019-02-23 03:27:50,14181,14215,Code bug,0,done
"Thanks for the quick fix! 
nit: could you change `test_SoftmaxOutput_normalization` to `test_softmax_output_normalization `? ",PrComment,https://github.com/apache/mxnet/pull/14302#discussion_r261851559,eric-haibin-lin,2019-03-03 04:52:57,14301,14302,Algorithm design bug,1,Thanks for the quick fix! nit: could you change [code] to [code]?
"I will change it later. Thank you!

Edit: The test case has been renamed : )",PrComment,https://github.com/apache/mxnet/pull/14302#discussion_r261851874,wkcn,2019-03-03 05:11:40,14301,14302,Algorithm design bug,1,I will change it later. Thank you! Edit: The test case has been renamed : )
nit: `mx.random.uniform` is the preferred way now.,PrComment,https://github.com/apache/mxnet/pull/14314#discussion_r261943116,szha,2019-03-04 07:22:00,14233,14314,Data bug,0,nit: [code] is the preferred way now.
Updated. Thank you!,PrComment,https://github.com/apache/mxnet/pull/14314#discussion_r261947315,wkcn,2019-03-04 07:42:30,14233,14314,Data bug,0,Updated. Thank you!
Can you also test end < -1 as in the branch on line https://github.com/apache/incubator-mxnet/pull/14403/files#diff-d5c9c7d539907caef01a08a74e16592bR689,PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265715731,apeforest,2019-03-14 18:44:08,13760,14403,Data bug,1,Can you also test end < -1 as in the branch on line [url]#diff-d5c9c7d539907caef01a08a74e16592bR689
"Please iterate different begin, end, step combinations to make the test more complete.",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265716156,apeforest,2019-03-14 18:45:13,13760,14403,Data bug,1,"Please iterate different begin, end, step combinations to make the test more complete."
"Can we give a more informative error message than just `len`? e.g. ""the shape size `len` in dimension `i`""?",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265717694,apeforest,2019-03-14 18:48:50,13760,14403,Data bug,1,"Can we give a more informative error message than just [code]? e.g. ""the shape size [code] in dimension [code]""?"
"Can we give a more informative error message than just `len`? e.g. ""the shape size `len` in dimension `i`""?",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265717723,apeforest,2019-03-14 18:48:54,13760,14403,Data bug,1,"Can we give a more informative error message than just [code]? e.g. ""the shape size [code] in dimension [code]""?"
"Can we give a more informative error message than just `len`? e.g. ""the shape size `len` in dimension `i`""?",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265717762,apeforest,2019-03-14 18:49:00,13760,14403,Data bug,1,"Can we give a more informative error message than just [code]? e.g. ""the shape size [code] in dimension [code]""?"
This error message is in accordance with the other error messages we have in this function. I am just correcting a typo here. Do you think we should change all the error messages?,PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265813981,mseth10,2019-03-15 00:23:16,13760,14403,Data bug,1,This error message is in accordance with the other error messages we have in this function. I am just correcting a typo here. Do you think we should change all the error messages?
"Yes, I think providing a clear error message will improve user experience significantly.",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r265821070,apeforest,2019-03-15 01:11:38,13760,14403,Data bug,1,"Yes, I think providing a clear error message will improve user experience significantly."
Modified the error message to be more informative.,PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r266115768,mseth10,2019-03-15 19:13:39,13760,14403,Data bug,1,Modified the error message to be more informative.
"Can you use a nested for loop here to test various combination of shape, step, begin and end?",PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r266146549,apeforest,2019-03-15 21:05:10,13760,14403,Data bug,1,"Can you use a nested for loop here to test various combination of shape, step, begin and end?"
Added more unit tests.,PrComment,https://github.com/apache/mxnet/pull/14403#discussion_r266155815,mseth10,2019-03-15 21:43:30,13760,14403,Data bug,1,Added more unit tests.
why use `__dict__`? or -1 index?,PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265381261,szha,2019-03-14 00:19:48,13616,14423,Code bug,0,why use [code]? or -1 index?
would it be more useful to show also the input count and output count?,PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265381409,szha,2019-03-14 00:20:40,13616,14423,Code bug,0,would it be more useful to show also the input count and output count?
"corrected __dict__ to self._cached_graph. 
-1 index as the last element of cached_graph is the output. I could have used cached_graph[1], but did not want to hit index out of bounds (in case it ever happens).

",PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265391945,vandanavk,2019-03-14 01:28:48,13616,14423,Code bug,0,"corrected __dict__ to self._cached_graph. -1 index as the last element of cached_graph is the output. I could have used cached_graph[1], but did not want to hit index out of bounds (in case it ever happens)."
"I think showing all layers like how HybridSequential prints, would be most useful, but not sure how to do that for Symbol API and SymbolBlock models.",PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265392063,vandanavk,2019-03-14 01:29:42,13616,14423,Code bug,0,"I think showing all layers like how HybridSequential prints, would be most useful, but not sure how to do that for Symbol API and SymbolBlock models."
"Printing the inner structure might be too complicated and also not readable, thus I suggested printing the count.

Why would `cached_graph[1]` ever hit index out of bounds?",PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265398834,szha,2019-03-14 02:17:05,13616,14423,Code bug,0,"Printing the inner structure might be too complicated and also not readable, thus I suggested printing the count. Why would [code] ever hit index out of bounds?"
"something like this?

```
SymbolBlock(
<Symbol dense1_fwd>
)
Number of inputs: 1
Number of outputs: 1
```",PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265729992,vandanavk,2019-03-14 19:22:54,13616,14423,Code bug,0,something like this? ``[code]``
`<Symbol dense1_fwd>: 1->1` ?,PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265766306,szha,2019-03-14 21:06:59,13616,14423,Code bug,0,[code] ?
@szha thanks for the inputs. submitted after addressing the comments. Added an example of multiple outputs in the PR description,PrComment,https://github.com/apache/mxnet/pull/14423#discussion_r265782414,vandanavk,2019-03-14 21:58:06,13616,14423,Code bug,0,@szha thanks for the inputs. submitted after addressing the comments. Added an example of multiple outputs in the PR description
can we call Engine stop before customop stop. This will ensure that all ops pushed to engine have executed before stopping engine.,PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266244339,anirudh2290,2019-03-17 15:19:57,14396,14451,Code bug,1,can we call Engine stop before customop stop. This will ensure that all ops pushed to engine have executed before stopping engine.
I take that back. Maybe that won't work since Stopping engine before custom op means there may be ops that havent been pushed to engine by custom op but it has stopped. Another option is to add waitall after the worker joined in CustomOp stop to make sure all pushed ops have completed.,PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266244846,anirudh2290,2019-03-17 15:31:08,14396,14451,Code bug,1,I take that back. Maybe that won't work since Stopping engine before custom op means there may be ops that havent been pushed to engine by custom op but it has stopped. Another option is to add waitall after the worker joined in CustomOp stop to make sure all pushed ops have completed.
"The second option doesn't work either. When it goes into Stop function, the custom worker thread has already frozen on python function, leading to the blocking of worker join. I cannot think of a simple and graceful way to complete or ignore the unfinished custom operation other than manually add waiting command in python code.",PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266248347,arcadiaphy,2019-03-17 16:52:33,14396,14451,Code bug,1,"The second option doesn't work either. When it goes into Stop function, the custom worker thread has already frozen on python function, leading to the blocking of worker join. I cannot think of a simple and graceful way to complete or ignore the unfinished custom operation other than manually add waiting command in python code."
do you know why it is frozen on worker join ? The worker should just push the operator to engine and return.,PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266249264,anirudh2290,2019-03-17 17:15:28,14396,14451,Code bug,1,do you know why it is frozen on worker join ? The worker should just push the operator to engine and return.
"It's very easy to understand the first situation. The destruction of static variable on engine exit happens when python interpreter has been shutdown, so something bad happens when you call CFUNCTYPE function at that time.
I push a new commit to move custom Stop function to MXNotifyShutdown, forcing custom function called before python shutdown, so the first situation is all right.",PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266250500,arcadiaphy,2019-03-17 17:44:21,14396,14451,Code bug,1,"It's very easy to understand the first situation. The destruction of static variable on engine exit happens when python interpreter has been shutdown, so something bad happens when you call CFUNCTYPE function at that time. I push a new commit to move custom Stop function to MXNotifyShutdown, forcing custom function called before python shutdown, so the first situation is all right."
"The second situation of forking is so mysterious, I don't have a clue now. The custom Stop function is registered in prepare fork handler, everything should be OK at that time, but the  worker thread just freezes.",PrComment,https://github.com/apache/mxnet/pull/14451#discussion_r266250694,arcadiaphy,2019-03-17 17:49:43,14396,14451,Code bug,1,"The second situation of forking is so mysterious, I don't have a clue now. The custom Stop function is registered in prepare fork handler, everything should be OK at that time, but the worker thread just freezes."
"you don't need to import `math`, use `np` which is already imported",PrComment,https://github.com/apache/mxnet/pull/14585#discussion_r271439731,zhreshold,2019-04-02 18:28:40,14505,14585,Data bug,0,"you don't need to import [code], use [code] which is already imported"
done!,PrComment,https://github.com/apache/mxnet/pull/14585#discussion_r271460514,abhinavs95,2019-04-02 19:25:43,14505,14585,Data bug,0,done!
"```suggestion
Two algorithms (``sample_type``) are available for upsampling:
```",PrComment,https://github.com/apache/mxnet/pull/14919#discussion_r282627992,aaronmarkham,2019-05-09 19:18:56,12970,14919,Test bug,0,``[code][code][code]``
"Why not just say:
```suggestion
Input data is expected to be NCHW.
```",PrComment,https://github.com/apache/mxnet/pull/14919#discussion_r282628667,aaronmarkham,2019-05-09 19:20:58,12970,14919,Test bug,0,Why not just say: ``[code]``
"Why not just say:
```suggestion
Input data is expected to be NCHW.
```",PrComment,https://github.com/apache/mxnet/pull/14919#discussion_r282629477,aaronmarkham,2019-05-09 19:23:19,12970,14919,Test bug,0,Why not just say: ``[code]``
why should None be mapped to INT_MAX?,PrComment,https://github.com/apache/mxnet/pull/14942#discussion_r319309924,anirudhacharya,2019-08-29 23:28:35,14875,14942,Deployment bug,0,why should None be mapped to INT_MAX?
I think it is sufficient to map None to the dimension of the particular axis. Changing this in the next iteration.,PrComment,https://github.com/apache/mxnet/pull/14942#discussion_r319693989,vandanavk,2019-08-30 22:50:20,14875,14942,Deployment bug,0,I think it is sufficient to map None to the dimension of the particular axis. Changing this in the next iteration.
@anirudhacharya fixed this. the latest commit does not map None to INT_MAX,PrComment,https://github.com/apache/mxnet/pull/14942#discussion_r362652961,vandanavk,2020-01-02 22:24:36,14875,14942,Deployment bug,0,@anirudhacharya fixed this. the latest commit does not map None to INT_MAX
"if `end` does not exist in new_attrs, converting it to a list will throw NoneType error. ( unless onnx spec requires ends as a required attribute?)",PrComment,https://github.com/apache/mxnet/pull/14942#discussion_r362656297,anirudhacharya,2020-01-02 22:37:54,14875,14942,Deployment bug,0,"if [code] does not exist in new_attrs, converting it to a list will throw NoneType error. ( unless onnx spec requires ends as a required attribute?)"
"Its a required field in ONNX, will always exist https://github.com/onnx/onnx/blob/master/docs/Changelog.md#slice-1",PrComment,https://github.com/apache/mxnet/pull/14942#discussion_r362656572,vandanavk,2020-01-02 22:38:53,14875,14942,Deployment bug,0,"Its a required field in ONNX, will always exist [url]#slice-1"
Raspbian and TX2 links are broken,PrComment,https://github.com/apache/mxnet/pull/14987#discussion_r288308183,IvyBazan,2019-05-28 21:23:02,14895,14987,Build bug,0,Raspbian and TX2 links are broken
"I could fix the tx2 one, but not raspbian in a great way since that install page is pretty messed up. ",PrComment,https://github.com/apache/mxnet/pull/14987#discussion_r288351954,aaronmarkham,2019-05-29 00:21:53,14895,14987,Build bug,0,"I could fix the tx2 one, but not raspbian in a great way since that install page is pretty messed up."
"Any other types in here? 
Could you add some comments?",PrComment,https://github.com/apache/mxnet/pull/15637#discussion_r306158978,pengzhao-intel,2019-07-23 07:10:41,15555,15637,Data bug,0,Any other types in here? Could you add some comments?
"Only for `kCrossDeviceCopy`, because `_copy_to` op is implemented with this, and it's quite strange. Not sure if I can explain this clearly in short. I'll try.",PrComment,https://github.com/apache/mxnet/pull/15637#discussion_r306161158,ZhennanQin,2019-07-23 07:17:06,15555,15637,Data bug,0,"Only for [code], because [code] op is implemented with this, and it's quite strange. Not sure if I can explain this clearly in short. I'll try."
Comments are added.,PrComment,https://github.com/apache/mxnet/pull/15637#discussion_r306599442,ZhennanQin,2019-07-24 01:52:36,15555,15637,Data bug,0,Comments are added.
is this a new env variable?,PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310365911,pengzhao-intel,2019-08-04 00:01:05,15659,15697,Build bug,0,is this a new env variable?
will temp be deleted when out of if section?,PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310365937,pengzhao-intel,2019-08-04 00:02:51,15659,15697,Build bug,0,will temp be deleted when out of if section?
"No, it has been already defined in `dropout-inl.h`.",PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310373670,wuxun-zhang,2019-08-04 07:12:44,15659,15697,Build bug,0,"No, it has been already defined in [code]."
Seems will not be deleted. I put it here mainly because `mask` buffer can not be reused when `sizeof(DType) > sizeof(int)` and must allocate new memory space to avoid memory overlapping. ,PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310375521,wuxun-zhang,2019-08-04 08:02:53,15659,15697,Build bug,0,Seems will not be deleted. I put it here mainly because [code] buffer can not be reused when [code] and must allocate new memory space to avoid memory overlapping.
"In original implementation, this env variable will be disabled in the end of `dropout-inl.h` (see https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/dropout-inl.h#L532). Now, I removed this line so that I can use this macro in `dropout.cc`. ",PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310375825,wuxun-zhang,2019-08-04 08:11:11,15659,15697,Build bug,0,"In original implementation, this env variable will be disabled in the end of [code] (see [url]#L532). Now, I removed this line so that I can use this macro in [code]."
thanks for the explanation. ,PrComment,https://github.com/apache/mxnet/pull/15697#discussion_r310388014,pengzhao-intel,2019-08-04 13:48:12,15659,15697,Build bug,0,thanks for the explanation.
"""Provided output type"" maybe?
Also, typically `LOG(FATAL)` was used for such errors - is this a new preferred way?",PrComment,https://github.com/apache/mxnet/pull/15829#discussion_r312636862,ptrendx,2019-08-09 20:26:17,15716,15829,Algorithm design bug,1,"""Provided output type"" maybe? Also, typically [code] was used for such errors - is this a new preferred way?"
"within InferType logic InferTypeError is thrown for example: https://github.com/apache/incubator-mxnet/blob/master/src/operator/operator_common.h#L229

I was trying to keep it consistent with this. I can change the message to Provided output type",PrComment,https://github.com/apache/mxnet/pull/15829#discussion_r312638606,anirudh2290,2019-08-09 20:31:57,15716,15829,Algorithm design bug,1,within InferType logic InferTypeError is thrown for example: [url]#L229 I was trying to keep it consistent with this. I can change the message to Provided output type
I have changed the error message.,PrComment,https://github.com/apache/mxnet/pull/15829#discussion_r312639335,anirudh2290,2019-08-09 20:34:27,15716,15829,Algorithm design bug,1,I have changed the error message.
"Makes sense, thanks for explanation :-)",PrComment,https://github.com/apache/mxnet/pull/15829#discussion_r312640097,ptrendx,2019-08-09 20:37:14,15716,15829,Algorithm design bug,1,"Makes sense, thanks for explanation :-)"
"I‘m not sure when will it happen but better to change the error message to:
```cpp
CHECK_EQ(req[0], kWriteTo) << ""Transpose does not support kWriteInplace and kAddTo"";
```",PrComment,https://github.com/apache/mxnet/pull/15865#discussion_r313229532,TaoLv,2019-08-13 05:57:40,15709,15865,Data bug,1,I‘m not sure when will it happen but better to change the error message to: ``[code]``
Agreed. Will fix it. Thanks.,PrComment,https://github.com/apache/mxnet/pull/15865#discussion_r313246996,kshitij12345,2019-08-13 07:08:11,15709,15865,Data bug,1,Agreed. Will fix it. Thanks.
Should have `mom[:] +=  grad` in the end.,PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320899764,zhanghang1989,2019-09-04 18:07:34,15543,16053,Algorithm design bug,1,Should have [code] in the end.
Should have `mom[:] +=  grad` in the end.,PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320899830,zhanghang1989,2019-09-04 18:07:47,15543,16053,Algorithm design bug,1,Should have [code] in the end.
Update the momentum state in the end `mom[:] -=  grad`,PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320900353,zhanghang1989,2019-09-04 18:08:57,15543,16053,Algorithm design bug,1,Update the momentum state in the end [code]
same here,PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320900451,zhanghang1989,2019-09-04 18:09:10,15543,16053,Algorithm design bug,1,same here
`lr` should be outside of the momentum state for mxnet standard practice.,PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320989758,zhanghang1989,2019-09-04 21:45:47,15543,16053,Algorithm design bug,1,[code] should be outside of the momentum state for mxnet standard practice.
"why? but that is not nesterov logic right?

![image](https://user-images.githubusercontent.com/2778341/64295137-695db800-cf25-11e9-880e-250298b2df12.png)



",PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320995909,anirudhacharya,2019-09-04 22:05:25,15543,16053,Algorithm design bug,1,why? but that is not nesterov logic right? ![image]([url]
"Please see SGD implementation:
http://mxnet.incubator.apache.org/api/python/optimization/optimization.html?highlight=optimizer.sgd#mxnet.optimizer.SGD",PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r320996900,zhanghang1989,2019-09-04 22:09:07,15543,16053,Algorithm design bug,1,Please see SGD implementation: [url]#mxnet.optimizer.SGD
"the sgd unit test also has `lr` as part of `mom` update - https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_optimizer.py#L142. 

let me know if you still think this needs to change.",PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r321256928,anirudhacharya,2019-09-05 13:24:33,15543,16053,Algorithm design bug,1,the sgd unit test also has [code] as part of [code] update - [url]#L142. let me know if you still think this needs to change.
"Thanks! You are right. The MXNet implementation is puting lr inside. The update should be:


![image](https://user-images.githubusercontent.com/8041160/64364156-2c4a0200-cfc7-11e9-92d6-331d8b656d73.png)
![image](https://user-images.githubusercontent.com/8041160/64364165-2f44f280-cfc7-11e9-9416-b092fd294294.png)

I will check your implementation again. Thanks!",PrComment,https://github.com/apache/mxnet/pull/16053#discussion_r321386949,zhanghang1989,2019-09-05 17:23:48,15543,16053,Algorithm design bug,1,Thanks! You are right. The MXNet implementation is puting lr inside. The update should be: ![image]([url] ![image]([url] I will check your implementation again. Thanks!
why are the lib64 files being removed?,PrComment,https://github.com/apache/mxnet/pull/16690#discussion_r341679063,ChaiBapchya,2019-11-01 17:40:57,16629,16690,Build bug,0,why are the lib64 files being removed?
To make the installation more deterministic. Now the binaries will be installed to MKLDNNROOT/lib while previously they will be installed to lib or lib64 or lib/x86_64-linux-gnu (as you can see in the issue report) according to different platforms.,PrComment,https://github.com/apache/mxnet/pull/16690#discussion_r341798034,TaoLv,2019-11-02 03:51:34,16629,16690,Build bug,0,To make the installation more deterministic. Now the binaries will be installed to MKLDNNROOT/lib while previously they will be installed to lib or lib64 or lib/x86_64-linux-gnu (as you can see in the issue report) according to different platforms.
"You may use the `@use_np` decorator like in https://github.com/apache/incubator-mxnet/blob/2c02bfffb2bdb4e8f1e5159e08e3250482e5f603/tests/python/unittest/test_numpy_ndarray.py#L28,
https://github.com/apache/incubator-mxnet/blob/2c02bfffb2bdb4e8f1e5159e08e3250482e5f603/tests/python/unittest/test_numpy_ndarray.py#L36-L40",PrComment,https://github.com/apache/mxnet/pull/16796#discussion_r345503712,sxjscience,2019-11-12 23:38:55,16723,16796,Data bug,1,"You may use the [code] decorator like in [url]#L28, [url]#L36-L40"
You may use `mx.npx.waitall()` to just rely on the numpy APIs.,PrComment,https://github.com/apache/mxnet/pull/16796#discussion_r348837518,sxjscience,2019-11-21 00:08:24,16723,16796,Data bug,1,You may use [code] to just rely on the numpy APIs.
"```suggestion
        << ""NDArray.Reshape: target shape size is larger than current shape"";
```",PrComment,https://github.com/apache/mxnet/pull/16895#discussion_r350359071,ptrendx,2019-11-25 18:47:09,16647,16895,Version compatibility bug,1,``[code]``
Done.,PrComment,https://github.com/apache/mxnet/pull/16895#discussion_r350371535,reminisce,2019-11-25 19:13:45,16647,16895,Version compatibility bug,1,Done.
"Can it be replaced with nd.repeat, in order to remove Python while-loop?",PrComment,https://github.com/apache/mxnet/pull/17001#discussion_r355087376,wkcn,2019-12-07 01:10:23,16996,17001,Data bug,0,"Can it be replaced with nd.repeat, in order to remove Python while-loop?"
Good idea,PrComment,https://github.com/apache/mxnet/pull/17001#discussion_r355092816,stu1130,2019-12-07 02:20:09,16996,17001,Data bug,0,Good idea
"The four lines 712 to 715 could be replaced with the line 714 to 715.

if not first_data or not second_data:
  return first_data if first_data else second_data

When first_data and second_data are both [], it will return [] too.",PrComment,https://github.com/apache/mxnet/pull/17001#discussion_r355102103,wkcn,2019-12-07 05:41:55,16996,17001,Data bug,0,"The four lines 712 to 715 could be replaced with the line 714 to 715. if not first_data or not second_data: return first_data if first_data else second_data When first_data and second_data are both [], it will return [] too."
done,PrComment,https://github.com/apache/mxnet/pull/17001#discussion_r355103494,stu1130,2019-12-07 06:16:29,16996,17001,Data bug,0,done
would you please hint me on what's the issue for `roxygen2` on ubuntu 19.04?,PrComment,https://github.com/apache/mxnet/pull/17228#discussion_r364042267,hetong007,2020-01-08 02:36:47,17103,17228,Documentation bug,0,would you please hint me on what's the issue for [code] on ubuntu 19.04?
It's not included in the ubuntu repo prior to ubuntu 19.04. So users on older systems need to install it manually. For some reason `roxygen2` seems to be a required dependency of the R package.,PrComment,https://github.com/apache/mxnet/pull/17228#discussion_r364170981,leezu,2020-01-08 10:49:54,17103,17228,Documentation bug,0,It's not included in the ubuntu repo prior to ubuntu 19.04. So users on older systems need to install it manually. For some reason [code] seems to be a required dependency of the R package.
Is this path to cmake required?,PrComment,https://github.com/apache/mxnet/pull/17229#discussion_r363903389,apeforest,2020-01-07 19:08:39,17226,17229,Build bug,0,Is this path to cmake required?
"Yes, because we use pip install --user to install an up-to-date cmake. Often ~/.local/bin will be part of user's PATH. But it's not guaranteed and not true on default Ubuntu installation. So it's safer to specify the complete path. ",PrComment,https://github.com/apache/mxnet/pull/17229#discussion_r363908711,leezu,2020-01-07 19:21:08,17226,17229,Build bug,0,"Yes, because we use pip install --user to install an up-to-date cmake. Often ~/.local/bin will be part of user's PATH. But it's not guaranteed and not true on default Ubuntu installation. So it's safer to specify the complete path."
"Somehow I found using this absolute path causes inconvenience as my 'cmake' is not installed on this path, nor is the default one on AWS DLAMI. So I can't just copy and paste this command to build mxnet. Do you have a better idea to not using a fixed path?",PrComment,https://github.com/apache/mxnet/pull/17229#discussion_r365391576,apeforest,2020-01-10 19:21:05,17226,17229,Build bug,0,"Somehow I found using this absolute path causes inconvenience as my 'cmake' is not installed on this path, nor is the default one on AWS DLAMI. So I can't just copy and paste this command to build mxnet. Do you have a better idea to not using a fixed path?"
it would be better to just use CMake and specify what's the minimum required version,PrComment,https://github.com/apache/mxnet/pull/17229#discussion_r365478112,larroy,2020-01-11 00:14:06,17226,17229,Build bug,0,it would be better to just use CMake and specify what's the minimum required version
"@larroy some users may not know how to understand cmake.
@apeforest DLAMI encourages users to use conda, which causes problems for above installation steps.

I'll open a PR to improve the experience shortly",PrComment,https://github.com/apache/mxnet/pull/17229#discussion_r365510911,leezu,2020-01-11 09:32:18,17226,17229,Build bug,0,"@larroy some users may not know how to understand cmake. @apeforest DLAMI encourages users to use conda, which causes problems for above installation steps. I'll open a PR to improve the experience shortly"
Remove?,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366134308,apeforest,2020-01-14 03:07:53,17239,17297,Build bug,0,Remove?
Indent,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366134334,apeforest,2020-01-14 03:08:01,17239,17297,Build bug,0,Indent
Remove ,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366134380,apeforest,2020-01-14 03:08:21,17239,17297,Build bug,0,Remove
Should this be HINTS instead of PATHS?,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366166613,leezu,2020-01-14 06:15:07,17239,17297,Build bug,0,Should this be HINTS instead of PATHS?
"https://cmake.org/cmake/help/v3.4/command/find_path.html
Looks like either Hints or paths is permitted. Any specific reason to use hints?",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366185864,ChaiBapchya,2020-01-14 07:34:22,17239,17297,Build bug,0,[url] Looks like either Hints or paths is permitted. Any specific reason to use hints?
When writing the comment I thought PATHS may skip default locations whereas HINTS would be defaults + specified locations. Thanks for pointing out they are equivalent.,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r366391503,leezu,2020-01-14 15:07:43,17239,17297,Build bug,0,When writing the comment I thought PATHS may skip default locations whereas HINTS would be defaults + specified locations. Thanks for pointing out they are equivalent.
"""first check"" may break the assumption that `NCCL_ROOT_DIR` can be used to specify nccl root directory?

Would adding `/usr/local/cuda` as last item in `find_path`, `find_library` below be sufficient? Or does it throw an error if `/usr/local/cuda` doesn't exist?",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367149908,leezu,2020-01-15 22:52:53,17239,17297,Build bug,0,"""first check"" may break the assumption that [code] can be used to specify nccl root directory? Would adding [code] as last item in [code], [code] below be sufficient? Or does it throw an error if [code] doesn't exist?"
"I'm not sure why it breaks the assumption. Can you help me rephrase the comment? I can remove ""first"" if that's what confuses the users.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367201643,ChaiBapchya,2020-01-16 02:17:21,17239,17297,Build bug,0,"I'm not sure why it breaks the assumption. Can you help me rephrase the comment? I can remove ""first"" if that's what confuses the users."
"Quoting from https://cmake.org/cmake/help/latest/command/find_path.html

> This command is used to find a directory containing the named file. A cache entry named by `<VAR>` is created to store the result of this command. If the file in a directory is found the result is stored in the variable and the search will not be repeated unless the variable is cleared. If nothing is found, the result will be `<VAR>-NOTFOUND`, and the search will be attempted again the next time find_path is invoked with the same variable.

Thus if there is some `nccl.h` at `/usr/local/cuda` but the user specifies `NCCL_ROOT_DIR=/home/ubuntu/myversionofnccl`, the `/usr/local/cuda` is wrongly used? Or do I misunderstand the doc? 

Btw, as per https://cmake.org/cmake/help/latest/policy/CMP0074.html `NCCL_ROOT_DIR` should probably be called `NCCL_ROOT` (or `NCCL_ROOT` should be added in addition to `NCCL_ROOT_DIR`).",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367382186,leezu,2020-01-16 12:05:51,17239,17297,Build bug,0,"Quoting from [url] > This command is used to find a directory containing the named file. A cache entry named by [code] is created to store the result of this command. If the file in a directory is found the result is stored in the variable and the search will not be repeated unless the variable is cleared. If nothing is found, the result will be [code], and the search will be attempted again the next time find_path is invoked with the same variable. Thus if there is some [code] at [code] but the user specifies [code], the [code] is wrongly used? Or do I misunderstand the doc? Btw, as per [url] [code] should probably be called [code] (or [code] should be added in addition to [code])."
"Not familiar with cmake, but do you need add indentation here?",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367568453,apeforest,2020-01-16 18:03:47,17239,17297,Build bug,0,"Not familiar with cmake, but do you need add indentation here?"
"Not familiar with cmake, but do you need add indentation here?",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367568504,apeforest,2020-01-16 18:03:51,17239,17297,Build bug,0,"Not familiar with cmake, but do you need add indentation here?"
"> Btw, as per https://cmake.org/cmake/help/latest/policy/CMP0074.html `NCCL_ROOT_DIR` should probably be called `NCCL_ROOT` (or `NCCL_ROOT` should be added in addition to `NCCL_ROOT_DIR`).


Ya. Even I saw this NCCL_ROOT and found that NCCL_ROOT_DIR is deprecated hence no longer to be used in PyTorch repo
https://github.com/pytorch/pytorch/blob/f94aab45fd7b1e7e32d8ea4324da6d3632a0d412/cmake/Modules/FindNCCL.cmake#L21

However, I couldn't find official cmake doc saying so. Nonetheless, I think I should add that too.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367575646,ChaiBapchya,2020-01-16 18:20:09,17239,17297,Build bug,0,"> Btw, as per [url] [code] should probably be called [code] (or [code] should be added in addition to [code]). Ya. Even I saw this NCCL_ROOT and found that NCCL_ROOT_DIR is deprecated hence no longer to be used in PyTorch repo [url]#L21 However, I couldn't find official cmake doc saying so. Nonetheless, I think I should add that too."
"> If there is some nccl.h at /usr/local/cuda but the user specifies NCCL_ROOT_DIR=/home/ubuntu/myversionofnccl, the /usr/local/cuda is wrongly used? Or do I misunderstand the doc?

I agree, since ""search"" will not be repeated until var is set, it makes sense to assume whatever location is found first is final. So in that case first check has to be env var and second for default.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367577696,ChaiBapchya,2020-01-16 18:25:01,17239,17297,Build bug,0,"> If there is some nccl.h at /usr/local/cuda but the user specifies NCCL_ROOT_DIR=/home/ubuntu/myversionofnccl, the /usr/local/cuda is wrongly used? Or do I misunderstand the doc? I agree, since ""search"" will not be repeated until var is set, it makes sense to assume whatever location is found first is final. So in that case first check has to be env var and second for default."
"Surprisingly, I couldn't find a check for `/usr/local/cuda` in case of pytorch's FindNCCL
I have created a discuss question on their forum anyway - https://discuss.pytorch.org/t/findnccl-cmake-search-for-usr-local-cuda/66891",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367578134,ChaiBapchya,2020-01-16 18:25:57,17239,17297,Build bug,0,"Surprisingly, I couldn't find a check for [code] in case of pytorch's FindNCCL I have created a discuss question on their forum anyway - [url]"
"> I saw this NCCL_ROOT and found that NCCL_ROOT_DIR is deprecated hence no longer to be used in PyTorch repo
> https://github.com/pytorch/pytorch/blob/f94aab45fd7b1e7e32d8ea4324da6d3632a0d412/cmake/Modules/FindNCCL.cmake#L21
> 
> However, I couldn't find official cmake doc saying so. Nonetheless, I think I should add that too.

Thanks. The official doc should be https://cmake.org/cmake/help/latest/policy/CMP0074.html ",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367579230,leezu,2020-01-16 18:28:21,17239,17297,Build bug,0,"> I saw this NCCL_ROOT and found that NCCL_ROOT_DIR is deprecated hence no longer to be used in PyTorch repo > [url]#L21 > > However, I couldn't find official cmake doc saying so. Nonetheless, I think I should add that too. Thanks. The official doc should be [url]"
yes good catch! indentation needed.,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367682940,ChaiBapchya,2020-01-16 22:27:53,17239,17297,Build bug,0,yes good catch! indentation needed.
"Moved /usr/local/cuda check to be after other checks
Also added NCCL_ROOT alongside NCCL_ROOT_DIR
Also added deprecation warning for ROOT_DIR ",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367701531,ChaiBapchya,2020-01-16 23:26:11,17239,17297,Build bug,0,Moved /usr/local/cuda check to be after other checks Also added NCCL_ROOT alongside NCCL_ROOT_DIR Also added deprecation warning for ROOT_DIR
"@ChaiBapchya Thank you!

After reading CMP0074 again,
> [...] now searches prefixes specified by the `<PackageName>_ROOT` CMake variable and the `<PackageName>_ROOT` environment variable. Package roots are maintained as a stack so nested calls to all find_* commands inside find modules and config packages also search the roots as prefixes.

seems to imply that it's not necessary to list `NCCL_ROOT` as part of `find_path` and `find_library`, but rather it's done automatically. Do you understand it in the same way?
Also https://cmake.org/cmake/help/latest/command/find_package.html#search-procedure (paragraph ""The set of installation prefixes is constructed using the following steps [...]"") suggests this.

If so,  `set(NCCL_ROOT $ENV{NCCL_ROOT} CACHE PATH ""Folder contains NVIDIA NCCL"")` would be superfluous. Also it seems none of the other Find modules check the `X_LIB_DIR` and `X_LIB_DIR` env variables.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367715337,leezu,2020-01-17 00:19:46,17239,17297,Build bug,0,"@ChaiBapchya Thank you! After reading CMP0074 again, > [...] now searches prefixes specified by the [code] CMake variable and the [code] environment variable. Package roots are maintained as a stack so nested calls to all find_* commands inside find modules and config packages also search the roots as prefixes. seems to imply that it's not necessary to list [code] as part of [code] and [code], but rather it's done automatically. Do you understand it in the same way? Also [url]#search-procedure (paragraph ""The set of installation prefixes is constructed using the following steps [...]"") suggests this. If so, [code] would be superfluous. Also it seems none of the other Find modules check the [code] and [code] env variables."
"1. NCCL_ROOT auto check - I guess it is done automatically. Though it's not mentioned explicitly anywhere. Hence asked here - https://stackoverflow.com/questions/59783425/does-cmake-find-path-find-library-search-for-packagename-root-by-default

2. > Also it seems none of the other Find modules check the X_LIB_DIR and X_LIB_DIR env variables.

Not sure what you implying? You think that is superfluous too? ",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r367813676,ChaiBapchya,2020-01-17 08:18:02,17239,17297,Build bug,0,1. NCCL_ROOT auto check - I guess it is done automatically. Though it's not mentioned explicitly anywhere. Hence asked here - [url] 2. > Also it seems none of the other Find modules check the X_LIB_DIR and X_LIB_DIR env variables. Not sure what you implying? You think that is superfluous too?
"Sorry for being picky, but isn't the indent supposed to be 2 space for all?",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r368079950,apeforest,2020-01-17 18:41:04,17239,17297,Build bug,0,"Sorry for being picky, but isn't the indent supposed to be 2 space for all?"
What happens if not UNIX? is there any error/warning message issued?,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r368080668,apeforest,2020-01-17 18:42:49,17239,17297,Build bug,0,What happens if not UNIX? is there any error/warning message issued?
"1. Autocheck will be done provided `NO_DEFAULT_PATH` or `NO_PACKAGE_ROOT_PATH` are not set. So I'm assuming that's the case.
Refer : https://stackoverflow.com/questions/59783425/does-cmake-find-path-find-library-search-for-packagename-root-by-default/59788313#59788313
",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r368081829,ChaiBapchya,2020-01-17 18:45:52,17239,17297,Build bug,0,1. Autocheck will be done provided [code] or [code] are not set. So I'm assuming that's the case. Refer : [url]#59788313
A variable will be set to not found. The calling cmake module needs to check that variable and error out if nccl is required. ,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r368082784,leezu,2020-01-17 18:48:15,17239,17297,Build bug,0,A variable will be set to not found. The calling cmake module needs to check that variable and error out if nccl is required.
"1. Yes, you should remove the manual checks for `NCCL_ROOT`.

2. I don't think it's superfluous (ie it does something), but it seems not necessary because it seems non-standard (ie other Find modules don't check the environment variables, so it's not expected that you check them here)",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r368281540,leezu,2020-01-19 10:05:59,17239,17297,Build bug,0,"1. Yes, you should remove the manual checks for [code]. 2. I don't think it's superfluous (ie it does something), but it seems not necessary because it seems non-standard (ie other Find modules don't check the environment variables, so it's not expected that you check them here)"
"> You can also set(ENV{variable_name} value) and get $ENV{variable_name} environment variables, though it is generally a very good idea to avoid them.

https://cliutils.gitlab.io/modern-cmake/chapters/basics/variables.html

Should the env variable checks here be removed? You could just delete these 3 lines.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369298016,leezu,2020-01-21 23:24:38,17239,17297,Build bug,0,"> You can also set(ENV{variable_name} value) and get $ENV{variable_name} environment variables, though it is generally a very good idea to avoid them. [url] Should the env variable checks here be removed? You could just delete these 3 lines."
"So I am guessing I should remove it here too?
https://github.com/apache/incubator-mxnet/blob/a9abba5a934755813481e6583435298a0c317957/cmake/Modules/FindNVTX.cmake#L18

",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369303550,ChaiBapchya,2020-01-21 23:43:33,17239,17297,Build bug,0,So I am guessing I should remove it here too? [url]#L18
"or is it specifically to do with ENV vars. In that case we can set """" to these 3 variables - NCCL_ROOT, NCCL_INCLUDE_DIR and NCCL_LIB_DIR
Since that seems to be the norm",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369304098,ChaiBapchya,2020-01-21 23:45:33,17239,17297,Build bug,0,"or is it specifically to do with ENV vars. In that case we can set """" to these 3 variables - NCCL_ROOT, NCCL_INCLUDE_DIR and NCCL_LIB_DIR Since that seems to be the norm"
@ChaiBapchya let's focus on one thing in a PR. I would change the NVTX_ROOT_DIR in separate PR if needed.,PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369304872,apeforest,2020-01-21 23:48:07,17239,17297,Build bug,0,@ChaiBapchya let's focus on one thing in a PR. I would change the NVTX_ROOT_DIR in separate PR if needed.
"@ChaiBapchya Yes, for this PR my concern is only that you are introducing environment variable depencies, which is not standard practice and not needed.
So for this PR, let's just remove the 3 lines.
For a later PR, NVTX can be refactored.",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369329996,leezu,2020-01-22 01:25:46,17239,17297,Build bug,0,"@ChaiBapchya Yes, for this PR my concern is only that you are introducing environment variable depencies, which is not standard practice and not needed. So for this PR, let's just remove the 3 lines. For a later PR, NVTX can be refactored."
"@leezu Setting empty string to a cache variable does serve a purpose (declaring a variable so as to check if it is set or not in future)
So if NCCL_ROOT_DIR is not already set then it assigns a default value to it (right now since we have deleted this line, if variable is not set via command line, it won't exist -> hence can't use it in this file)

Conclusion - we should have that line of empty string as default value for a cache variable.

What do you think?

https://stackoverflow.com/questions/59867283/whats-the-point-of-using-empty-string-to-set-cache-variable-in-cmake/59868538#59868538",PrComment,https://github.com/apache/mxnet/pull/17297#discussion_r369890499,ChaiBapchya,2020-01-23 01:16:36,17239,17297,Build bug,0,"@leezu Setting empty string to a cache variable does serve a purpose (declaring a variable so as to check if it is set or not in future) So if NCCL_ROOT_DIR is not already set then it assigns a default value to it (right now since we have deleted this line, if variable is not set via command line, it won't exist -> hence can't use it in this file) Conclusion - we should have that line of empty string as default value for a cache variable. What do you think? [url]#59868538"
"Isn't this covered by the first line? If not, should we expand the pattern for the first line instead?",PrComment,https://github.com/apache/mxnet/pull/17348#discussion_r367790272,szha,2020-01-17 06:49:04,17292,17348,Build bug,0,"Isn't this covered by the first line? If not, should we expand the pattern for the first line instead?"
I believe the first line misses the prefix before MX. Not sure if we want to whitelist all ```*MX*``` APIs.,PrComment,https://github.com/apache/mxnet/pull/17348#discussion_r368088331,apeforest,2020-01-17 19:01:21,17292,17348,Build bug,0,I believe the first line misses the prefix before MX. Not sure if we want to whitelist all ``[code]`` APIs.
Let's fix the first line then,PrComment,https://github.com/apache/mxnet/pull/17348#discussion_r368258561,szha,2020-01-19 01:28:33,17292,17348,Build bug,0,Let's fix the first line then
Updated. Please review again. Thanks!,PrComment,https://github.com/apache/mxnet/pull/17348#discussion_r368266596,apeforest,2020-01-19 05:17:29,17292,17348,Build bug,0,Updated. Please review again. Thanks!
"nitpick
```suggestion
    """"""Download a given URL
```",PrComment,https://github.com/apache/mxnet/pull/17372#discussion_r368258140,ChaiBapchya,2020-01-19 01:15:48,17332,17372,Deployment bug,0,nitpick ``[code]``
"From the doc:

>  Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows NT or later). If delete is true (the default), the file is deleted as soon as it is closed.

Here you are taking the file name of the opened file, passing it to download and therein opening it again. One workaround is to work with the open file object itself when using `tempfile.NamedTemporaryFile`.

Reference https://docs.python.org/2/library/tempfile.html",PrComment,https://github.com/apache/mxnet/pull/17372#discussion_r370251465,leezu,2020-01-23 17:21:26,17332,17372,Deployment bug,0,"From the doc: > Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows NT or later). If delete is true (the default), the file is deleted as soon as it is closed. Here you are taking the file name of the opened file, passing it to download and therein opening it again. One workaround is to work with the open file object itself when using [code]. Reference [url]"
Thanks for the comment! That makes me think NamedTemporaryFile and TemporaryFile are quite restricted. I have to modify other functions that uses this file so that they operate on the open file object itself. For now I'll fallback to the previous method where the file is explicitly cleaned up. ,PrComment,https://github.com/apache/mxnet/pull/17372#discussion_r370283042,eric-haibin-lin,2020-01-23 18:28:25,17332,17372,Deployment bug,0,Thanks for the comment! That makes me think NamedTemporaryFile and TemporaryFile are quite restricted. I have to modify other functions that uses this file so that they operate on the open file object itself. For now I'll fallback to the previous method where the file is explicitly cleaned up.
Yes. You can set `delete=False` and this restriction should go away.,PrComment,https://github.com/apache/mxnet/pull/17372#discussion_r370284078,leezu,2020-01-23 18:30:38,17332,17372,Deployment bug,0,Yes. You can set [code] and this restriction should go away.
"I think @apeforest suggested using `TemporaryFile` because it can be automatically cleaned up. If we need to cleanup ourselves, then there's not much difference using it or not",PrComment,https://github.com/apache/mxnet/pull/17372#discussion_r370324247,eric-haibin-lin,2020-01-23 19:56:16,17332,17372,Deployment bug,0,"I think @apeforest suggested using [code] because it can be automatically cleaned up. If we need to cleanup ourselves, then there's not much difference using it or not"
you should also change the comment line here.,PrComment,https://github.com/apache/mxnet/pull/17386#discussion_r368431866,TaoLv,2020-01-20 09:04:22,17352,17386,Test bug,0,you should also change the comment line here.
I don't remember the reason why we need to write this way in the first place. @ChaiBapchya could you please review. @connorgoggins Have you run through existing tests to make sure this does not break any existing usage?,PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382340355,apeforest,2020-02-21 00:46:22,17640,17642,Code bug,0,I don't remember the reason why we need to write this way in the first place. @ChaiBapchya could you please review. @connorgoggins Have you run through existing tests to make sure this does not break any existing usage?
"@apeforest yes, ran full OpPerf suite with Python profiler with this change and everything passed (see results [here](https://gist.github.com/connorgoggins/1202852d45e106f9e4d1959e1ffd1985)).",PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382346393,connorgoggins,2020-02-21 01:05:59,17640,17642,Code bug,0,"@apeforest yes, ran full OpPerf suite with Python profiler with this change and everything passed (see results [here]([url]"
"So the args that are passed to this function are
args[0] = op
args[1] = warmup / runs (number of times to run for warmup or number of times to run)
args[2] - rest of the args

https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/benchmark/opperf/utils/benchmark_utils.py#L114

https://github.com/apache/incubator-mxnet/blob/9dcf71d8fe33f77ed316a95fcffaf1f7f883ff70/benchmark/opperf/utils/benchmark_utils.py#L121

The way it worked for native MXNet CPP profiler is that you could pass the runs (and it would capture the time for each value along with mean/max, etc)

But for Python's time it function, we had to manually run the `for loop` for the number of runs.
So that's what I did there

1. we copy the number of runs in a variable in run and then run it that many number of times 
2. For each run, we use python time it function to time it and then take average, mean, max, etc values for each of those individual python time runs.

Makes sense? @apeforest ",PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382350528,ChaiBapchya,2020-02-21 01:23:03,17640,17642,Code bug,0,"So the args that are passed to this function are args[0] = op args[1] = warmup / runs (number of times to run for warmup or number of times to run) args[2] - rest of the args [url]#L114 [url]#L121 The way it worked for native MXNet CPP profiler is that you could pass the runs (and it would capture the time for each value along with mean/max, etc) But for Python's time it function, we had to manually run the [code] for the number of runs. So that's what I did there 1. we copy the number of runs in a variable in run and then run it that many number of times 2. For each run, we use python time it function to time it and then take average, mean, max, etc values for each of those individual python time runs. Makes sense? @apeforest"
"@ChaiBapchya So basically you are saying we don't need to do this modified_args for python profiler, right? So @connorgoggins change is valid?",PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382418032,apeforest,2020-02-21 06:35:57,17640,17642,Code bug,0,"@ChaiBapchya So basically you are saying we don't need to do this modified_args for python profiler, right? So @connorgoggins change is valid?"
We **do** need to modify the args to meet the requirement for capturing per run timing info using python's time_it function. This needs to handled in a way that doesn't break existing native profiler.,PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382762557,ChaiBapchya,2020-02-21 19:24:40,17640,17642,Code bug,0,We **do** need to modify the args to meet the requirement for capturing per run timing info using python's time_it function. This needs to handled in a way that doesn't break existing native profiler.
"@connorgoggins @apeforest 
If we pass the *args as is, it will still have
args[0] as op
args[1] as runs
For eg if user passed runs as 10

So the native profiler would run 10 times and so will the for loop run 10 times (for timing Python profiler)

Coz the func here is nd_forward_backward_profile or nd_forward_profile (both take runs as a parameter)",PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382762705,ChaiBapchya,2020-02-21 19:24:59,17640,17642,Code bug,0,"@connorgoggins @apeforest If we pass the *args as is, it will still have args[0] as op args[1] as runs For eg if user passed runs as 10 So the native profiler would run 10 times and so will the for loop run 10 times (for timing Python profiler) Coz the func here is nd_forward_backward_profile or nd_forward_profile (both take runs as a parameter)"
Now it looks good!,PrComment,https://github.com/apache/mxnet/pull/17642#discussion_r382768475,ChaiBapchya,2020-02-21 19:37:17,17640,17642,Code bug,0,Now it looks good!
"There's reference implementation in mkldnn that uses a different threshold and also does type cast. https://github.com/intel/mkl-dnn/commit/23f45a2e4cb4158e6111d3e59d91b66038636edb#diff-b476290dff3dd01b30438be4ae713928R174

Shall we adopt the same logic for consistency?",PrComment,https://github.com/apache/mxnet/pull/17849#discussion_r393266735,szha,2020-03-16 19:36:34,17844,17849,Processor bug,1,There's reference implementation in mkldnn that uses a different threshold and also does type cast. [url]#diff-b476290dff3dd01b30438be4ae713928R174 Shall we adopt the same logic for consistency?
"From the test I did - difference between `logf(1 + expf(20)) - 20` is 0 already, so I think there should not be any difference (and if anything, going to max float should give less precision).",PrComment,https://github.com/apache/mxnet/pull/17849#discussion_r393270412,ptrendx,2020-03-16 19:44:27,17844,17849,Processor bug,1,"From the test I did - difference between [code] is 0 already, so I think there should not be any difference (and if anything, going to max float should give less precision)."
"I took the reference threshold from MxNets own [mshadow impl](https://github.com/apache/incubator-mxnet/blob/ea2dabaae5e1453572c9105f77322977bd14c108/src/operator/mshadow_op.h#L420).

`log(1 + exp(v))` is litteraly the same floating point value as `v` for float32 and `15.004232 <= v < 88.72284`. For `v < 15.004232`, there is a non-zero difference `log(1 + exp(v)) - v`, and for `v >= 88.72284`, `exp(v)` is `+inf`.

Given that choosing any value in that range should yield exactly the same results, I would intuitively pick a lower value since it skips the `log(1+exp(v))` computation in favour of identity `v` earlier.

Although, I guess, `88.72284` is slightly less ""magic number""  than `20`, since it's `log(FLT_MAX)`.

I am not particularly committed to either version, so the final decision is up to you",PrComment,https://github.com/apache/mxnet/pull/17849#discussion_r393288295,RuRo,2020-03-16 20:21:25,17844,17849,Processor bug,1,"I took the reference threshold from MxNets own [mshadow impl]([url]#L420). [code] is litteraly the same floating point value as [code] for float32 and [code]. For [code], there is a non-zero difference [code], and for [code], [code] is [code]. Given that choosing any value in that range should yield exactly the same results, I would intuitively pick a lower value since it skips the [code] computation in favour of identity [code] earlier. Although, I guess, [code] is slightly less ""magic number"" than [code], since it's [code]. I am not particularly committed to either version, so the final decision is up to you"
OK,PrComment,https://github.com/apache/mxnet/pull/17849#discussion_r393296479,szha,2020-03-16 20:39:26,17844,17849,Processor bug,1,OK
"This has to be true. Otherwise, a single change will result in a full rebuild, drastically increasing build times.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408411038,marcoabreu,2020-04-14 20:21:31,18061,18056,Test bug,1,"This has to be true. Otherwise, a single change will result in a full rebuild, drastically increasing build times."
"I don't mind changing it to `True`, but the existing behaviour before introducing the flag was `False`.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408419117,leezu,2020-04-14 20:36:11,18061,18056,Test bug,1,"I don't mind changing it to [code], but the existing behaviour before introducing the flag was [code]."
"One issue I noted during local development when setting this to `True`, is that my disk can fill up very quickly because Docker will not clean any of the cached layers automatically.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408419434,leezu,2020-04-14 20:36:49,18061,18056,Test bug,1,"One issue I noted during local development when setting this to [code], is that my disk can fill up very quickly because Docker will not clean any of the cached layers automatically."
"Well there has been some back and forth between refactors - maybe somebody removed it :/. Initially, I designed it that way that intermediary layers were kept since that's the whole point of the partial cache. This test validates that assumption: https://github.com/apache/incubator-mxnet/blob/master/ci/test_docker_cache.py#L184

Note that this test suite is not run in CI...",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408426593,marcoabreu,2020-04-14 20:49:51,18061,18056,Test bug,1,"Well there has been some back and forth between refactors - maybe somebody removed it :/. Initially, I designed it that way that intermediary layers were kept since that's the whole point of the partial cache. This test validates that assumption: [url]#L184 Note that this test suite is not run in CI..."
Well you can't have everything - either partial caching and manual cleanup or automatic cleanup but no partial caching. I think it is fine to expect that people use docker purge every now and then.,PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408427070,marcoabreu,2020-04-14 20:50:46,18061,18056,Test bug,1,Well you can't have everything - either partial caching and manual cleanup or automatic cleanup but no partial caching. I think it is fine to expect that people use docker purge every now and then.
"Does the CI docker purge automatically? My docker cache quickly grow by 200GB after developing locally with the intermediate caching enabled. It's fine to purge time to time manually, but it's better to automate this.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408450992,leezu,2020-04-14 21:36:54,18061,18056,Test bug,1,"Does the CI docker purge automatically? My docker cache quickly grow by 200GB after developing locally with the intermediate caching enabled. It's fine to purge time to time manually, but it's better to automate this."
"> Note that this test suite is not run in CI...

Is it possible to run this suite in CI? If so, would you like to help enable it?",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408453141,leezu,2020-04-14 21:41:27,18061,18056,Test bug,1,"> Note that this test suite is not run in CI... Is it possible to run this suite in CI? If so, would you like to help enable it?"
"For reference, you removed the `rm=false` yourself in f107397b753ff08bb19e7572bc1a9ebedd832f88
",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408487032,leezu,2020-04-14 23:07:39,18061,18056,Test bug,1,"For reference, you removed the [code] yourself in f107397b753ff08bb19e7572bc1a9ebedd832f88"
"CI does not docker purge automatically. But since the lifecycle of unix slaves is very very short, it's not an issue. Also, in 99% of the cases, no new layers are built and only the cache is used, so the likelyhood of a single slave regenerating a lot of new mismatching layers is quite low. On a dev machine though, that's a different case.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408509009,marcoabreu,2020-04-15 00:17:50,18061,18056,Test bug,1,"CI does not docker purge automatically. But since the lifecycle of unix slaves is very very short, it's not an issue. Also, in 99% of the cases, no new layers are built and only the cache is used, so the likelyhood of a single slave regenerating a lot of new mismatching layers is quite low. On a dev machine though, that's a different case."
"Yeah it should be possible. It will just cause issues if the test fails since that might mean that stuff gets left dangling, but due to the mentioned lifecycle, that shouldn't have a big impact.

Feel free to enable it.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408509392,marcoabreu,2020-04-15 00:18:56,18061,18056,Test bug,1,"Yeah it should be possible. It will just cause issues if the test fails since that might mean that stuff gets left dangling, but due to the mentioned lifecycle, that shouldn't have a big impact. Feel free to enable it."
"Good catch! I can't fully recall what was the thing around --rm=false, but I recall that I wrote tests to cover all these unknowns. Rule of thumb is that if the tests pass, we're good to go (given they use the same parameters of course).",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408509652,marcoabreu,2020-04-15 00:19:46,18061,18056,Test bug,1,"Good catch! I can't fully recall what was the thing around --rm=false, but I recall that I wrote tests to cover all these unknowns. Rule of thumb is that if the tests pass, we're good to go (given they use the same parameters of course)."
"> CI does not docker purge automatically. But since the lifecycle of unix slaves is very very short, it's not an issue

If the intermediate containers are only kept on the slave, and we only generate them in PRs that change the Dockerfile, I think we don't need to use them at all on CI. `--rm=false` then seems only useful for development.",PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408513268,leezu,2020-04-15 00:32:08,18061,18056,Test bug,1,"> CI does not docker purge automatically. But since the lifecycle of unix slaves is very very short, it's not an issue If the intermediate containers are only kept on the slave, and we only generate them in PRs that change the Dockerfile, I think we don't need to use them at all on CI. [code] then seems only useful for development."
Yep exactly. But please make sure that the tests pass before merging - otherwise we might break the partial caching.,PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408515877,marcoabreu,2020-04-15 00:41:55,18061,18056,Test bug,1,Yep exactly. But please make sure that the tests pass before merging - otherwise we might break the partial caching.
This PR does not change the status quo. There is no risk.,PrComment,https://github.com/apache/mxnet/pull/18056#discussion_r408536446,leezu,2020-04-15 01:58:43,18061,18056,Test bug,1,This PR does not change the status quo. There is no risk.
https://docs.python.org/3/library/os.path.html#os.path.dirname ?,PrComment,https://github.com/apache/mxnet/pull/18323#discussion_r427732782,leezu,2020-05-20 04:20:48,18322,18323,Test bug,0,[url]#os.path.dirname ?
"We should use
```
LOG(FATAL) << ""NotImplementedError: xxx setting for yyy op is not implemented yet."";
```",PrComment,https://github.com/apache/mxnet/pull/18393#discussion_r429599165,szha,2020-05-24 04:57:03,18353,18393,Data bug,0,We should use ``[code]``
you can delete this,PrComment,https://github.com/apache/mxnet/pull/18515#discussion_r437033495,leezu,2020-06-08 22:21:36,18491,18515,Version compatibility bug,0,you can delete this
"Done.

Sorry for the mess, my first PR....",PrComment,https://github.com/apache/mxnet/pull/18515#discussion_r437036705,andevellicus,2020-06-08 22:30:15,18491,18515,Version compatibility bug,0,"Done. Sorry for the mess, my first PR...."
"```suggestion
```",PrComment,https://github.com/apache/mxnet/pull/18515#discussion_r437984016,iblislin,2020-06-10 09:21:27,18491,18515,Version compatibility bug,0,``[code]``
"```suggestion
  shape = collect(m.match for m ∈ eachmatch(r""\d+"", str))
```",PrComment,https://github.com/apache/mxnet/pull/18515#discussion_r437984517,iblislin,2020-06-10 09:22:18,18491,18515,Version compatibility bug,0,``[code]``
why the fixed dtype?,PrComment,https://github.com/apache/mxnet/pull/18689#discussion_r453087239,szha,2020-07-10 21:34:12,18680,18689,Documentation bug,0,why the fixed dtype?
That's a question for @xidulu ,PrComment,https://github.com/apache/mxnet/pull/18689#discussion_r453097047,leezu,2020-07-10 22:02:17,18680,18689,Documentation bug,0,That's a question for @xidulu
"Since _value_  is a a python scalar, the default output’s dtype would be float64, which is inconsistent DeepNumpy’s behavior.",PrComment,https://github.com/apache/mxnet/pull/18689#discussion_r453139086,xidulu,2020-07-11 01:35:24,18680,18689,Documentation bug,0,"Since _value_ is a a python scalar, the default output’s dtype would be float64, which is inconsistent DeepNumpy’s behavior."
"@xidulu I see. That makes sense. We do have a flag for determining whether the output type should follow numpy convention or not. You can find it here:
https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/util.py#L1076-L1092

It would be great if that convention is observed.",PrComment,https://github.com/apache/mxnet/pull/18689#discussion_r453140667,szha,2020-07-11 01:51:22,18680,18689,Documentation bug,0,@xidulu I see. That makes sense. We do have a flag for determining whether the output type should follow numpy convention or not. You can find it here: [url]#L1076-L1092 It would be great if that convention is observed.
"@szha  Ur right, this decorator should be taking into account. 
I will try to figure out a way to let these utilities be aware of the type flag.",PrComment,https://github.com/apache/mxnet/pull/18689#discussion_r453144899,xidulu,2020-07-11 02:39:19,18680,18689,Documentation bug,0,"@szha Ur right, this decorator should be taking into account. I will try to figure out a way to let these utilities be aware of the type flag."
we also no longer offer these and instead only support 3.6+,PrComment,https://github.com/apache/mxnet/pull/18850#discussion_r464608285,szha,2020-08-03 19:08:49,18849,18850,Build bug,0,we also no longer offer these and instead only support 3.6+
how is this related to the cached graph and cached op fields?,PrComment,https://github.com/apache/mxnet/pull/18853#discussion_r464607166,szha,2020-08-03 19:06:26,18843,18853,Deployment bug,0,how is this related to the cached graph and cached op fields?
The _var is used as input,PrComment,https://github.com/apache/mxnet/pull/18853#discussion_r464655416,leezu,2020-08-03 20:51:17,18843,18853,Deployment bug,0,The _var is used as input
fyi @zhreshold I added these ops to npx,PrComment,https://github.com/apache/mxnet/pull/18972#discussion_r474200033,szha,2020-08-20 18:50:47,18927,18972,Data bug,1,fyi @zhreshold I added these ops to npx
You will need to also implement kAddTo.,PrComment,https://github.com/apache/mxnet/pull/19044#discussion_r479817156,sxjscience,2020-08-30 21:22:25,19043,19044,Algorithm design bug,1,You will need to also implement kAddTo.
Need to investigate if it's possible to remove the reinterpret_cast.,PrComment,https://github.com/apache/mxnet/pull/19044#discussion_r479891282,sxjscience,2020-08-31 05:12:39,19043,19044,Algorithm design bug,1,Need to investigate if it's possible to remove the reinterpret_cast.
Need to raise an error when mode is not constant.,PrComment,https://github.com/apache/mxnet/pull/19044#discussion_r479891385,sxjscience,2020-08-31 05:13:09,19043,19044,Algorithm design bug,1,Need to raise an error when mode is not constant.
"> Need to investigate if it's possible to remove the reinterpret_cast.

Thanks. I am trying to avoid using 'reinterpret_cast'.
",PrComment,https://github.com/apache/mxnet/pull/19044#discussion_r479992647,ryanwentaoxu,2020-08-31 08:59:12,19043,19044,Algorithm design bug,1,> Need to investigate if it's possible to remove the reinterpret_cast. Thanks. I am trying to avoid using 'reinterpret_cast'.
also try to add test case for `add`.,PrComment,https://github.com/apache/mxnet/pull/19044#discussion_r479995832,sxjscience,2020-08-31 09:05:07,19043,19044,Algorithm design bug,1,also try to add test case for [code].
looks like `--show-current` breaks the `prepare` process,PrComment,https://github.com/apache/mxnet/pull/19190#discussion_r491651187,ys2843,2020-09-20 04:33:45,18913,19190,Visualization bug,1,looks like [code] breaks the [code] process
nit: remove comment,PrComment,https://github.com/apache/mxnet/pull/19378#discussion_r508850860,mseth10,2020-10-20 21:28:18,19360,19378,Test bug,0,nit: remove comment
I'm not sure if the indentation here is correct.,PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r520184777,sxjscience,2020-11-09 23:23:57,19463,19488,Data bug,1,I'm not sure if the indentation here is correct.
What are your doubts? `Else` clause on line 95 aligns with `for` on line 91 - it gets executed if the loop completes without a break.,PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r520194107,mk-61,2020-11-09 23:49:00,19463,19488,Data bug,1,What are your doubts? [code] clause on line 95 aligns with [code] on line 91 - it gets executed if the loop completes without a break.
Thanks for clarifying.,PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r520221070,sxjscience,2020-11-10 01:08:33,19463,19488,Data bug,1,Thanks for clarifying.
Can we try to narrow the scope to `function`?,PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r526578507,sxjscience,2020-11-19 03:56:16,19463,19488,Data bug,1,Can we try to narrow the scope to [code]?
"It doesn't matter. The only reason I put scope='module' here is that amp.init() is called once per module. But even that doesn't matter much, since all subsequent calls would be a noop.
Regardless of what you declare in pytest amp.init() cannot be undone, at least with the current API.",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r526583980,mk-61,2020-11-19 04:17:34,19463,19488,Data bug,1,"It doesn't matter. The only reason I put scope='module' here is that amp.init() is called once per module. But even that doesn't matter much, since all subsequent calls would be a noop. Regardless of what you declare in pytest amp.init() cannot be undone, at least with the current API."
"Should we add a utility to disable amp? If it is too much work to add quickly, you may add a separate pytest call in the `runtime_functions.sh` file for the amp test files.",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r526584389,leezu,2020-11-19 04:19:27,19463,19488,Data bug,1,"Should we add a utility to disable amp? If it is too much work to add quickly, you may add a separate pytest call in the [code] file for the amp test files."
"In fact, the amp in pytorch is implemented as a context manager.",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r526592545,sxjscience,2020-11-19 04:50:32,19463,19488,Data bug,1,"In fact, the amp in pytorch is implemented as a context manager."
"I agree it's a nifty API to enable/disable AMP with a context manager, but would definitely require a separate, non-trivial change.
For now, to unblock this PR, moved amp_init tests into a separate process.",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r527138082,mk-61,2020-11-19 19:20:24,19463,19488,Data bug,1,"I agree it's a nifty API to enable/disable AMP with a context manager, but would definitely require a separate, non-trivial change. For now, to unblock this PR, moved amp_init tests into a separate process."
@mk-61 would you like to help create a tracking issue for that change? It sounds like something we should do before the stable 2.0 release,PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r527151701,leezu,2020-11-19 19:42:42,19463,19488,Data bug,1,@mk-61 would you like to help create a tracking issue for that change? It sounds like something we should do before the stable 2.0 release
"@leezu - for now, I've just added an item here - https://github.com/apache/incubator-mxnet/issues/18896 - I keep it as a TODO list for AMP, of sorts. Still, can create a separate issue if you prefer to track it like this.
About adding it before the stable 2.0 release, well... To be clear - it's not just a matter of exposing something existing via a new API. Also, I'd like to clarify our plans about https://github.com/apache/incubator-mxnet/issues/18697 first, since it affects pretty much everything AMP-related.
When do we expect the stable 2.0 release? We still have some other, higher-priority items to fix.
Finally, my hope was that if a new API is going to be a strict superset of already existing one - i.e., we'll still keep amp.init() call, but also add a way to turn it off - may be it's acceptable to add it later? Or, to put it differently, may be it's less of an issue to add a new APIs, as long as we are not removing anything?",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r527182329,mk-61,2020-11-19 20:36:52,19463,19488,Data bug,1,"@leezu - for now, I've just added an item here - [url] - I keep it as a TODO list for AMP, of sorts. Still, can create a separate issue if you prefer to track it like this. About adding it before the stable 2.0 release, well... To be clear - it's not just a matter of exposing something existing via a new API. Also, I'd like to clarify our plans about [url] first, since it affects pretty much everything AMP-related. When do we expect the stable 2.0 release? We still have some other, higher-priority items to fix. Finally, my hope was that if a new API is going to be a strict superset of already existing one - i.e., we'll still keep amp.init() call, but also add a way to turn it off - may be it's acceptable to add it later? Or, to put it differently, may be it's less of an issue to add a new APIs, as long as we are not removing anything?"
"> @leezu - for now, I've just added an item here - #18896 - I keep it as a TODO list for AMP, of sorts. Still, can create a separate issue if you prefer to track it like this.

That's fine. Thank you

>  About adding it before the stable 2.0 release, well... To be clear - it's not just a matter of exposing something existing via a new API. Also, I'd like to clarify our plans about #18697 first, since it affects pretty much everything AMP-related.

Using a graph pass may address the current issue of global state. A potential downside is that graph pass will only work with hybrid models.

> When do we expect the stable 2.0 release? We still have some other, higher-priority items to fix.

We'd like to create an Alpha release soon. There is no date for a stable release yet.

>  Finally, my hope was that if a new API is going to be a strict superset of already existing one - i.e., we'll still keep amp.init() call, but also add a way to turn it off - may be it's acceptable to add it later? Or, to put it differently, may be it's less of an issue to add a new APIs, as long as we are not removing anything?

It's fine to change the API in 2.0, if we feel it should be improved.",PrComment,https://github.com/apache/mxnet/pull/19488#discussion_r527195906,leezu,2020-11-19 21:01:57,19463,19488,Data bug,1,"> @leezu - for now, I've just added an item here - #18896 - I keep it as a TODO list for AMP, of sorts. Still, can create a separate issue if you prefer to track it like this. That's fine. Thank you > About adding it before the stable 2.0 release, well... To be clear - it's not just a matter of exposing something existing via a new API. Also, I'd like to clarify our plans about #18697 first, since it affects pretty much everything AMP-related. Using a graph pass may address the current issue of global state. A potential downside is that graph pass will only work with hybrid models. > When do we expect the stable 2.0 release? We still have some other, higher-priority items to fix. We'd like to create an Alpha release soon. There is no date for a stable release yet. > Finally, my hope was that if a new API is going to be a strict superset of already existing one - i.e., we'll still keep amp.init() call, but also add a way to turn it off - may be it's acceptable to add it later? Or, to put it differently, may be it's less of an issue to add a new APIs, as long as we are not removing anything? It's fine to change the API in 2.0, if we feel it should be improved."
"I think we can try another strange-looking shape, e.g., (32765, 511)",PrComment,https://github.com/apache/mxnet/pull/19596#discussion_r532360133,sxjscience,2020-11-30 05:48:05,19595,19596,Algorithm design bug,1,"I think we can try another strange-looking shape, e.g., (32765, 511)"
The only requirement for triggering the bug is that the total size needs to be greater than the chunk size used by miniz when writing to the file ,PrComment,https://github.com/apache/mxnet/pull/19596#discussion_r532688411,leezu,2020-11-30 15:36:54,19595,19596,Algorithm design bug,1,The only requirement for triggering the bug is that the total size needs to be greater than the chunk size used by miniz when writing to the file
"adding a few varied sizes may help capture other bugs in the future, including those on the very small and very large ends.",PrComment,https://github.com/apache/mxnet/pull/19596#discussion_r532692168,szha,2020-11-30 15:41:48,19595,19596,Algorithm design bug,1,"adding a few varied sizes may help capture other bugs in the future, including those on the very small and very large ends."
"that said, I think fixing the bug is more urgent and I'm ok with merging it once CI passes",PrComment,https://github.com/apache/mxnet/pull/19596#discussion_r532692637,szha,2020-11-30 15:42:27,19595,19596,Algorithm design bug,1,"that said, I think fixing the bug is more urgent and I'm ok with merging it once CI passes"
"Yes, we can add more tests later. We should always try to add more tests to these the basic functionalities.",PrComment,https://github.com/apache/mxnet/pull/19596#discussion_r532895672,sxjscience,2020-11-30 20:52:07,19595,19596,Algorithm design bug,1,"Yes, we can add more tests later. We should always try to add more tests to these the basic functionalities."
I always had the impression that initialization with literal empty dict is preferred.,PrComment,https://github.com/apache/mxnet/pull/19812#discussion_r568136470,szha,2021-02-01 21:01:29,19811,19812,Test bug,0,I always had the impression that initialization with literal empty dict is preferred.
The only exception may be function / method arguments,PrComment,https://github.com/apache/mxnet/pull/19812#discussion_r568171085,leezu,2021-02-01 22:03:02,19811,19812,Test bug,0,The only exception may be function / method arguments
will change to literal,PrComment,https://github.com/apache/mxnet/pull/19812#discussion_r568199667,samskalicky,2021-02-01 22:55:48,19811,19812,Test bug,0,will change to literal
could you add a separate test for npx interface?,PrComment,https://github.com/apache/mxnet/pull/20129#discussion_r611247865,szha,2021-04-11 21:23:20,19586,20129,Processor bug,1,could you add a separate test for npx interface?
"AFAIK numpy concatenate does not use oneDNN. I believe this implementation is used: https://github.com/apache/incubator-mxnet/blob/master/src/operator/numpy/np_matrix_op.cc#L681. 
Should we add oneDNN support?",PrComment,https://github.com/apache/mxnet/pull/20129#discussion_r614140568,PawelGlomski-Intel,2021-04-15 14:51:37,19586,20129,Processor bug,1,AFAIK numpy concatenate does not use oneDNN. I believe this implementation is used: [url]#L681. Should we add oneDNN support?
"Since there shouldn't be functionality difference, I'd recommend making that decision based on expected performance.",PrComment,https://github.com/apache/mxnet/pull/20129#discussion_r614210811,szha,2021-04-15 16:18:11,19586,20129,Processor bug,1,"Since there shouldn't be functionality difference, I'd recommend making that decision based on expected performance."
How about moving this to line 595 above and avoid duplication of the nested ifs?,PrComment,https://github.com/apache/mxnet/pull/20146#discussion_r611868606,leezu,2021-04-12 18:42:35,20145,20146,Code bug,0,How about moving this to line 595 above and avoid duplication of the nested ifs?
"I think we need to add this compile def after https://github.com/apache/incubator-mxnet/blob/2cd1b4666d4e6d5df20bc139012061ef0d2e1a37/CMakeLists.txt#L656-L660. In fact I tried what you suggested and got into this error 
```
[2021-04-08T23:00:06.226Z] CMake Error at CMakeLists.txt:600 (target_compile_definitions):

[2021-04-08T23:00:06.226Z]   Cannot specify compile definitions for target ""mxnet"" which is not built by

[2021-04-08T23:00:06.226Z]   this project.

```",PrComment,https://github.com/apache/mxnet/pull/20146#discussion_r611928255,Zha0q1,2021-04-12 20:22:13,20145,20146,Code bug,0,I think we need to add this compile def after [url]#L656-L660. In fact I tried what you suggested and got into this error ``[code]``
"You're right. So you can consider add it after line 660, or move 656-660 to an earlier location in the file. It's not necessary to do it now.",PrComment,https://github.com/apache/mxnet/pull/20146#discussion_r611973124,leezu,2021-04-12 21:41:20,20145,20146,Code bug,0,"You're right. So you can consider add it after line 660, or move 656-660 to an earlier location in the file. It's not necessary to do it now."
"A question, since I do not really know - do we need to put `kWriteTo` here? I think it should be handled by the memory pass later on, right?",PrComment,https://github.com/apache/mxnet/pull/20462#discussion_r673472678,ptrendx,2021-07-20 20:33:54,20293,20462,Algorithm design bug,1,"A question, since I do not really know - do we need to put [code] here? I think it should be handled by the memory pass later on, right?"
"Yes, we need to put `kWriteTo` here. Because memory plan already happens in [SetBackwardGraph](https://github.com/apache/incubator-mxnet/blob/3480ba2c6df02bb907d3a975d354efa8697c4e71/src/imperative/cached_op.cc#L259) before this. Here we get the updated ref_count from the graph and modify the req_types based on the ref_count.  ",PrComment,https://github.com/apache/mxnet/pull/20462#discussion_r673515352,barry-jin,2021-07-20 21:46:18,20293,20462,Algorithm design bug,1,"Yes, we need to put [code] here. Because memory plan already happens in [SetBackwardGraph]([url]#L259) before this. Here we get the updated ref_count from the graph and modify the req_types based on the ref_count."
"Does tls_verify is a GitLab Runner configuration? Shouldn't the tls_verify be disabled by default? Following the [doc](https://docs.gitlab.com/runner/configuration/advanced-configuration.html)

> tls_verify	Enable or disable TLS verification of connections to Docker daemon. Disabled by default.",PrComment,https://github.com/apache/mxnet/pull/20512#discussion_r687762342,mozga-intel,2021-08-12 14:23:42,20494,20512,Test bug,1,Does tls_verify is a GitLab Runner configuration? Shouldn't the tls_verify be disabled by default? Following the [doc]([url] > tls_verify Enable or disable TLS verification of connections to Docker daemon. Disabled by default.
"No, it's a configuration in sphinx to disable verifying the server certifications. 
ref: https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-tls_verify ",PrComment,https://github.com/apache/mxnet/pull/20512#discussion_r687880038,barry-jin,2021-08-12 16:08:12,20494,20512,Test bug,1,"No, it's a configuration in sphinx to disable verifying the server certifications. ref: [url]#confval-tls_verify"
Please check generated documentation. Space or especially 2 spaces are sometimes meaningfull. Without it  there will be no new line.,PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r916649711,anko-intel,2022-07-08 09:40:47,20731,21090,Visualization bug,0,Please check generated documentation. Space or especially 2 spaces are sometimes meaningfull. Without it there will be no new line.
"I checked it already. Here are the screenshots:
![image](https://user-images.githubusercontent.com/59650839/177974295-ebe5b812-3815-4c6e-bf4c-c875ae5224f4.png)
![image](https://user-images.githubusercontent.com/59650839/177974315-7d5e9da6-0b82-4e70-acd9-07e8679ef4c9.png)
![image](https://user-images.githubusercontent.com/59650839/177974335-56a30cd7-ab35-4ba1-839b-719efa6b27e2.png)
![image](https://user-images.githubusercontent.com/59650839/177974345-c174bdda-bc90-41a8-9e4d-b293e06b4ea3.png)
![image](https://user-images.githubusercontent.com/59650839/177974369-4d397961-05ca-4f16-b1be-88cd9ab2c0a4.png)
![image](https://user-images.githubusercontent.com/59650839/177974379-b42b4cb3-e31c-4103-afcd-5b1854b3387a.png)

I thought that adding a new line in text is enough to have a new line present in generated document. Could you point me to the markdown language documentation regarding spaces creating/necessary to create new line? This seems quite bizarre to me and would like to understand the logic behind it. ",PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r916685610,bartekkuncer,2022-07-08 10:27:25,20731,21090,Visualization bug,0,I checked it already. Here are the screenshots: ![image]([url] ![image]([url] ![image]([url] ![image]([url] ![image]([url] ![image]([url] I thought that adding a new line in text is enough to have a new line present in generated document. Could you point me to the markdown language documentation regarding spaces creating/necessary to create new line? This seems quite bizarre to me and would like to understand the logic behind it.
"Thank you for the screenshot. I've just observed such behaviour, when fixing documentation. Now I find some tips about it in the documentation:  https://www.markdownguide.org/basic-syntax#line-break-best-practices
You can see such behaviou if you generate this part documentation: https://github.com/apache/incubator-mxnet/pull/21060/files#diff-f16304c2f893c2e7d51d74dff3892e5e11b39b8043751ade78b0286bc49afbd2L225-L227
for the older version, where such double spaces where probably accidentally missing.",PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r916726087,anko-intel,2022-07-08 11:24:42,20731,21090,Visualization bug,0,"Thank you for the screenshot. I've just observed such behaviour, when fixing documentation. Now I find some tips about it in the documentation: [url]#line-break-best-practices You can see such behaviou if you generate this part documentation: [url]#diff-f16304c2f893c2e7d51d74dff3892e5e11b39b8043751ade78b0286bc49afbd2L225-L227 for the older version, where such double spaces where probably accidentally missing."
Thanks for the links!,PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r916733346,bartekkuncer,2022-07-08 11:36:20,20731,21090,Visualization bug,0,Thanks for the links!
"```suggestion
You can refer to [Step 5 in Crash Course](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html#New-in-MXNet-2.0:-faster-C++-backend-dataloaders) for a detailed performance increase with C++ backend.
```",PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r917649125,bgawrych,2022-07-11 08:02:48,20731,21090,Visualization bug,0,``[code]``
Shouldn't numpy be written as NumPy? Also there is inconsistency in ndarray - sometimes it's ndarray and sometimes NDArray,PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r917653470,bgawrych,2022-07-11 08:08:10,20731,21090,Visualization bug,0,Shouldn't numpy be written as NumPy? Also there is inconsistency in ndarray - sometimes it's ndarray and sometimes NDArray
Fixed for NumPy. NDArray is our legacy NDArray whereas ndarray written in small letters refers to NumPy's ndarray. ,PrComment,https://github.com/apache/mxnet/pull/21090#discussion_r918778463,bartekkuncer,2022-07-12 09:54:15,20731,21090,Visualization bug,0,Fixed for NumPy. NDArray is our legacy NDArray whereas ndarray written in small letters refers to NumPy's ndarray.
